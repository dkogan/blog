#+TITLE: Dima Kogan
#+DESCRIPTION: Dima's notes
#+DATE: <2014-07-25 Fri 17:34>
#+AUTHOR Dima Kogan

#+STARTUP: logdone
#+STARTUP: overview

#+PLUGIN_QRCODE: nil
#+TEMPLATE_DIR: templates
#+URL: http://notes.secretsauce.net

#+DEFAULT_CATEGORY: Notes
#+FILENAME_SANITIZER: ob-sanitize-string
#+POST_SORTER: ob-sort-posts-by-title
#+OPTIONS: tex:dvipng

* Blog details
** Copyright
  :PROPERTIES:
  :SNIPPET:  t
  :END:

   The text is in the public domain. All code is copyright [[mailto:dima -at- secretsauce -dot- net][Dima Kogan]], licensed
   under the LGPL /unless otherwise noted/

** Navigation
  :PROPERTIES:
  :SNIPPET:  t
  :END:

- [[file:{lisp}(ob:path-to-root){/lisp}/tags][Tags]]
- [[file:{lisp}(ob:path-to-root){/lisp}/archives.html][Archives]]
- [[file:{lisp}(ob:path-to-root){/lisp}/index.xml][RSS]]
- [[https://www.github.com/dkogan][Github]]

* Toplevel page
  :PROPERTIES:
  :PAGE:     index.html
  :TEMPLATE: blog_static_no_title.html
  :END:

You found my blog! This is mostly technical; if you want to hear about mountain
explorations instead, look [[https://www.tapatalk.com/groups/sangabrielmnts/search.php?author=dima&sf=firstpost][here]]. The full set of entries is in the [[file:{lisp}(ob:path-to-root){/lisp}/archives.html][Archives]].
Various coding projects/experiments are in the [[https://www.github.com/dkogan][Github]] repostories. Contact info
is at the bottom of the page, in tiny print.

Most recent post: [[file:{lisp}(ob:link-to-post (car ALL-POSTS)){/lisp}][{lisp}(ob:post-title (car ALL-POSTS)){/lisp}]]

* Notes
** DONE First post                                             :o@blog:emacs:
  CLOSED: [2014-03-17 Mon 03:48]

So it has come to this. I'd like to know what the hell I actually do all day, so
maybe keeping a record will make that more clear. Today, I started a blog! I had
some ideas about the publishing system I wanted going in, and finding that
system was interesting. Requirements:

- Editing in emacs with org-mode
- Publishing with a simple file copy (or git push, etc)
- Static pages; no server work other than sending the files
- No javascript; no client work other than basic rendering
- Premade templates so that things look reasonable out of the box
- Does not require me to actually learn web development

Turns out you can't have everything, but you can get close. In its simplest, org
itself can push HTML without anything else. This is minimally-styled, and not
assembled into pages that talk to each other. The org-mode wiki (worg) has a
list of various publishing systems that use org as a backend:
http://orgmode.org/worg/org-blog-wiki.html. There are several exporters to
common blogging platforms, and a few specially-written things.

In the end, I was looking at two systems: [[http://renard.github.io/o-blog/][*o-blog*]] and [[https://github.com/kelvinh/org-page][*org-page*]]. Both are
/blogging/ systems, so you get RSS, tags, timelines, etc.

*** Advantages of o-blog

- Neither o-blog or org-page are used heavily, but o-blog appears to have more
  users: I could only find one org-page site that wasn't the author's personal
  page
- Org-page feels a bit rougher than o-blog, which has a much nicer-looking
  layout out of the box
- Org-page feels more boilerplaty. Each file needs lots of tags that could
  potentially conflict
- Org-page has some sort of git integration, which maybe is actually a positive,
  but I didn't dig into it. The main publishing function takes some git refs,
  the system expects a particular branching structure, etc. Those are probably
  good, but it should be obvious what the basic export-all-this-stuff command is

*** Advantages of org-page

- Org-page organizes the posts into several files, while o-blog has the whole
  thing in one file. With o-blog this probably will get slow as the posts
  accumulate. The author is currently working on an update that supposedly would
  resolve this
- The major downside of o-blog is that it produces a very heavy,
  javascript-laden site that doesn't work /at all/ with JS turned off.

I was leaning towards o-blog, so I learned some web-development. O-blog uses
LESS instead of CSS, using some runtime javascript to convert the LESS (that the
browser doesn't understand) to CSS (which the browser /does/ understand). So I
exported the LESS to CSS, scrubbed the output to get rid of the shinier bits,
and I have my blog. This look reasonable, but still not ideal. Current issues:

- The new JS-free navigation bars I wrote disappear when the browser window is
  narrower than some threshold. This is almost certainly intentional in the CSS.
  It needs to do something better than simply disappearing. The JS version /did/
  do someting better, and I should try to match that
- O-blog renders equations with MathJax, which uses javascript. Org has the
  capability to generate images for each equation, and output those. I'd like to
  do that, but o-blog can't figure that out. I'll fix it at some point
- Tables look somewhat weird. Pretty sure this is an org feature (not o-blog).
  On opera I see a full grid, except the left bar. On firefox I see the top and
  bottom bars and no others.

There're probably more, and I'll discover them as I go. In the meantime, the
code and content are on Github.
** DONE Using DEMs to get GPX elevation profiles                :hiking:data:
   CLOSED: [2014-03-18 Tue 00:49]

When considering a new hike, I often want to know the elevation profile of the
route. Usually all I have is a 2D elevation track, from [[http://www.openstreetmap.org][OpenStreetMap]] for
instance; clearly this lacks elevation information.

Unrelatedly we have access to gridded elevation data. This primarily comes from
the [[http://en.wikipedia.org/wiki/SRTM][SRTM]] project: data available here: http://dds.cr.usgs.gov/srtm/version2_1/.
The raw SRTM data is pretty good, but there are some gaps. Some people have
cleaned up the raw data, to make clean tiles available. One such data source is
here: http://www.viewfinderpanoramas.org/dem3/.

So we have 2D track data and topography. We can thus combine these into a full
3D track. This isn't perfect since DEM data is granular, but it's way better
than nothing.

I just found out that there's a route to [[http://www.openstreetmap.org/#map=15/34.1662/-117.9293][Fish Canyon Falls]] that goes around the
rock quarry, and thus is open year-round. Bypassing the quarry requires climbing
up a steep hillside to gain a ridge, then descending the other side of the ridge
to the bottom of the canyon behind the quarry. Just how much extra climbing is
involved here? To find out, I wrote this:

#+CAPTION: =gpxSampleDEM.pl=
#+begin_src perl
#!/usr/bin/perl
use strict;
use warnings;

use Getopt::Euclid;
use feature ':5.10';
use autodie;

use Geo::Gpx;
use PDL;


my $W = 1201; # I use 3-minute DEMs, so each DEM is 1201 x 1201

my $gpx_fh;
if( $ARGV{'<input>'} eq '-' )
{
    $gpx_fh = \*STDIN;
}
else
{
  open $gpx_fh, '<', $ARGV{'<input>'};
}

my $gpx = Geo::Gpx->new( input => $gpx_fh );

my $iter = $gpx->iterate_points();
while( my $pt = $iter->() )
{
    say join( ' ', $pt->{lon}, $pt->{lat}, elevation( $pt->{lon}, $pt->{lat} ) );
}



sub elevation
{
    my ($lon, $lat) = @_;

    state %DEMs;
    my $demfileE = floor $lon;
    my $demfileN = floor $lat;

    $DEMs{$demfileE}{$demfileN} //= readDEM($demfileE, $demfileN);
    my $dem = $DEMs{$demfileE}{$demfileN};
    return 0 if( !ref($dem) );

    # use PDL::Graphics::Gnuplot;
    # gplot( with => 'image', $dem );
    # sleep(20);

    # the DEMs start in the NW corner
    my $ilon =      ($lon - $demfileE)  * $W;
    my $ilat = (1 - ($lat - $demfileN)) * $W;

    return 100.0/2.54/12.0 * $dem->interpND( pdl[$ilon, $ilat] );
}

sub readDEM
{
    my ($demfileE, $demfileN) = @_;

    my $path;
    if   ($demfileN >= 0 && $demfileE >= 0){ $path = sprintf("$ARGV{'--demdir'}/N%.2dE%.3d.hgt", $demfileN,  $demfileE); }
    elsif($demfileN >= 0 && $demfileE <  0){ $path = sprintf("$ARGV{'--demdir'}/N%.2dW%.3d.hgt", $demfileN, -$demfileE); }
    elsif($demfileN  < 0 && $demfileE >= 0){ $path = sprintf("$ARGV{'--demdir'}/S%.2dE%.3d.hgt", -$demfileN, $demfileE); }
    else                                   { $path = sprintf("$ARGV{'--demdir'}/S%.2dW%.3d.hgt", -$demfileN, -$demfileE); }

    say STDERR "Reading DEM '$path'";
    if( ! -e $path )
    {
        warn "DEM '$path' not found. All of its elevations will read as 0";
        return 0;
    }

    # I read the DEM on disk into the piddle, then flip the endianness of the
    # data. I wouldn't have to copy anything if the data was little-endian to
    # start with; I'd just mmap into the piddle.
    open my $fd, '<', $path;
    my $dem = zeros(short, $W, $W);
    sysread( $fd, ${$dem->get_dataref}, $W*$W*2, 0 );
    ${$dem->get_dataref} = pack( "s*", unpack("s>*", ${$dem->get_dataref}));

    # I also convert to floating point. Turns out the PDL interpolation routines
    # don't work with integers
    return $dem->float;
}



__END__

=head1 NAME

gpxSampleDEM.pl - Samples SRTM DEM data to compute elevations for a GPX track

=head1 OPTIONAL ARGUMENTS

=over

=item --demdir <demdir>

Directory that contains the DEM files

=for Euclid:
  demdir.type: string, -d demdir && -e demdir
  demdir.default: '.'

=item <input>

GPX input. If omitted or '-', the input is read from standard input

=for Euclid:
  input.type: readable
  input.default: '-'

=back

=head1 AUTHOR

Dima Kogan, C<< <dima@secretsauce.net> >>
#+end_src

The script is fairly straightforward. It examines every track point in the GPX,
finds the appropriate elevation using plain bilinear interpolation, and outputs
a (lon,lat,ele) tuple on STDOUT. On Debian the dependencies are

- =libgetopt-euclid-perl=
- =libgeo-gpx-perl=
- =pdl=

You need to pre-download 3" DEMs, and pass the directory to the script (1" would
certainly work better, but I haven't tried). Given the [[file:files/FishCanyonFalls/FishCanyonFallsTrail.gpx][gpx file]] scraped from an
OpenStreetMap way (itself traced from the satellite imagery), you can do this:

#+begin_src sh
gpxSampleDEM.pl --demdir DEMs FishCanyonFallsTrail.gpx | \
  feedgnuplot --domain --3d --lines --square_xy          \
     --xlabel 'lon(deg)' --ylabel 'lat(deg)' --zlabel 'Elevation(m)'
#+end_src

This makes an interactive 3D plot of the route. For a more traditional elevation
profile that's monotonic in distance, you can do something like this:

#+begin_src sh
gpxSampleDEM.pl --demdir DEMs FishCanyonFallsTrail.gpx | \
  awk '{print $3}'                                     | \
  feedgnuplot --lines                                    \
     --xlabel 'Monotonic with distance' --ylabel 'Elevation(m)'
#+end_src

I actually did go see this waterfall today (which is really nice). Here's a plot
of the elevation profile I gathered with my GPS unit today overlaid over the
elevation profile from the DEM:

#+begin_comment
Following plot made by exporting each data source, and plotting with gnuplot

gpx_xyz.pl ~/hiking/gpx/FishCanyonFalls.gpx | awk '!/#/ {print $3}' > real
gpxSampleDEM.pl --demdir ~/projects/horizon/DEMs_SRTM3.bak/ FishCanyonFallsTrail.gpx | awk '{print $3}' > fake

set xlabel "Monotonic with distance"
set ylabel "Elevation (m)"
set terminal svg
set output "FishCanyonFalls.svg"
plot "real" using ($0/1101):1 with lines title "Actual track from a hike", "fake" using ($0/1400):1 with lines title "Generated from a DEM"
set output
#+end_comment

#+ATTR_HTML: :width 80%
[[file:files/FishCanyonFalls/FishCanyonFalls.svg]]

Immediately several issues are noticeable[fn:1]. First of all, while each curve
is monotonic with distance, the relationship of the domain with distance is
different. This plot assumes they're both /linear/ with distance. It's not
really true, but close enough I suppose.

Second, we see that the DEM curve has some high-frequency oscillations. Those
are switchbacks that sample the DEM in a way that the DEM data is too coarse to
represent. The trail does not really oscillate like that, which is confirmed by
the GPS track. This effect would probably be mitigated with finer DEM data (1"
DEMs are available), but I haven't attempted this.

Third, we see that during the initial climb the DEM-derived elevation
consistently underreports the elevation. I suspect this is another artifact of
the coarseness of the DEM. If we're walking on a ridge, a bilinear interpolation
would take into account neighboring DEM pixels, which would be lower in
elevation (since it's a ridge). So on a ridge I would expect the DEM-derived
elevations to be under-reported, and in a canyon I'd expect them to be
over-reported. In this particular track, the initial climb and the initial
descent are on ridges, while the second climb is in a canyon. This brings us to
the next point.

The data in the second climb doesn't match /at all/. Here it's the GPS data
that's at fault. The canyon walls block the GPS signal, so GPS readings are
unreliable there.

So the grand conclusion of all this would appear to be that you can use 3" DEM
data to derive an elevation profile, but one should not expect this profile to
be terribly accurate. Still it's useful. Based purely on the DEM, I can see that
a round-trip on this route would entail 2800ft of net elevation gain. Seeing the
real track, this probably is an underestimate of ~200ft. Not bad.


[fn:1] The above analysis assumes that the implementation of the DEM sampler is
bug-free and that the DEM data is correct. While I don't know of any bugs, there
could be some. Same for the DEM data

** DONE X11 urgency hint and notifications                    :tools:desktop:
   CLOSED: [2014-03-19 Wed 00:20]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Already-running process notifications")){/lisp}][Already-running process notifications]]

[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Even better notifications")){/lisp}][Even better notifications]]
#+end_o_blog_alert

X11 has a common system for window notifications: the urgency hint. The relevant
section of the [[http://tronche.com/gui/x/icccm/sec-4.html#s-4.1.2.4][ICCCM standard]]:

#+begin_quote
The UrgencyHint flag, if set in the flags field, indicates that the client deems
the window contents to be urgent, requiring the timely response of the user. The
window manager must make some effort to draw the user's attention to this window
while this flag is set.
#+end_quote

Some window managers are uncompliant and don't support this. Possibly as a
result, people really like to reinvent this particular wheel: [[http://www.linuxjournal.com/content/tech-tip-get-notifications-your-scripts-notify-send][notify-send]],
[[http://mattn.github.io/growl-for-linux/][growl]], and more. My WM ([[http://notion.sourceforge.net/][notion]]) /does/ support this very well, with some really
nice UI integration. Thus applications can request to be drawn as urgent. This
really begs for a commandline tool so shells can request the user's attention at
key points. For instance I really want to say something like

#+begin_src sh
make; seturgent
#+end_src

I.e. this would launch a source build, and when the build completes, this
particular terminal emulator window would request the user's attention. The
build could take a long, time, and the user may want to do stuff with the build
products, but in the meantime they can go do something else.

This =seturgent= tool didn't exist, so I wrote one:

#+CAPTION: =seturgent=
#+begin_src perl
#!/usr/bin/perl

# Copyright 2012,2013 Dima Kogan
# License: GPL 3 or later

use strict;
use warnings;
use feature qw(say);

use X11::Protocol;
use X11::Protocol::WM;
use X11::WindowHierarchy;

# if no arguments are given, sets urgency on the current window
#
# if an argument is given, uses it as a regex on the window name (all matches
# are set as urgent)

my $usage = "$0 [regex on the window name]";
die $usage if @ARGV > 1;


my $x = X11::Protocol->new()
  or die "Couldn't open display";

my @ids;
if( ! @ARGV )
{
  @ids = ($ENV{WINDOWID});
}
else
{
  my @windows = x11_filter_hierarchy( filter => qr{$ARGV[0]} )
    or die "No matching windows found";

  say "Found " . scalar(@windows) . " matching windows";
  @ids = map {$_->{id}} @windows;
}

foreach my $id(@ids)
{
  die "No window id" unless $id;
  X11::Protocol::WM::change_wm_hints( $x, $id,
                                      urgency => 1 );
}
#+end_src

This uses [[https://metacpan.org/pod/X11::WindowHierarchy][X11::WindowHierarchy]] to find the window, and [[https://metacpan.org/pod/X11::Protocol::WM][X11::Protocol::WM]] to set
the urgency hint. Both are available in Debian. Usage is very straightforward:
with no arguments, the current window is set urgent. Otherwise, the one and only
argument is treated like a regex on the window title. If a single match is
found, that window is set urgent.

Now I /can/ say

#+begin_src sh
make; seturgent
#+end_src
** DONE Already-running process notifications                 :tools:desktop:
   CLOSED: [2014-03-20 Thu 22:35]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Even better notifications")){/lisp}][Even better notifications]]
#+end_o_blog_alert

The tool described in the last post ([[file:{lisp}(ob:link-to-post (ob:get-post-by-title "X11 urgency hint and notifications")){/lisp}][X11 urgency hint and notifications]]) works
well, but there's a common use case it does not support: completion notification
of already-running process. That post describes how to be notified when a build
completes:

#+begin_src sh
make; seturgent
#+end_src

But what if we already started the build? Another helper tool is required. Here
it is:

#+BEGIN_SRC sh

# As is, this can't be an external utility since it uses the shell builtin
# 'wait', which only works on direct children of this shell. An external utility
# creates another shell, so this doesn't work
function waitfor()
{
    # waits for a process to exit, and sets urgency when that happens. Expects a
    # single pgrep-able argument on the commandline. If no argument is given,
    # it'll look for the only child process.

    # if this process is a child of this shell, I use a blocking wait.
    # Otherwise, I poll.

    PID_ALL=$(pgrep -s0 -f $1)

    # filter out the current process (the shell) and 'xclip'. I have xclip
    # zombies apparently
    PID=$(comm -23 <(echo $PID_ALL | sort) <(echo $$ `pidof xclip` | xargs -n1 | sort))
    N=$(echo $PID | wc -w)

    if [[ $N -eq 1 ]]; then
        echo "Found unique process with pid $PID"
        kill -CONT $PID # resume this process, since it's almost certainly
                        # paused right now
        wait $PID;
        seturgent
        true
    elif [[ $N -ne 0 ]]; then
        echo "Found more than one matching process. Doing nothing";
        false
    elif [[ -z $1 ]]; then
        echo "No children of the current shell to wait on. Doing nothing";
        false
    else
        echo "Found no matching processes in this shell. Looking globally.";
        PID=$(pgrep -f $1)
        N=$(echo $PID | wc -w)
        if [[ $N -eq 0 ]]; then
            echo "Found no matching global process either. Giving up.";
            false
        elif [[ $N -ne 1 ]]; then
            echo "Found more than one global process. Giving up";
            false
        else
            echo "Found unique process with pid $PID"
            while (ps -p $PID > /dev/null) { sleep 10; }
            seturgent;
            true
        fi
    fi
}

#+END_SRC

This is a =zsh= shell script that lives in my =.zshrc=.

- with no argument, it acts on the only child of this shell
- with an argument, it uses =pgrep= to find a matching process, first in the
  local shell, then outside of the local shell

Once the target process is identified, the script waits for the process to exit,
then it sets the urgency hint on the terminal emulator window. If there's any
ambiguity about which process is being targeted, nothing is done.

The most common use case: if a long-running process is currently active, one
would temporarily suspend it with =C-z=, then issue a =waitfor=. This
re-activates the process, and sets the urgency when finished. One could also
re-implement the use case from the previous post as

#+begin_src sh
make & waitfor
#+end_src


As said previously, this is a =zsh= script. It probably needs to be tweaked a
little bit to work in =bash=, but I have not done this.

The reason this is a shell script, is that the wait-for-this-process-to-finish
operation on Linux only works from the parent of the process being waited on. As
implemented, =waitfor()= doesn't spawn a new process, and runs in the shell
process itself, which is the parent of the thing being waited on. If this was
anything other than a shell script, then the waiter would /also/ be a child of
the shell, so the process being waited on, and the process doing the waiting
would be /siblings/. The script works that case too, but it polls every 10
seconds, instead of being notified of completion.

I've been using this for a little bit. It's not perfect, and there're some warts
I'd like to fix. Still, it does the job, and it's already something I use every
day.

** DONE Cscope benchmarks                              :tools:dev:data:emacs:
   CLOSED: [2014-03-25 Tue 03:36]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "GNU Global benchmarks")){/lisp}][GNU Global benchmarks]]

[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "More Cscope benchmarks")){/lisp}][More Cscope benchmarks]]
#+end_o_blog_alert

I read and write lots of C code, and I find the [[http://cscope.sourceforge.net/][cscope]] tool to be invaluable in
searching and navigating code bases. Recently I took over maintership of the
[[https://github.com/dkogan/xcscope.el][xcscope.el]] Emacs interface to cscope. There are a surprising number of different
Emacs interfaces to cscope, and this one seems to be the most mature and
full-featured (and I made it much nicer).

One feature that some other interfaces have ([[http://lists.gnu.org/archive/html/gnu-emacs-sources/2008-04/msg00021.html][ascope]] for instance) is that
instead of running a new cscope process for each query, they leave the process
running, and reuse it for each query. This keeps the database in memory, and
doesn't waste cycles reloading it every time. This is the major feature of these
interfaces, and glorious performance benefits are claimed.

Currently =xcscope= does /not/ do this, and I sometimes consider implementing
this feature. It's going to be a pain to do, so I decided to run some tests to
see if the performance benefits really are worth it.

*** Benchmark machine

All tests were run on my relatively quick server. It has a quad-core Ivy bridge
Core-i5 CPU, 4GB of RAM and a non-SSD hard disk.

*** Test description

The code base under test is the linux kernel. This should be near the upper
bound of what most people would be indexing. Sure, larger projects exist, but
you're generally working on a contained piece, rather than the whole thing at
once (this is true of the kernel too, actually).

I perform multiple discrete cscope operations using the command-line tool. Each
operation starts a new =cscope= process, which reloads the database. I.e. I
perform the operation that's supposedly slow every time.

I measure how long it takes to build the search database, then to re-build it,
then to re-build it after =touch=-ing a file. Then I measure how long it takes
to run a search, then to re-run it, then to re-run it after =touch=-ing a file.

I do all this with the default settings, then again with settings more
appropriate for a kernel:

- /kernel mode/: =-k= option. Doesn't look in =/usr/include=
- /inverted-index mode/: =-q= option. Builds an extra index for faster searches

Each search is also run with the =-d= option. This only runs the search; it does
/not/ also update the database with each search. By default, cscope /does/
update the database with every search.

Specifically, I get the list of files with 

#+begin_src sh
cscope-indexer -l -r  
#+end_src

I build an index with

#+begin_src sh
cscope -b
#+end_src

If I'm indexing in kernel mode and I'm building an inverted index, I also pass
in =-q -k=. The test search looks for all uses of the =main= symbol:

#+begin_src sh
cscope -L0 main
#+end_src

Once again, if I'm indexing in kernel mode and I'm building an inverted index, I
also pass in =-q -k=. When I want to touch an arbitrary file, I do

#+begin_src sh
touch include/drm/drm_edid.h 
#+end_src

There's no significance to this file. It's just anything that's in the index.

As one can imagine, the disk cache plays a very significant role here, and
subsequent runs of the same command complete faster than the first. For this
reason all tests are run with both a cold cache (by dumping the disk cache prior
to the test) and a warm cache (/not/ dumping the cache, and pre-running the
operation a few times before timing). I also ran these tests on an actual hard
disk, and also on a tmpfs ramdisk.

All timings were performed multiple times, with the initial few values and the
outliers thrown out. The exact script used to collect the data is described and
available in the [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "GNU Global benchmarks")){/lisp}][next post]].

*** Results

All timings in seconds.

**** Cold disk cache

|                                              | Normal mode/ext3 | Kernel mode/ext3 | Normal mode/tmpfs | Kernel mode/tmpfs |
|----------------------------------------------+------------------+------------------+-------------------+-------------------|
| Initial database build                       |             45.9 |             80.2 |              14.0 |              44.2 |
| Database re-build after touching a file      |             10.4 |             48.9 |               3.2 |              30.1 |
| Initial search                               |              7.5 |              3.0 |               0.8 |              31.2 |
| Re-search after touching a file              |             12.7 |             43.7 |               3.5 |              32.1 |
| Initial no-db-update search                  |              5.3 |              0.8 |               0.8 |               0.8 |
| No-db-update re-search after touching a file |              5.1 |              0.8 |               0.7 |               0.8 |

**** Warm disk cache

|                                              | Normal mode/ext3 | Kernel mode/ext3 | Normal mode/tmpfs | Kernel mode/tmpfs |
|----------------------------------------------+------------------+------------------+-------------------+-------------------|
| Initial database build                       |             13.8 |             49.6 |              12.9 |              44.4 |
| Database re-build after touching a file      |              3.5 |             35.5 |               2.7 |              30.8 |
| Initial search                               |              0.8 |              0.1 |               0.8 |              30.8 |
| Re-search after touching a file              |              4.0 |             33.5 |               3.5 |              31.9 |
| Initial no-db-update search                  |              0.7 |              0.0 |               0.7 |               0.7 |
| No-db-update re-search after touching a file |              0.7 |              0.0 |               0.7 |               0.7 |

*** Conclusions

I've known about the cscope inverted index for a while, but never actually tried
to use it. Looks like it works as advertised: takes significantly longer to
build, but once built the speedup it provides is substantial. It would appear
that the main benefit of the inverted index is that less data needs to be read
from disk and /not/ that less searching is required. At least on this particular
test machine the inverted index has no upside if the data is all in RAM already
(tmpfs). On a slower box maybe we'd see the search times become significant, but
not here.

It's extremely clear that the overhead of just loading the database is
immaterial. It's effectively instant to load the database and then to run a
search in an inverted index with a warm cache. It's a bit slower without an
inverted index, but all the time there is spent searching, not loading the index
into memory. I know this because I get the same no-inverted-index search timings
with the cscope interactive tool, which loads the database just once. The only
way keeping the =cscope= process running is advantageous is if this makes it
more likely the caches stay warm. This is difficult to test, but I doubt it's
true. If I run repeated queries even with a new process every time, the data
stays cached, and things run quickly. What I think is much more likely is that
the people who wrote cscope interfaces such as =ascope= only used interfaces
such as =xcscope= without the =-d= option. I.e. they were updating the database
with every query, which clearly can be slow with a large codebase. Then they
were /not/ doing this with their persistent =cscope= sessions, and attributing
the performance gains to not loading the database rather than rebuilding the
index too often. In any case, I think it's pretty clear that this feature is not
worth the work, so I'm keeping =xcscope= as is.

** DONE GNU Global benchmarks                                :tools:data:dev:
   CLOSED: [2014-03-30 Sun 00:42]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "More Cscope benchmarks")){/lisp}][More Cscope benchmarks]]
#+end_o_blog_alert

The [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Cscope benchmarks")){/lisp}][last post]] reports some performance numbers for [[http://cscope.sourceforge.net][cscope]]. There's another,
similar tool that I've been curious about: [[http://www.gnu.org/s/global/global.html][GNU global]]. It's like cscope in many
ways. It doesn't have some of the nicer cscope search types (caller, callee,
assignment, etc), and cscope works fine so I've never felt the need to move.
Since I just ran some cscope benchmarks, I thought it'd be interesting to run
the exact same tests with GNU global. Here I use the =gtags-cscope= frontend.
This is a compatibility layer in GNU global that has an identical interface to
cscope (among other things this makes it trivial to use =xcscope.el= with GNU
global).

*** Test description

The test conditions are the same as before. The testing in this /and/ the
previous post was performed by a script, which appears at the end of this post.
=gtags-cscope= doesn't have a separate inverted-index mode, so just a single
test appears here.

Here I'm using GNU global 6.2.10 built from source (upstream is in some sort of
fight with the Debian maintainer, so the packages are ancient). Cscope is 15.8a.

*** Results

All timings in seconds. Timings from the previous post are re-iterated for easy
comparison.

**** Cold disk cache

|                                              | Normal mode/ext3 | Kernel mode/ext3 | GNU Global/ext3 | Normal mode/tmpfs | Kernel mode/tmpfs | GNU Global/tmpfs |
|----------------------------------------------+------------------+------------------+-----------------+-------------------+-------------------+------------------|
| Initial database build                       |             45.9 |             80.2 |            84.1 |              14.0 |              44.2 |             14.0 |
| Database re-build after touching a file      |             10.4 |             48.9 |            26.8 |               3.2 |              30.1 |              0.7 |
| Initial search                               |              7.5 |              3.0 |            23.3 |               0.8 |              31.2 |              0.2 |
| Re-search after touching a file              |             12.7 |             43.7 |            28.4 |               3.5 |              32.1 |              0.7 |
| Initial no-db-update search                  |              5.3 |              0.8 |             0.1 |               0.8 |               0.8 |              0.0 |
| No-db-update re-search after touching a file |              5.1 |              0.8 |             0.1 |               0.7 |               0.8 |              0.0 |

**** Warm disk cache

|                                              | Normal mode/ext3 | Kernel mode/ext3 | GNU Global/ext3 | Normal mode/tmpfs | Kernel mode/tmpfs | GNU Global/tmpfs |
|----------------------------------------------+------------------+------------------+-----------------+-------------------+-------------------+------------------|
| Initial database build                       |             13.8 |             49.6 |            18.0 |              12.9 |              44.4 |             13.7 |
| Database re-build after touching a file      |              3.5 |             35.5 |             1.3 |               2.7 |              30.8 |              0.6 |
| Initial search                               |              0.8 |              0.1 |             0.4 |               0.8 |              30.8 |              0.2 |
| Re-search after touching a file              |              4.0 |             33.5 |             1.3 |               3.5 |              31.9 |              0.6 |
| Initial no-db-update search                  |              0.7 |              0.0 |             0.0 |               0.7 |               0.7 |              0.0 |
| No-db-update re-search after touching a file |              0.7 |              0.0 |             0.0 |               0.7 |               0.7 |              0.0 |

*** Conclusions

During *normal* use, we'd have a warm cache and we'd be using a real hard disk.
This is the bottom-left area of the timing tables. Those timings indicate that
GNU Global is much faster than cscope. Search performance appears to be on-par
with with an inverted-index-enabled cscope, but database build times only suffer
a little bit. This is interesting, and maybe would be worth switching to at some
point.

*** Benchmark script

All the timings were performed with the following =zsh= script. It uses some
=zsh=-isms, but could be converted to =bash= if somebody cares to do it.

#+begin_src sh
#!/bin/zsh

# needed in cleandb()
setopt nonomatch

function dropcaches() {
    if [[ $warmcold == "cold" ]]; then
        sync ;
        sudo sysctl -w vm.drop_caches=3;
    fi
    sleep 2;
}

function cleandb() {
    # requires nonomatch option to ignore missing globs
    rm -f cscope.out* G*;
}

function touchfile() {
    sleep 2; # very important. cscope needs this to see the file update
    touch include/drm/drm_edid.h;
}

TIMEFMT='%E'

awktally='
BEGIN {
  skip = ENVIRON["skip"]
}

/^[0-9\.]+s$/ {
  gsub("s","");
  str = str " " $1
  if( n >= skip )
  {
    sum += $1;
  }
  n++;
}

END {
  print ENVIRON["name"] ": skipping: " skip " all: " str " mean: " sum/(n-skip)
}'

typeset -A skipcounts
skipcounts=(cold 2 warm 2)

typeset -A modeoptions
modeoptions=(normal "" kernel "-k -q")

cscope-indexer -l -r

Nrepeat=8

for mode (normal kernel global)
{
    if [[ $mode == "global" ]]; then
        cmd="gtags-cscope";
    else
        cmd="cscope $modeoptions[$mode]";
    fi

    for dotouch (0 1)
    {
        for warmcold (cold warm)
        {
            export name="$warmcold initial build; $mode mode; touching: $dotouch";
            export skip=$skipcounts[$warmcold];
            repeat $(($Nrepeat + $skip)) {
                if (($dotouch)); then
                    touchfile;
                else
                    cleandb;
                fi
                dropcaches;
                time ${(z)cmd} -b;
            } |& awk $awktally
        }
    }

    for dotouch (0 1)
    {
        for warmcold (cold warm)
        {
            export name="$warmcold initial search; $mode mode; touching: $dotouch";
            export skip=$skipcounts[$warmcold];
            repeat $(($Nrepeat + $skip)) {
                if (($dotouch)); then
                    touchfile;
                fi
                dropcaches;
                time ${(z)cmd} -L0 main > /dev/null;
            } |& awk $awktally
        }
    }

    for dotouch (0 1)
    {
        for warmcold (cold warm)
        {
            export name="$warmcold initial no-db search; $mode mode; touching: $dotouch";
            export skip=$skipcounts[$warmcold];
            repeat $(($Nrepeat + $skip)) {
                if (($dotouch)); then
                    touchfile;
                fi
                dropcaches;
                time ${(z)cmd} -d -L0 main > /dev/null;
            } |& awk $awktally
        }
    }
}
#+end_src

*** benchmark data                                                 :noexport:
**** ext3
cold initial build; normal mode; touching: 0: skipping: 2 all:  45.91 44.30 45.80 45.82 45.18 45.99 44.65 49.29 45.90 44.61 mean: 45.905
warm initial build; normal mode; touching: 0: skipping: 2 all:  14.27 13.67 14.25 14.24 14.27 13.24 13.23 13.56 14.71 13.21 mean: 13.8388
cold initial build; normal mode; touching: 1: skipping: 2 all:  9.87 9.78 10.00 9.85 9.99 13.60 9.97 9.91 10.04 10.01 mean: 10.4213
warm initial build; normal mode; touching: 1: skipping: 2 all:  3.41 3.42 3.83 3.19 3.47 3.75 3.35 3.19 3.43 3.65 mean: 3.4825
cold initial search; normal mode; touching: 0: skipping: 2 all:  7.12 7.09 7.12 7.20 7.15 7.20 7.08 10.33 7.14 7.12 mean: 7.5425
warm initial search; normal mode; touching: 0: skipping: 2 all:  0.83 0.82 0.82 0.82 0.83 0.83 0.82 0.82 0.82 0.82 mean: 0.8225
cold initial search; normal mode; touching: 1: skipping: 2 all:  11.97 11.81 11.79 12.29 14.96 13.74 12.29 12.13 12.11 12.08 mean: 12.6737
warm initial search; normal mode; touching: 1: skipping: 2 all:  3.99 4.01 3.90 4.01 3.99 3.91 4.10 4.05 3.84 3.96 mean: 3.97
cold initial no-db search; normal mode; touching: 0: skipping: 2 all:  6.06 4.06 6.15 4.16 8.56 4.07 5.71 4.05 5.79 4.07 mean: 5.32
warm initial no-db search; normal mode; touching: 0: skipping: 2 all:  0.75 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 mean: 0.74
cold initial no-db search; normal mode; touching: 1: skipping: 2 all:  5.74 4.07 5.80 4.05 5.80 4.05 6.96 4.07 5.80 4.05 mean: 5.0725
warm initial no-db search; normal mode; touching: 1: skipping: 2 all:  0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.75 0.74 mean: 0.74125
cold initial build; kernel mode; touching: 0: skipping: 2 all:  79.45 79.86 79.10 85.14 79.02 79.01 78.37 83.88 78.54 78.48 mean: 80.1925
warm initial build; kernel mode; touching: 0: skipping: 2 all:  67.54 54.28 51.28 48.01 48.80 50.04 49.71 50.04 49.12 49.78 mean: 49.5975
cold initial build; kernel mode; touching: 1: skipping: 2 all:  49.06 48.46 49.80 52.14 46.29 46.43 51.68 47.65 49.56 47.30 mean: 48.8563
warm initial build; kernel mode; touching: 1: skipping: 2 all:  47.38 37.87 36.10 38.85 35.39 34.04 33.23 37.30 33.47 35.28 mean: 35.4575
cold initial search; kernel mode; touching: 0: skipping: 2 all:  2.69 2.74 2.82 4.07 2.78 2.87 2.84 2.86 2.82 2.82 mean: 2.985
warm initial search; kernel mode; touching: 0: skipping: 2 all:  0.11 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 mean: 0.09
cold initial search; kernel mode; touching: 1: skipping: 2 all:  49.42 47.28 45.30 42.83 43.94 41.10 42.96 47.20 43.05 43.23 mean: 43.7013
warm initial search; kernel mode; touching: 1: skipping: 2 all:  36.59 33.33 33.77 32.52 34.47 32.23 32.93 33.60 34.35 33.92 mean: 33.4738
cold initial no-db search; kernel mode; touching: 0: skipping: 2 all:  1.15 0.56 0.94 0.62 0.92 0.61 1.05 0.61 0.93 0.59 mean: 0.78375
warm initial no-db search; kernel mode; touching: 0: skipping: 2 all:  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 mean: 0
cold initial no-db search; kernel mode; touching: 1: skipping: 2 all:  0.90 1.28 0.91 0.59 0.94 0.59 0.87 0.61 0.94 0.61 mean: 0.7575
warm initial no-db search; kernel mode; touching: 1: skipping: 2 all:  0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 mean: 0
cold initial build; global mode; touching: 0: skipping: 2 all:  86.15 83.39 84.38 86.24 85.76 81.48 82.46 85.79 84.17 82.54 mean: 84.1025
warm initial build; global mode; touching: 0: skipping: 2 all:  17.76 18.55 17.78 17.69 18.22 18.95 17.53 17.69 18.08 17.71 mean: 17.9563
cold initial build; global mode; touching: 1: skipping: 2 all:  26.69 30.73 27.52 25.89 26.93 25.70 25.73 25.89 29.63 27.17 mean: 26.8075
warm initial build; global mode; touching: 1: skipping: 2 all:  1.32 1.31 1.31 1.31 1.38 1.36 1.32 1.31 1.30 1.27 mean: 1.32
cold initial search; global mode; touching: 0: skipping: 2 all:  23.03 26.28 24.50 22.10 23.01 22.02 22.10 23.34 22.46 26.58 mean: 23.2638
warm initial search; global mode; touching: 0: skipping: 2 all:  0.38 0.37 0.36 0.37 0.37 0.36 0.36 0.36 0.36 0.36 mean: 0.3625
cold initial search; global mode; touching: 1: skipping: 2 all:  27.16 26.11 27.19 30.34 27.49 28.63 28.26 27.84 29.41 27.84 mean: 28.375
warm initial search; global mode; touching: 1: skipping: 2 all:  1.35 1.30 1.28 1.31 1.31 1.29 1.31 1.30 1.31 1.33 mean: 1.305
cold initial no-db search; global mode; touching: 0: skipping: 2 all:  1.39 0.82 1.07 0.63 0.21 0.07 0.05 0.04 0.06 0.05 mean: 0.2725
warm initial no-db search; global mode; touching: 0: skipping: 2 all:  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 mean: 0.01
cold initial no-db search; global mode; touching: 1: skipping: 2 all:  0.05 0.04 0.03 0.05 0.04 0.04 1.90 0.41 0.22 0.03 mean: 0.34
warm initial no-db search; global mode; touching: 1: skipping: 2 all:  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 mean: 0.01

**** tmpfs
cold initial build; normal mode; touching: 0: skipping: 2 all:  14.07 14.11 14.15 13.99 13.94 14.01 14.20 14.01 14.02 14.06 mean: 14.0475
warm initial build; normal mode; touching: 0: skipping: 2 all:  12.99 12.97 12.82 12.98 12.89 13.02 12.87 12.93 13.00 12.92 mean: 12.9287
cold initial build; normal mode; touching: 1: skipping: 2 all:  3.19 3.21 3.16 3.17 3.17 3.20 3.17 3.19 3.16 3.20 mean: 3.1775
warm initial build; normal mode; touching: 1: skipping: 2 all:  2.72 2.73 2.72 2.72 2.72 2.73 2.72 2.72 2.72 2.72 mean: 2.72125
cold initial search; normal mode; touching: 0: skipping: 2 all:  1.00 0.85 0.86 0.83 0.84 0.84 0.84 0.84 0.84 0.84 mean: 0.84125
warm initial search; normal mode; touching: 0: skipping: 2 all:  0.85 0.82 0.82 0.83 0.82 0.82 0.82 0.82 0.82 0.82 mean: 0.82125
cold initial search; normal mode; touching: 1: skipping: 2 all:  3.86 3.66 3.53 3.51 3.50 3.54 3.50 3.50 3.51 3.54 mean: 3.51625
warm initial search; normal mode; touching: 1: skipping: 2 all:  3.47 3.46 3.45 3.45 3.46 3.46 3.46 3.45 3.45 3.45 mean: 3.45375
cold initial no-db search; normal mode; touching: 0: skipping: 2 all:  0.77 0.74 0.75 0.75 0.75 0.75 0.75 0.75 0.80 0.74 mean: 0.755
warm initial no-db search; normal mode; touching: 0: skipping: 2 all:  0.75 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 mean: 0.74
cold initial no-db search; normal mode; touching: 1: skipping: 2 all:  0.75 0.74 0.74 0.74 0.74 0.74 0.75 0.75 0.76 0.76 mean: 0.7475
warm initial no-db search; normal mode; touching: 1: skipping: 2 all:  0.76 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 mean: 0.74
cold initial build; kernel mode; touching: 0: skipping: 2 all:  41.84 44.43 45.34 44.74 43.31 43.62 43.44 44.84 43.48 44.99 mean: 44.22
warm initial build; kernel mode; touching: 0: skipping: 2 all:  44.53 43.35 45.38 42.49 44.83 43.90 44.55 43.17 46.30 44.33 mean: 44.3687
cold initial build; kernel mode; touching: 1: skipping: 2 all:  30.15 29.48 29.68 29.75 30.45 29.80 30.18 30.34 30.11 30.74 mean: 30.1313
warm initial build; kernel mode; touching: 1: skipping: 2 all:  29.93 30.82 30.61 29.85 30.34 29.70 31.60 32.37 30.68 31.20 mean: 30.7937
cold initial search; kernel mode; touching: 0: skipping: 2 all:  33.00 31.38 31.03 31.81 31.35 31.12 31.17 32.33 30.26 30.48 mean: 31.1937
warm initial search; kernel mode; touching: 0: skipping: 2 all:  31.13 30.00 29.20 31.73 30.28 29.70 31.97 29.27 33.26 30.98 mean: 30.7987
cold initial search; kernel mode; touching: 1: skipping: 2 all:  31.60 30.70 30.90 32.33 33.94 31.82 31.68 31.33 33.09 31.56 mean: 32.0812
warm initial search; kernel mode; touching: 1: skipping: 2 all:  30.97 31.27 31.32 31.26 32.95 32.07 31.94 31.14 33.00 31.56 mean: 31.905
cold initial no-db search; kernel mode; touching: 0: skipping: 2 all:  0.88 0.74 0.78 0.74 0.78 0.79 0.74 0.75 0.77 0.76 mean: 0.76375
warm initial no-db search; kernel mode; touching: 0: skipping: 2 all:  0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 0.74 mean: 0.74
cold initial no-db search; kernel mode; touching: 1: skipping: 2 all:  0.77 0.75 0.74 0.74 0.93 0.78 0.77 0.74 0.77 0.75 mean: 0.7775
warm initial no-db search; kernel mode; touching: 1: skipping: 2 all:  0.74 0.74 0.74 0.74 0.74 0.76 0.74 0.74 0.74 0.74 mean: 0.7425
cold initial build; global mode; touching: 0: skipping: 2 all:  14.25 13.88 14.12 14.73 13.99 13.60 13.74 14.55 13.79 13.67 mean: 14.0238
warm initial build; global mode; touching: 0: skipping: 2 all:  13.47 13.53 13.78 13.59 13.85 14.02 13.43 13.68 13.59 13.99 mean: 13.7412
cold initial build; global mode; touching: 1: skipping: 2 all:  0.96 0.64 0.64 0.61 0.62 0.60 0.63 0.96 0.89 0.65 mean: 0.7
warm initial build; global mode; touching: 1: skipping: 2 all:  0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 mean: 0.59
cold initial search; global mode; touching: 0: skipping: 2 all:  0.62 0.28 0.20 0.22 0.20 0.22 0.21 0.20 0.20 0.20 mean: 0.20625
warm initial search; global mode; touching: 0: skipping: 2 all:  0.19 0.19 0.19 0.19 0.19 0.19 0.19 0.19 0.19 0.19 mean: 0.19
cold initial search; global mode; touching: 1: skipping: 2 all:  0.61 0.60 0.60 0.60 0.60 0.60 0.61 0.63 0.96 0.69 mean: 0.66125
warm initial search; global mode; touching: 1: skipping: 2 all:  0.60 0.59 0.59 0.59 0.59 0.59 0.59 0.61 0.59 0.59 mean: 0.5925
cold initial no-db search; global mode; touching: 0: skipping: 2 all:  0.08 0.05 0.04 0.02 0.06 0.05 0.03 0.02 0.04 0.17 mean: 0.05375
warm initial no-db search; global mode; touching: 0: skipping: 2 all:  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 mean: 0.01
cold initial no-db search; global mode; touching: 1: skipping: 2 all:  0.09 0.07 0.05 0.02 0.02 0.02 0.02 0.04 0.06 0.04 mean: 0.03375
warm initial no-db search; global mode; touching: 1: skipping: 2 all:  0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 mean: 0.01


**** Ext3-only data in a table, with formulas

ext3-only:
|                                              |       |       |       |       |       |       |       |       |       |       |     mean | stdev/mean |       |       |       |       |       |       |       |       |       |       |     mean |          stdev/mean |
|----------------------------------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+----------+------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+----------+---------------------|
| Initial database build                       | 45.91 | 44.30 | 45.80 | 45.82 | 45.18 | 45.99 | 44.65 | 49.29 | 45.90 | 44.61 |   45.905 |          3 | 14.27 | 13.67 | 14.25 | 14.24 | 14.27 | 13.24 | 13.23 | 13.56 | 14.71 | 13.21 | 13.83875 |                   4 |
| Database re-build after touching a file      |  9.87 |  9.78 | 10.00 |  9.85 |  9.99 | 13.60 |  9.97 |  9.91 | 10.04 | 10.01 | 10.42125 |         12 |  3.41 |  3.42 |  3.83 |  3.19 |  3.47 |  3.75 |  3.35 |  3.19 |  3.43 |  3.65 |   3.4825 |                   7 |
| Initial search                               |  7.12 |  7.09 |  7.12 |  7.20 |  7.15 |  7.20 |  7.08 | 10.33 |  7.14 |  7.12 |   7.5425 |         15 |  0.83 |  0.82 |  0.82 |  0.82 |  0.83 |  0.83 |  0.82 |  0.82 |  0.82 |  0.82 |   0.8225 |                   1 |
| Re-searchafter touching a file               | 11.97 | 11.81 | 11.79 | 12.29 | 14.96 | 13.74 | 12.29 | 12.13 | 12.11 | 12.08 | 12.67375 |          9 |  3.99 |  4.01 |  3.90 |  4.01 |  3.99 |  3.91 |  4.10 |  4.05 |  3.84 |  3.96 |     3.97 |                   2 |
| Initial no-db-update search                  |  6.06 |  4.06 |  6.15 |  4.16 |  8.56 |  4.07 |  5.71 |  4.05 |  5.79 |  4.07 |     5.32 |         30 |  0.75 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |     0.74 |                   0 |
| No-db-update re-search after touching a file |  5.74 |  4.07 |  5.80 |  4.05 |  5.80 |  4.05 |  6.96 |  4.07 |  5.80 |  4.05 |   5.0725 |         23 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.74 |  0.75 |  0.74 |  0.74125 |                   0 |
| Initial database build                       | 79.45 | 79.86 | 79.10 | 85.14 | 79.02 | 79.01 | 78.37 | 83.88 | 78.54 | 78.48 |  80.1925 |          3 | 67.54 | 54.28 | 51.28 | 48.01 | 48.80 | 50.04 | 49.71 | 50.04 | 49.12 | 49.78 |  49.5975 |                   2 |
| Database re-build after touching a file      | 49.06 | 48.46 | 49.80 | 52.14 | 46.29 | 46.43 | 51.68 | 47.65 | 49.56 | 47.30 | 48.85625 |          5 | 47.38 | 37.87 | 36.10 | 38.85 | 35.39 | 34.04 | 33.23 | 37.30 | 33.47 | 35.28 |  35.4575 |                   5 |
| Initial search                               |  2.69 |  2.74 |  2.82 |  4.07 |  2.78 |  2.87 |  2.84 |  2.86 |  2.82 |  2.82 |    2.985 |         15 |  0.11 |  0.09 |  0.09 |  0.09 |  0.09 |  0.09 |  0.09 |  0.09 |  0.09 |  0.09 |     0.09 |                   0 |
| Re-searchafter touching a file               | 49.42 | 47.28 | 45.30 | 42.83 | 43.94 | 41.10 | 42.96 | 47.20 | 43.05 | 43.23 | 43.70125 |          4 | 36.59 | 33.33 | 33.77 | 32.52 | 34.47 | 32.23 | 32.93 | 33.60 | 34.35 | 33.92 | 33.47375 |                   2 |
| Initial no-db-update search                  |  1.15 |  0.56 |  0.94 |  0.62 |  0.92 |  0.61 |  1.05 |  0.61 |  0.93 |  0.59 |  0.78375 |         25 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |       0. | nint((0. / 0.) 100) |
| No-db-update re-search after touching a file |  0.90 |  1.28 |  0.91 |  0.59 |  0.94 |  0.59 |  0.87 |  0.61 |  0.94 |  0.61 |   0.7575 |         22 |  0.01 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |       0. | nint((0. / 0.) 100) |
| Initial database build                       | 86.15 | 83.39 | 84.38 | 86.24 | 85.76 | 81.48 | 82.46 | 85.79 | 84.17 | 82.54 |  84.1025 |          2 | 17.76 | 18.55 | 17.78 | 17.69 | 18.22 | 18.95 | 17.53 | 17.69 | 18.08 | 17.71 | 17.95625 |                   3 |
| Database re-build after touching a file      | 26.69 | 30.73 | 27.52 | 25.89 | 26.93 | 25.70 | 25.73 | 25.89 | 29.63 | 27.17 |  26.8075 |          5 |  1.32 |  1.31 |  1.31 |  1.31 |  1.38 |  1.36 |  1.32 |  1.31 |  1.30 |  1.27 |     1.32 |                   3 |
| Initial search                               | 23.03 | 26.28 | 24.50 | 22.10 | 23.01 | 22.02 | 22.10 | 23.34 | 22.46 | 26.58 | 23.26375 |          7 |  0.38 |  0.37 |  0.36 |  0.37 |  0.37 |  0.36 |  0.36 |  0.36 |  0.36 |  0.36 |   0.3625 |                   1 |
| Re-searchafter touching a file               | 27.16 | 26.11 | 27.19 | 30.34 | 27.49 | 28.63 | 28.26 | 27.84 | 29.41 | 27.84 |   28.375 |          4 |  1.35 |  1.30 |  1.28 |  1.31 |  1.31 |  1.29 |  1.31 |  1.30 |  1.31 |  1.33 |    1.305 |                   1 |
| Initial no-db-update search                  |  1.39 |  0.82 |  1.07 |  0.63 |  0.21 |  0.07 |  0.05 |  0.04 |  0.06 |  0.05 |   0.2725 |        139 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |     0.01 |                   0 |
| No-db-update re-search after touching a file |  0.05 |  0.04 |  0.03 |  0.05 |  0.04 |  0.04 |  1.90 |  0.41 |  0.22 |  0.03 |     0.34 |        190 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |  0.01 |     0.01 |                   0 |
#+TBLFM: $12=vmean($4..$11)::$13=round((vsdev($4..$11)/$12)*100)::$24=vmean($16..$23)::$25=round((vsdev($16..$23)/$24)*100)

**** Final table; mean-only, outliers removed

Data order same as before:
| Initial database build                       |
| Database re-build after touching a file      |
| Initial search                               |
| Re-searchafter touching a file               |
| Initial no-db-update search                  |
| No-db-update re-search after touching a file |


|        | Cold cache | Warm cache |
|--------+------------+------------|
| Normal |       45.9 |       13.8 |
| ext3   |       10.4 |        3.5 |
|        |        7.5 |        0.8 |
|        |       12.7 |        4.0 |
|        |        5.3 |        0.7 |
|        |        5.1 |        0.7 |
|--------+------------+------------|
| Kernel |       80.2 |       49.6 |
| ext3   |       48.9 |       35.5 |
|        |        3.0 |        0.1 |
|        |       43.7 |       33.5 |
|        |        0.8 |        0.0 |
|        |        0.8 |        0.0 |
|--------+------------+------------|
| Global |       84.1 |       18.0 |
| ext3   |       26.8 |        1.3 |
|        |       23.3 |        0.4 |
|        |       28.4 |        1.3 |
|        |        0.1 |        0.0 |
|        |        0.1 |        0.0 |
|--------+------------+------------|
| Normal |       14.0 |       12.9 |
| tmpfs  |        3.2 |        2.7 |
|        |        0.8 |        0.8 |
|        |        3.5 |        3.5 |
|        |        0.8 |        0.7 |
|        |        0.7 |        0.7 |
|--------+------------+------------|
| Kernel |       44.2 |       44.4 |
| tmpfs  |       30.1 |       30.8 |
|        |       31.2 |       30.8 |
|        |       32.1 |       31.9 |
|        |        0.8 |        0.7 |
|        |        0.8 |        0.7 |
|--------+------------+------------|
| Global |       14.0 |       13.7 |
| tmpfs  |        0.7 |        0.6 |
|        |        0.2 |        0.2 |
|        |        0.7 |        0.6 |
|        |        0.0 |        0.0 |
|        |        0.0 |        0.0 |

** DONE Running qemu with a custom kernel on ARM                  :tools:dev:
   CLOSED: [2014-04-07 Mon 23:38]

So I was porting [[http://www.sysdig.org][sysdig]] to ARM, and needed a target device to test the progress.
Sysdig uses syscall tracepoints, which were added to Linux relatively recently,
in version 3.7. Thus the ARM devices I had lying around were too old, and thus
weren't suitable to test on (I could forward port the kernel patches that make
them work, but this would be too much of a tangential effort).

The solution I settled on was emulation. Qemu can run in /system/ mode to
emulate a full machine. Specific instructions on how to run a vanilla-ish Debian
system with a custom kernel were hard to find, so I'm documenting them here.
Aurélien Jarno has disk images of a fresh Debian install and kernel images for
Qemu emulation here: http://people.debian.org/~aurel32/qemu/armel/. This is the
=armel= images, but he has the other arches as well.

Those images work fine. I just need to run a custom kernel I build myself.
Aurélien provides the kernel image /and/ an init ramdisk. It's not immediately
clear how to build this =initrd= image (and my various attempts weren't
fruitful). It was also not obvious how to run without =initrd=. The solution
that worked for me in the end was a monolithic kernel (all necessary drivers
compiled in) and a particular set of qemu options to workwithout a ramdisk.

I built a vanilla Linux 3.14 kernel =zImage=. The multiarch cross-toolchain
isn't in Debian proper yet, but packages are available from
http://people.debian.org/~wookey/tools/debian/.

The [[file:files/kernelstuff/versatile.config][kernel config]] was based off the vanilla =versatile_defconfig=, with a few
drivers and things built in. As usual, the kernel can be built with something
like this:

#+begin_src sh
cd linux
git reset --hard v3.14
cp /tmp/versatile.config .config
make ARCH=arm CROSS_COMPILE=arm-linux-gnueabi- -j4 zImage
#+end_src

This produces an image in =arch/arm/boot/zImage=. Everything I care about is
built-in, so I don't care about shipping modules, or building a ramdisk. A qemu
command to use this:

#+begin_src sh
qemu-system-arm -M versatilepb -kernel ~/linux/arch/arm/boot/zImage -hda debian_wheezy_armel_standard.qcow2 -append "noinitrd root=/dev/sda1 rw"
#+end_src

This appears to boot successfully, mounting everything, bringing up the network,
etc. There is a benign warning about not being able to talk to the modules from
the original kernel, but I obviously don't care. By default =eth0= is at
=10.0.2.15= with the host machine reachable at =10.0.2.2=.

** DONE Reading DWARF prototypes in ltrace                        :tools:dev:
   CLOSED: [2014-04-14 Mon 02:51]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Reading DWARF prototypes in ltrace (part 2)")){/lisp}][Reading DWARF prototypes in ltrace (part 2)]]
#+end_o_blog_alert

From time to time I use the [[http://ltrace.org/][=ltrace=]] tool for introspection into user-space
processes. This is similar to =strace=, but hooks into library API calls intead
of just system calls. This is quite useful, but has some extra challenges.

With system calls you know beforehand the full set of functions you are hooking,
their prototypes, and the meaning and purpose of each argument. With general
libraries the space of all the possible APIs is huge, so you generally do /not/
know this. =ltrace= can read configuration files that define these interfaces,
so with a bit of manual effort you can provide this information. It would be
really nice to be able to trace generic function calls with no extra effort at
all. Much of the prototype data exists in debug infomation, which is often
available along with the executable binary. So by parsing this information, we
can trace API calls without needing to edit a configuration file.

*** Stock behavior

Let's say I have the following simple project. There are 3 files: =tstlib.h=,
=tstlib.c= and =tst.c=. These define a small library and an application
respectively. Let's say I have

=tstlib.h=
#+begin_src C
#pragma once

struct tree
{
    int x;
    struct tree* left;
    struct tree* right;
};
struct tree treetest(struct tree* t);

struct loop_a;
struct loop_b;
typedef struct loop_a { struct loop_b*   b; int x;} loop_a_t;
        struct loop_b {        loop_a_t* a; int x;};
void looptest( loop_a_t* a );

enum E { A,B,C };
typedef enum E E_t;
int enumtest( enum E a, E_t b );

struct witharray
{
    double x[5];
};
double arraytest( struct witharray* s );
#+end_src

=tstlib.c=
#+begin_src C
#include "tstlib.h"

struct tree treetest(struct tree* t)
{
    if(t->left  != NULL) treetest(t->left);
    if(t->right != NULL) treetest(t->right);
    t->x++;

    return *t;
}

void looptest( loop_a_t* a )
{
    a->x++;
    a->b->x++;
}

int enumtest( enum E a, E_t b )
{
    return a == b;
}

double arraytest( struct witharray* s )
{
    return s->x[0];
}
#+end_src

=tst.c=
#+begin_src C
#include "tstlib.h"
#include <unistd.h>

void main(void)
{
    struct tree d = {.x = 4};
    struct tree c = {.x = 3, .right = &d};
    struct tree b = {.x = 2};
    struct tree a = {.x = 1, .left = &b, .right = &c};
    treetest( &a );

    struct loop_a la = {.x = 5};
    struct loop_b lb = {.x = 6};
    la.b = &lb;
    lb.a = &la;
    looptest(&la);

    enum E ea = A, eb = B;
    enumtest( ea, eb );

    struct witharray s = {.x = {1.0,2.0,1.0,2.0,1.0}};
    arraytest( &s );
}
#+end_src

Now I build this with debug information, placing the library in a DSO and
setting the RPATH:

#+begin_src sh
cc -g -c -o tst.o tst.c
cc -fpic -g -c -o tstlib.o tstlib.c
cc -shared -Wl,-rpath=/home/dima/projects/ltrace/ltracetests -o tstlib.so  tstlib.o
cc -Wl,-rpath=/home/dima/projects/ltrace/ltracetests tst.o tstlib.so -o tst
#+end_src

I now run the stock =ltrace= to see calls into the =tstlib= library. I'm using
the latest =ltrace= in Debian/sid: version 0.7.3-4:

#+begin_src sh
dima@shorty:~/projects/ltrace/ltracetests$ ltrace -n2 -l tstlib.so ./tst

tst->treetest(0x7fff6b36ad30, 0x7fff6b36ada0, 0x7fff6b36ada0, 0 <unfinished ...>
  tstlib.so->treetest(0x7fff6b36acf0, 0x7fff6b36adc0, 0x7fff6b36adc0, 0) = 0
  tstlib.so->treetest(0x7fff6b36acf0, 0x7fff6b36ade0, 0x7fff6b36ade0, 0 <unfinished ...>
    tstlib.so->treetest(0x7fff6b36acb0, 0x7fff6b36ae00, 0x7fff6b36ae00, 0) = 0
  <... treetest resumed> )                                            = 0x7fff6b36acb0
<... treetest resumed> )                                              = 0x7fff6b36ad30
tst->looptest(0x7fff6b36ad90, 0x7fff6b36ae00, 0x7fff6b36ade0, 0x7fff6b36adc0) = 0x7fff6b36ad80
tst->enumtest(0, 1, 1, 0x7fff6b36adc0)                                = 0
tst->arraytest(0x7fff6b36ad50, 1, 1, 0x7fff6b36adc0)                  = 0x3ff0000000000000
+++ exited (status 0) +++
#+end_src

So we clearly see the calls, but the meaning of the arguments (and return
values) isn't clear. This is because =ltrace= has no idea what the prototypes of
anything are, and assumes that every API call is =long f(long,long,long,long)=.

*** Patched behavior

I made a patch to read in the prototypes from DWARF debugging information. The
initial version lives at https://github.com/dkogan/ltrace. This is far from
done, but it's enough to evaluate the core functionality. With the patched
=ltrace=:

#+begin_src sh
dima@shorty:~/projects/ltrace/ltracetests$ ltrace -n2 -l tstlib.so ./tst

tst->treetest({ 1, { 2, nil, nil }, { 3, nil, { 4, nil, nil } } } <unfinished ...>
  tstlib.so->treetest({ 2, nil, nil })                                = nil
  tstlib.so->treetest({ 3, nil, { 4, nil, nil } } <unfinished ...>
    tstlib.so->treetest({ 4, nil, nil })                              = nil
  <... treetest resumed> )                                            = { 5, nil, nil }
<... treetest resumed> )                                              = { 2, { 3, nil, nil }, { 4, nil, { 5, nil, nil } } }
tst->looptest({ { recurse^, 6 }, 5 })                                 = <void>
tst->enumtest(A, B)                                                   = 0
tst->arraytest({ [ 1.000000, 2.000000, 1.000000, 2.000000... ] })     = 1.000000
+++ exited (status 0) +++
#+end_src

Much better! We see the tree structure, the array and the enum values. The
return values make sense too. So this is potentially very useful.

*** Issues to resolve

Playing with this for a bit, it's becoming more clear what the issues are. The
DWARF information gives you the prototype, but an API definition is more than
just a prototype. For one thing, if a function has a pointer argument, this can
represent and input or an output. My implementation currently assumes it's an
input, but being wrong either way is problematic here:

- If a pointer is an output and ltrace interprets it as an input, then the
  output is never printed (as we can see in the loop test above). Furthermore,
  the input /will/ be printed and since there could be nested pointers, this
  could result in a segmentation fault. In this case =ltrace= can thus crash the
  process being instrumented. Oof.

- If a pointer is an input treated as an output, then again, we won't see useful
  information, and will be printing potentially bogus data at the output.

This can be remedied somewhat by assuming that an input /must/ be =const= (and
vice versa), but one can't assume that across the board.

Even if we somehow know that a pointer is an input, we still don't know how to
print it. How many integers does an =int*= point to? Currently I assume the
answer is 1, but what if it's not? Guessing too low we don't print enough useful
information; guessing too high can overrun our memory.

These are all things that =ltrace='s configuration files can take care of. So it
sounds to me like the best approach is a joint system, where both DWARF and the
config files are read in, and complementary definitions are used. It wouldn't be
fully automatic, but at least it could be /right/. In theory this is implemented
in the tree I linked to above, but it doesn't work yet.

This all needs a bit more thought, but I think I'm on to something.

** DONE Argument alignment in Linux system calls                  :tools:dev:
   CLOSED: [2014-04-16 Wed 02:08]

The last two posts talked about patches to =sysdig= and =ltrace=. This week
wouldn't be complete without patching =strace= as well. My patch series to make
=sysdig= work on ARM apparently had a bug: =preadv= and =pwritev= were not
reporting their =offset= argument properly. These two syscalls had the same
exact issue, so I'll just talk about =preadv=. The userspace prototype of this
syscall looks like this:

#+begin_src C
ssize_t preadv(int fd, const struct iovec *iov, int iovcnt, off_t offset);
#+end_src

=off_t= is a 64-bit value, so on 32-bit architectures this must be split across
two different registers when making the syscall. Some architectures also have
alignment requirements. In my case, the Linux ARM EABI requires that such values
be passed in a consecutive even/odd register pair, with a register of padding if
needed. Thus in the case of =preadv=, the values would be passed as follows:

| argument  | register |
|-----------+----------|
| fd        | r0       |
| iov       | r1       |
| iovcnt    | r2       |
| *padding* | r3       |
| offset    | r4/r5    |

The sysdig ARM code was doing this, and it worked fine for other syscalls, but
this was /not/ working for =preadv= and =pwritev=. To my surprise I discovered
that even =strace= was misreporting the value of the =offset= argument. I wrote
a small test program:

#+begin_src C
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/uio.h>

int main(void)
{
    const off_t offset = 1234567890123456789LL;
    char buf[4];

    int fd_zero = open("/dev/zero", O_RDONLY);
    pread (fd_zero, buf, sizeof(buf), offset);
    preadv(fd_zero,
           &(struct iovec){ .iov_base = buf,
                   .iov_len = sizeof(buf)},
           1, offset );

    int fd_null = open("/dev/null", O_WRONLY);
    pwrite(fd_null, buf, sizeof(buf), offset);
    pwritev(fd_null,
            &(struct iovec){.iov_base = buf, .iov_len = sizeof(buf)},
            1, offset );

    return 0;
}
#+end_src

Then I built it with =gcc -std=gnu99 -D_FILE_OFFSET_BITS=64=, and ran it under
=strace= on ARM. The relevant parts of =strace= output:

#+begin_src C
open("/dev/zero", O_RDONLY|O_LARGEFILE) = 3
pread(3, "\0\0\0\0", 4, 1234567890123456789) = 4
preadv(3, [{"\0\0\0\0", 4}], 1, 4582412532) = 4
open("/dev/null", O_WRONLY|O_LARGEFILE) = 4
pwrite(4, "\0\0\0\0", 4, 1234567890123456789) = 4
pwritev(4, [{"\0\0\0\0", 4}], 1, 4582412532) = 4
#+end_src

Note that the =offset= parameter in =preadv= and =pwritev= is reported
as 4582412532. As you can see in the source, the offset is actually the same for
all the calls: 1234567890123456789. So something fishy is going on. Digging
through kernel source revealed the answer. Here's how the =pread= and =preadv=
system calls are defined (I'm looking at =fs/read_write.c= in Linux 3.14):

#+begin_src C
SYSCALL_DEFINE4(pread64, unsigned int, fd, char __user *, buf,
			size_t, count, loff_t, pos)
SYSCALL_DEFINE5(preadv, unsigned long, fd, const struct iovec __user *, vec,
		unsigned long, vlen, unsigned long, pos_l, unsigned long, pos_h)
#+end_src

Note that =pread= defines its =pos= argument as a 64-bit value of type =loff_t=.
This is what you'd expect and also what the userspace =pread= prototype looks
like. Now look at =preadv=. It does /not/ have a 64-bit =pos= argument. Instead
it has two separate 32-bit arguments. This is /different/ from the userspace
prototype! So as far as the kernel is concerned, there are no 64 bit arguments
here, so no alignment requirements apply. So the /actual/ register map in the
=preadv= syscall looks like

| argument  | register |
|-----------+----------|
| fd        | r0       |
| iov       | r1       |
| iovcnt    | r2       |
| offset    | r3/r4    |

So libc must know to do this translation when invoking the syscall to connect
the two different prototypes. Both =sysdig= and =strace= did not know this, and
were interpreting the syscall inputs incorrectly.

There's even an [[https://lwn.net/Articles/311630/][LWN article]] about the discussion that took place when this was
originally implemented. There are various compatibility issues, and this was the
best method, apparently.

** DONE More Cscope benchmarks                               :tools:dev:data:
   CLOSED: [2014-04-20 Sun 23:43]

A patch to [[http://cscope.sourceforge.net][cscope]] was just posted: https://sourceforge.net/p/cscope/patches/86/.
This claims to speed up the building of the inverted index by using a more
efficient search algorithm in one place, and a better sorting implementation in
another. Since I did some cscope benchmarks earler ([[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Cscope benchmarks")){/lisp}][here]] and [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "GNU Global benchmarks")){/lisp}][here]]), I can easily
evaluate this patch, so I did this.

*** Test description

The results aren't directly comparable to the timings in the previous posts,
since the project being indexed (Linux kernel) is at a very different version;
much more recent and with many more sources. The test machine is the same as
before. All the tests were done with a real ext3 hard disk, /not/ a ramdisk. The
cscope is the stock cscope 15.8a-2 from Debian.

*** Results

All timings are in seconds.

**** Cold disk cache

|                                              |   Stock | Patched |
|----------------------------------------------+---------+---------|
| Initial database build                       | 123.572 | 95.5225 |
| Database re-build after touching a file      | 57.2912 |   30.91 |
| Initial search                               | 9.11125 |   8.415 |
| Re-search after touching a file              | 59.6287 |   31.92 |
| Initial no-db-update search                  | 0.80625 |  1.2075 |
| No-db-update re-search after touching a file |   0.805 |    0.95 |

**** Warm disk cache

|                                              |   Stock | Patched |
|----------------------------------------------+---------+---------|
| Initial database build                       | 55.3537 | 29.5287 |
| Database re-build after touching a file      | 45.4975 |  18.805 |
| Initial search                               | 0.12125 |    0.12 |
| Re-search after touching a file              |  45.985 | 19.0437 |
| Initial no-db-update search                  |       0 |       0 |
| No-db-update re-search after touching a file |       0 | 0.00125 |

Note that this tests /only the timings/. I did not actually look at the results
being produced. Presumably they match, but I did not check.

*** Conclusions

Yeah. Much faster. Hopefully this produces the correct results, and gets merged
in some form.

*** Benchmark script

Here's the script that was used to get the timings. It's pretty much the same as
before, with small modifications to set what is being tested. As before, this is
a =zsh= script. It uses some =zsh=-isms, but could be converted to =bash= if
somebody cares to do it.

#+begin_src sh
#!/bin/zsh

# needed in cleandb()
setopt nonomatch

function dropcaches() {
    if [[ $warmcold == "cold" ]]; then
        sync ;
        sudo sysctl -w vm.drop_caches=3;
    fi
    sleep 2;
}

function cleandb() {
    # requires nonomatch option to ignore missing globs
    rm -f cscope.out* G*;
}

function touchfile() {
    sleep 2; # very important. cscope needs this to see the file update
    touch include/drm/drm_edid.h;
}

TIMEFMT='%E'

awktally='
BEGIN {
  skip = ENVIRON["skip"]
}

/^[0-9\.]+s$/ {
  gsub("s","");
  str = str " " $1
  if( n >= skip )
  {
    sum += $1;
  }
  n++;
}

END {
  print ENVIRON["name"] ": skipping: " skip " all: " str " mean: " sum/(n-skip)
}'

typeset -A skipcounts
skipcounts=(cold 2 warm 2)

modeoptions="-k -q"

cscope-indexer -l -r

Nrepeat=8

for mode (kernel patched)
{
    if [[ $mode == "patched" ]]; then
        cmd="/tmp/cscope-15.8a-patched/src/cscope $modeoptions";
    else
        cmd="/tmp/cscope-15.8a/src/cscope $modeoptions";
    fi

    for dotouch (0 1)
    {
        for warmcold (cold warm)
        {
            export name="$warmcold initial build; $mode mode; touching: $dotouch";
            export skip=$skipcounts[$warmcold];
            repeat $(($Nrepeat + $skip)) {
                if (($dotouch)); then
                    touchfile;
                else
                    cleandb;
                fi
                dropcaches;
                time ${(z)cmd} -b;
            } |& awk $awktally
        }
    }

    for dotouch (0 1)
    {
        for warmcold (cold warm)
        {
            export name="$warmcold initial search; $mode mode; touching: $dotouch";
            export skip=$skipcounts[$warmcold];
            repeat $(($Nrepeat + $skip)) {
                if (($dotouch)); then
                    touchfile;
                fi
                dropcaches;
                time ${(z)cmd} -L0 main > /dev/null;
            } |& awk $awktally
        }
    }

    for dotouch (0 1)
    {
        for warmcold (cold warm)
        {
            export name="$warmcold initial no-db search; $mode mode; touching: $dotouch";
            export skip=$skipcounts[$warmcold];
            repeat $(($Nrepeat + $skip)) {
                if (($dotouch)); then
                    touchfile;
                fi
                dropcaches;
                time ${(z)cmd} -d -L0 main > /dev/null;
            } |& awk $awktally
        }
    }
}
#+end_src

*** original benchmark data                                        :noexport:
cold initial build; kernel mode; touching: 0: skipping: 2 all:  126.28 126.63 124.29 125.34 122.14 121.44 124.63 122.28 122.81 125.65 mean: 123.572
warm initial build; kernel mode; touching: 0: skipping: 2 all:  89.94 62.50 56.71 55.47 55.03 54.61 55.70 55.26 54.72 55.33 mean: 55.3537
cold initial build; kernel mode; touching: 1: skipping: 2 all:  56.69 59.06 57.32 57.04 57.00 57.08 57.56 57.46 57.27 57.60 mean: 57.2912
warm initial build; kernel mode; touching: 1: skipping: 2 all:  52.52 44.44 45.47 44.71 45.16 44.60 46.76 44.70 45.61 46.97 mean: 45.4975
cold initial search; kernel mode; touching: 0: skipping: 2 all:  8.33 8.27 8.30 11.44 9.70 8.50 8.28 8.38 9.96 8.33 mean: 9.11125
warm initial search; kernel mode; touching: 0: skipping: 2 all:  0.13 0.13 0.12 0.12 0.12 0.13 0.12 0.12 0.12 0.12 mean: 0.12125
cold initial search; kernel mode; touching: 1: skipping: 2 all:  60.87 61.72 62.23 59.06 59.06 60.08 58.66 58.50 57.91 61.53 mean: 59.6287
warm initial search; kernel mode; touching: 1: skipping: 2 all:  50.66 48.51 47.33 46.55 45.50 45.18 44.43 46.96 47.43 44.50 mean: 45.985
cold initial no-db search; kernel mode; touching: 0: skipping: 2 all:  0.98 0.65 0.96 0.65 0.97 0.65 0.97 0.64 0.96 0.65 mean: 0.80625
warm initial no-db search; kernel mode; touching: 0: skipping: 2 all:  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 mean: 0
cold initial no-db search; kernel mode; touching: 1: skipping: 2 all:  0.97 0.64 0.96 0.64 0.97 0.65 0.95 0.65 0.97 0.65 mean: 0.805
warm initial no-db search; kernel mode; touching: 1: skipping: 2 all:  0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 mean: 0
cold initial build; patched mode; touching: 0: skipping: 2 all:  95.11 97.38 95.83 97.90 95.01 95.20 93.65 94.51 95.92 96.16 mean: 95.5225
warm initial build; patched mode; touching: 0: skipping: 2 all:  30.03 29.29 29.68 29.13 29.57 29.33 29.92 29.01 29.34 30.25 mean: 29.5287
cold initial build; patched mode; touching: 1: skipping: 2 all:  31.32 30.97 30.92 30.80 30.78 31.09 32.26 30.75 29.89 30.79 mean: 30.91
warm initial build; patched mode; touching: 1: skipping: 2 all:  18.05 18.20 18.46 18.94 18.68 18.43 18.92 19.12 18.86 19.03 mean: 18.805
cold initial search; patched mode; touching: 0: skipping: 2 all:  9.68 8.12 8.12 8.11 9.28 8.17 8.15 7.95 8.19 9.35 mean: 8.415
warm initial search; patched mode; touching: 0: skipping: 2 all:  0.13 0.13 0.12 0.12 0.12 0.12 0.12 0.12 0.12 0.12 mean: 0.12
cold initial search; patched mode; touching: 1: skipping: 2 all:  28.92 29.93 31.87 33.01 31.26 34.48 31.39 30.79 31.38 31.18 mean: 31.92
warm initial search; patched mode; touching: 1: skipping: 2 all:  18.63 18.58 18.62 19.00 18.61 18.51 19.34 18.36 19.06 20.85 mean: 19.0437
cold initial no-db search; patched mode; touching: 0: skipping: 2 all:  1.11 0.67 1.09 0.66 1.06 0.67 3.29 0.68 1.54 0.67 mean: 1.2075
warm initial no-db search; patched mode; touching: 0: skipping: 2 all:  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 mean: 0
cold initial no-db search; patched mode; touching: 1: skipping: 2 all:  1.04 0.66 1.02 0.65 1.02 0.77 1.28 0.81 1.22 0.83 mean: 0.95
warm initial no-db search; patched mode; touching: 1: skipping: 2 all:  0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 mean: 0.00125

** DONE Even better notifications                             :tools:desktop:
   CLOSED: [2014-05-01 Thu 14:39]

Two previous posts ([[file:{lisp}(ob:link-to-post (ob:get-post-by-title "X11 urgency hint and notifications")){/lisp}][X11 urgency hint and notifications]] and [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Already-running process notifications")){/lisp}][Already-running
process notifications]]) talked about ways to notify the user about terminating
shell processes. I've been living with this setup for a little while, and I just
thought of a better way to do this. Instead of the user asking for notifications
about particular processes, why not get notifications about /all/ processes?

As before, I'm using the X11 urgency hint. This hint is automatically removed by
the window manager when the hinted window is focused. Thus if you set an urgency
hint on an already-focused window, nothing will happen. Thus setting urgency on
completion of every single command won't generate too much noise, since most of
the time we're in the same terminal window at the start /and/ the stop of the
command. You /will/ see a notification when you move to a different window
before the process exits, which is /exactly/ what you want here.

=zsh= has a convenient hook that can be used for this: =precmd= is called right
before the shell prompt is printed. So to notify on all completions, you can put
into your =.zshrc=:

#+begin_src sh
function precmd {
  seturgent
}
#+end_src

This works, with one caveat: as described previously, =seturgent= is a perl
script, and calling it this way one can feel the overhead. It feels slower than
it should be. Since =seturgent= isn't doing any searching here, I rewrote the
chunk of it we're using in C. As one would think, it's way quicker:

=seturgent_fast.c=
#+begin_src C
#include <stdio.h>
#include <stdlib.h>
#include <X11/Xlib.h>
#include <X11/Xutil.h>

int main(void)
{
    const char* window_idstring = getenv("WINDOWID");
    if( window_idstring == NULL )
    {
        fprintf(stderr, "No WINDOWID set\n");
        return 1;
    }
    Window w = atoi(window_idstring);
    if( w <= 0 )
    {
        fprintf(stderr, "Couldn't parse window id '%s'\n",
                window_idstring);
        return 1;
    }


    Display* display;
    const char* displaystring = getenv("DISPLAY");
    if( displaystring == NULL )
    {
        fprintf(stderr, "No DISPLAY set\n");
        return 1;
    }

    display = XOpenDisplay(displaystring);
    if( display == NULL )
    {
        fprintf(stderr, "Couldn't open display '%s\n", displaystring);
        return 1;
    }

    XWMHints* hints = XGetWMHints(display, w);
    if( hints == NULL )
    {
        fprintf(stderr, "Couldn't retrieve hints\n");
        return 1;
    }

    hints->flags |= XUrgencyHint;
    XSetWMHints(display, w, hints);

    XFree(hints);
    XFlush(display);
    XCloseDisplay(display);
    return 0;
}
#+end_src

This can be built simply with

#+begin_src sh
gcc -o seturgent_fast{,.c} -lX11
#+end_src

Running this for a bit the main discovery is that it's a bit easier to maintain
focus. Previously, I'd start a build or APT update (or whatever), then go do
something else, checking on the progress of the long task periodically. This
punctuated workflow is fairly inefficient, and the notification system help to
minimize it as much as is possible.

So yeah. I'll run this for a bit more, and we'll see if there's more to improve.
   
** DONE Emacs-snapshot package hosting                                :emacs:
   CLOSED: [2014-06-07 Sat 17:29]

A few months ago, Julian Danjou stopped updating his bleeding-edge GNU Emacs
Debian packages (http://emacs.naquadah.org/). I've been using those for a while,
and I'd like to continue doing so. Thus, I'm now building and hosting my own
bleeding-edge packages: http://emacs.secretsauce.net/.

There's nothing particularly noteworthy about the building or hosting of these.
The =/etc/apt/sources.list= entries are

#+BEGIN_EXAMPLE
deb     [arch=amd64] http://emacs.secretsauce.net unstable main
deb-src [arch=amd64] http://emacs.secretsauce.net unstable main
#+END_EXAMPLE

In other news, it turns out that web site hosting is now incredibly cheap.

** DONE Tab completion for sysdig                                     :tools:
   CLOSED: [2014-06-23 Mon 18:10]

I just implemented =zsh= tab-completion functionality for =sysdig=:

 https://raw.githubusercontent.com/dkogan/sysdig/master/scripts/completions/zsh/_sysdig

The patch was merged to =sysdig= upstream.

It's fairly nice, and makes =sysdig= easier to use for those who don't yet have
all the knobs memorized, such as myself. I complete on

- commandline options
- commandline option arguments
- chisel names
- chisel arguments
- filter field names

Some of those are hard-coded in the completion script, and some are reported by
the =sysdig= executable itself. Having written this I'm now acutely aware of
missing similar functionality in =tcpdump= and =perf=. Both of those have some
tab completion, but do not complete on event types. If they did, writing things
like =tcpdump= filters would be much easier. That's a good thing to add at some
point.

Another interesting discovery is that it is apparently normal for =zsh=
completion scripts to live in the =zsh= repository, /not/ in the repository of
the thing being completed. So in this case I apparently went against to grain by
contributing my script to =sysdig= instead of =zsh=. This feels right, though.
But if I make those additions to =tcpdump= and/or =perf= completions, those will
go to the =zsh= people.

** DONE Ltrace filtering details                                      :tools:
   CLOSED: [2014-06-25 Wed 16:51]
   
In an [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Reading DWARF prototypes in ltrace")){/lisp}][earlier post]] I talked about teaching ltrace to read function prototypes
from DWARF data. I'm making more progress on that front, and the initial code
has been merged into the upstream ltrace repository. One point of confusion for
me was the difference between ltrace's various filtering commandline options
=-x=, =-e=, =-l= and =-L=. I added a more thorough description and an example to
the ltrace manpage, and I'm discussing this here.

The ltrace filters specify which functions should be instrumented. Since ltrace
introduces non-negligible overhead to the running process, it's very desirable
to instrument only the functions you care about. Otherwise the process can be
slowed significantly.

Broadly speaking

- =-x= is *show me what calls these symbols (including local calls)*
- =-e= is *show me what calls these symbols (inter-library calls only)*
- =-l= is *show me what calls into this library*

Inter-library and intra-library calls are treated separately because they are
implemented differently in the binary. Calls into a shared object use the PLT
mechanism, while local calls do not.

If no =-e= or =-l= filtering options are given, ltrace assumes a default filter
of =-e @MAIN= (trace all non-local calls into the main executable). If only a
=-x= is given, this default filter is /still/ present, and it can be turned off
by passing =-L=. In my experience, if you're using =-x=, you pretty much always
want =-L= as well.

*** Example

Suppose I have a library defined with this header =tstlib.h=:

#+BEGIN_SRC C
#pragma once
void func_f_lib(void);
void func_g_lib(void);
#+END_SRC

and this implementation =tstlib.c=:

#+BEGIN_SRC C
#include "tstlib.h"
void func_f_lib(void)
{
    func_g_lib();
}
void func_g_lib(void)
{
}
#+END_SRC

Note that =func_f_lib()= and =func_g_lib()= are both external symbols. I have an
executable that uses this library defined like this =tst.c=:

#+BEGIN_SRC C
#include "tstlib.h"
void func_f_main(void)
{
}
void main(void)
{
    func_f_main();
    func_f_lib();
}
#+END_SRC

Note that =func_f_main()= is an external symbol as well.

If linking with =-Bdynamic= (the default for pretty much everybody), the
internal =func_g_lib()= and =func_g_main()= calls use the PLT like external
calls, and thus ltrace says:

#+BEGIN_EXAMPLE
$ gcc -Wl,-Bdynamic -shared -fPIC -g -o tstlib.so tstlib.c

$ gcc -Wl,-rpath=$PWD -g -o tst tst.c tstlib.so

$ ltrace -x 'func*' -L ./tst

func_f_main()                             = <void>
func_f_lib@tstlib.so( <unfinished ...>
func_g_lib@tstlib.so()                    = <void>
<... func_f_lib resumed> )                = <void>
+++ exited (status 163) +++


$ ltrace -e 'func*' ./tst

tst->func_f_lib( <unfinished ...>
tstlib.so->func_g_lib()                   = <void>
<... func_f_lib resumed> )                = <void>
+++ exited (status 163) +++


$ ltrace -l tstlib.so ./tst

tst->func_f_lib( <unfinished ...>
tstlib.so->func_g_lib()                   = <void>
<... func_f_lib resumed> )                = <void>
+++ exited (status 163) +++
#+END_EXAMPLE

By contrast, if linking the shared library with =-Bsymbolic=, then the internal
=func_g_lib()= call bypasses the PLT, and ltrace says:

#+BEGIN_EXAMPLE
$ gcc -Wl,-Bsymbolic -shared -fPIC -g -o tstlib.so tstlib.c

$ gcc -Wl,-rpath=$PWD -g -o tst tst.c tstlib.so

$ ltrace -x 'func*' -L ./tst

func_f_main() = <void>
func_f_lib@tstlib.so( <unfinished ...>
func_g_lib@tstlib.so()                    = <void>
<... func_f_lib resumed> )                = <void>
+++ exited (status 163) +++


$ ltrace -e 'func*' ./tst

tst->func_f_lib()                         = <void>
+++ exited (status 163) +++


$ ltrace -l tstlib.so ./tst

tst->func_f_lib()                         = <void>
+++ exited (status 163) +++
#+END_EXAMPLE

Note that the =-x= traces are the same in both cases, since =-x= traces local
/and/ external calls. However =-e= and =-l= trace only external calls, so with
=-Bsymbolic=, local calls to =func_g_lib()= do not appear there.

** DONE Reading DWARF prototypes in ltrace (part 2)               :tools:dev:
   CLOSED: [2014-07-10 Thu 01:20]

As mentioned [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Reading DWARF prototypes in ltrace")){/lisp}][earlier]], I'm adding functionality to ltrace to read function
prototypes from DWARF debugging information. The bulk of this work was merged
upstream. I'm now hunting corner cases and various details in this whole system
before moving on to implement more features. Unsurprisingly, trying to trace
calls in libc is a rich source of corner cases. Some of these are discussed here
in no particular order.

*** Missing features
    
Ltrace currently chokes (crashes!) when encountering prototypes with particular
features. Some of these are

- Complex numbers
- =void= variables
- =union= fields
- bit fields

Most of the time these aren't used, but glibc has them somewhere, and ltrace can
get confused when the new DWARF-reading code parses glibc.


*** C++ symbol names

Some DWARF symbol DIEs have a =DW_AT_linkage_name= tag in addition to the normal
=DW_AT_name= tag. The purpose of this wasn't entirely obvious until I tried to
ltrace a C++ program. Suppose I have this trivial C++ program:

=tst.cc=
#+BEGIN_SRC C
class C
{
    void f(void);
};

void C::f(void)
{
}
#+END_SRC

I compile it, and dump the debug info:

#+begin_example

$ g++ -g -o tst.o -c tst.cc && readelf -w tst.o
....
 <2><37>: Abbrev Number: 3 (DW_TAG_subprogram)
    <38>   DW_AT_external    : 1
    <38>   DW_AT_name        : f
    <3a>   DW_AT_decl_file   : 1
    <3b>   DW_AT_decl_line   : 3
    <3c>   DW_AT_linkage_name: (indirect string, offset: 0x4e): _ZN1C1fEv
    <40>   DW_AT_declaration : 1
    <40>   DW_AT_object_pointer: <0x44>
....
#+end_example

Note that for my method =f= the =DW_AT_name= is =f=, but the
=DW_AT_linkage_name= is =_ZN1C1fEv=. The linker does not know C++, and it only
seems symbol names. Here this symbol name is the mangled =_ZN1C1fEv=, so as far
as ltrace is concerned, this is the name of this function and thus it should use
=DW_AT_linkage_name= here. One could think that the parsing rule in ltrace
should be "use =DW_AT_linkage_name= if it exists, otherwise use
=DW_AT_linkage_name=". One would be wrong, since the next section shows that
this logic is too simple.

*** Aliased symbols (different symbol, same address)

Trying to ltrace this simple program doesn't work when reading the DWARF
prototypes automatically:

=tst.c=
#+begin_src C
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <time.h>
int main(void)
{
    nanosleep( &(struct timespec){.tv_sec=0,.tv_nsec=33}, NULL);
    usleep(44);
    return 0;
}
#+end_src

I get this:

#+begin_example
$ gcc -o tst tst.c
$ ltrace -l 'libc.so*' -L ./tst
tst->__libc_start_main(0x40054d, 1, 0x7fffc04253f8, 0x400590 <unfinished ...>
tst->nanosleep(0x7fffc0425300, 0, 0x7fffc0425408, 0)                 = 0
tst->usleep(44)                                                      = <void>
+++ exited (status 0) +++
#+end_example

Note that the =nanosleep()= call does not have the correct prototype. This is
because we call =nanosleep()=, but the DWARF defines =__nanosleep= and
=__GI___nanosleep=:

#+begin_example
$ nm -D tst | grep nanosleep

                 U nanosleep


$ nm -D /lib/x86_64-linux-gnu/libc-2.18.so | grep nanosleep

00000000000f26f0 T __clock_nanosleep
00000000000b7070 W __nanosleep
00000000000f26f0 W clock_nanosleep
00000000000b7070 W nanosleep


$ readelf -w /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.18.so | grep nanosleep

    <20c7cf>   DW_AT_name        : (indirect string, offset: 0x13a95): __nanosleep
    <20c7d5>   DW_AT_linkage_name: (indirect string, offset: 0x13a90): __GI___nanosleep
    <280d67>   DW_AT_name        : (indirect string, offset: 0x13a95): __nanosleep
    <280d6d>   DW_AT_linkage_name: (indirect string, offset: 0x13a90): __GI___nanosleep
    <2dc871>   DW_AT_name        : (indirect string, offset: 0x1d940): __clock_nanosleep
    <3b0b59>   DW_AT_name        : (indirect string, offset: 0x13a95): __nanosleep
    <3b0b5f>   DW_AT_linkage_name: (indirect string, offset: 0x13a90): __GI___nanosleep
#+end_example

We can resolve this discrepancy by noting that the =nanosleep= symbol in the
libc symbol table has the same address as =__nanosleep=, and use =__nanosleep='s
DWARF prototype. I implemented this, and the patch is currently in review.

*** Aliased addresses (same symbol, different address)

Testing further, I discovered that in the libc on my machine (Debian/sid amd64)
some symbols appear at multiple addresses:

#+begin_example
$ nm -D /lib/x86_64-linux-gnu/libc-2.18.so | awk '{print $NF}' | sort | uniq -d
_sys_errlist
_sys_nerr
_sys_siglist
memcpy
nftw
nftw64
posix_spawn
posix_spawnp
pthread_cond_broadcast
pthread_cond_destroy
pthread_cond_init
pthread_cond_signal
pthread_cond_timedwait
pthread_cond_wait
realpath
regexec
sched_getaffinity
sched_setaffinity
sys_errlist
sys_nerr
sys_sigabbrev
sys_siglist
#+end_example

This can make the DWARF parser confused. Looking into it, it looks like those
are versioned symbols, with different implementation for different libc
versions. This same-symbol-different-address idea doesn't fit into the data
structures, as I've currently defined them. Currently I simply take the first
such symbol I encounter and ignore the rest. I probalby should parse this out
fully, but it hardly seems worth the effort.

** DONE Closures and cookies in C                                       :dev:
   CLOSED: [2014-07-17 Thu 17:43]

I recently discovered something that's /very/ old news to functional programming
people, but was new to me, especially as being applied to C. It is common
practice in C coding to use callback functions when a general routine needs to
do something specific that the caller knows about. Furthermore, a =void* cookie=
is generally used to pass context to this callback.

Here's a simple C99 example of a linked-list structure and a iterator that calls
a callback function for each element of the list. The iterator takes a =cookie=
argument, which it passes on untouched to the callback. This =cookie= means
something to the caller of the iterator and to the callback, but it means
nothing to the iterator itself. This is the usual idiom. Furthermore, the
example contains a simple use of this iterator, to print all node values to a
particular =FILE=:

#+begin_src c
#include <stdio.h>

struct node
{
    int x;
    struct node* next;
};

typedef void (*foreach_callback)(const struct node* node, void* cookie);

static void foreach(const struct node* list,
                    const foreach_callback cb, void* cookie)
{
    while(list != NULL)
    {
        cb(list, cookie);
        list = list->next;
    }
}




static void print_node(const struct node* node, FILE* fp)
{
    fprintf(fp, "%d\n", node->x);
}

static void print_nodes(const struct node* list, FILE* fp)
{
    foreach(list, (foreach_callback)print_node, fp);
}

int main(void)
{
    struct node list =
        {.x = 10,
         .next = &(struct node){.x = 11,
                                .next = &(struct node){.x = 12,
                                                       .next = NULL}}};

    print_nodes(&list, stdout);
    return 0;
}
#+end_src

This works fine, and things have been done this way for a very long time. As
written, =print_node()= is visible to most of the source file, even though it is
only used by =print_nodes()=. It would be nice if =print_node()= was visible
/only/ from that one function that uses it. This is not possible in standard C.
However GCC has a [[https://gcc.gnu.org/onlinedocs/gcc/Nested-Functions.html][non-standard extension]] that allows such things: nested
functions. With that in mind, we can rewrite the example like so:

#+begin_src c
#include <stdio.h>

struct node
{
    int x;
    struct node* next;
};

typedef void (*foreach_callback)(const struct node* node, void* cookie);

static void foreach(const struct node* list,
                    const foreach_callback cb, void* cookie)
{
    while(list != NULL)
    {
        cb(list, cookie);
        list = list->next;
    }
}




static void print_nodes(const struct node* list, FILE* fp)
{
    void print_node(const struct node* node, FILE* fp)
    {
        fprintf(fp, "%d\n", node->x);
    }


    foreach(list, (foreach_callback)print_node, fp);
}

int main(void)
{
    struct node list =
        {.x = 10,
         .next = &(struct node){.x = 11,
                                .next = &(struct node){.x = 12,
                                                       .next = NULL}}};

    print_nodes(&list, stdout);
    return 0;
}
#+end_src

That's nicer. =print_nodes()= is now self-contained, and none of its
implementation details leak out. At this point we're not using the nested
function as anything more than just syntactic sugar. However, these aren't
simply /nested functions/; they're full /closures/, i.e. the nested function has
access to the local variables of its surrounding scope. This means that
=print_node()= can see the =fp= argument to =print_nodes()=, and doesn't need to
be passed it. Thus *we do not need the cookie!* The state maintained by the
nested function takes on the work that the cookie did for us previously. As
such, we can rewrite this example with no cookies at all:


#+begin_src c
#include <stdio.h>

struct node
{
    int x;
    struct node* next;
};

typedef void (*foreach_callback)(const struct node* node);

static void foreach(const struct node* list, const foreach_callback cb)
{
    while(list != NULL)
    {
        cb(list);
        list = list->next;
    }
}




static void print_nodes(const struct node* list, FILE* fp)
{
    void print_node(const struct node* node)
    {
        fprintf(fp, "%d\n", node->x);
    }


    foreach(list, print_node);
}

int main(void)
{
    struct node list =
        {.x = 10,
         .next = &(struct node){.x = 11,
                                .next = &(struct node){.x = 12,
                                                       .next = NULL}}};

    print_nodes(&list, stdout);
    return 0;
}
#+end_src


Neat! Clearly this is non-portable. It also is potentially unsafe since
internally the compiler generates a bit of code on the stack and runs it. Still,
the code looks nicer and we don't need cookies.

** DONE Debian cross-gcc snapshot packages                        :tools:dev:
   CLOSED: [2014-07-20 Sun 01:55]

#+begin_o_blog_alert info Note
This is largely out-of-date. These packages are now in Debian/unstable, and I
shut down the server described here in favor of the packages in the main
archive.
#+end_o_blog_alert

For a while, getting a cross-compiler on Debian was much more complicated than
simply running an =apt-get install=. The /current/ situation is that
cross-compilers are mostly ready-to-build, but for uninteresting reasons,
packages are still not available in the main Debian repository. I am now
building these packages every week, and hosting them on my APT server:

http://toolchains.secretsauce.net/

I attempt to build compilers targeting

- armel
- armhf
- mips
- mipsel
- powerpc

Compilers for the following languages are built:

- C
- C++
- Fortran
- Java
- Go
- Objective C
- Objective C++

Hopefully we'll get cross-compilers into Debian proper at some point, so that
using my unofficial repo becomes unnecessary

** DONE Decoding P25 with RTL-SDR on Debian                             :SDR:
   CLOSED: [2014-07-25 Fri 17:31]

I wanted to play with an [[http://en.wikipedia.org/wiki/Software_defined_radio][software-defined radio]] for a while now. A simple one
can be had for about $10 by repurposing a USB TV adapter:
http://sdr.osmocom.org/trac/wiki/rtl-sdr. I bought one, and looked into using it
as a police scanner. Suprisingly to me, there's quite a bit of information out
there about the protocols and frequencies used by LAPD:
http://harrymarnell.net/lapd-freqs.htm. So they're using [[http://en.wikipedia.org/wiki/P25][P25]]-encoded digital
signals, which are unencrypted, apparently.

There are some guides out there on how to decode these with an RTL-SDR, but
they're all highly Windows-centric (and look like a pain in the butt, to be
honest). This post is a set of notes on getting this working on a Debian box.

*** Obtaining the tools

The core RTL-SDR libraries, GNU Radio and UI tools such as [[http://gqrx.dk/][GQRX]] are already in
Debian, so getting them is trivial. There is a tool for decoding P25, =dsd=;
it's not in Debian, so we have to build it. First we get and build =mbelib=, a
library it uses. We check out the code, roll bakc to the latest tag and build:


#+begin_example

dima@shorty:/tmp$ git clone https://github.com/szechyjs/mbelib
...
dima@shorty:/tmp$ cd mbelib

dima@shorty:/tmp/mbelib$ git tag -l
v1.2.1
v1.2.3
v1.2.4
v1.2.5

dima@shorty:/tmp/mbelib$ git reset --hard v1.2.5
HEAD is now at 316bab6 Bump version to v1.2.5

dima@shorty:/tmp/mbelib$ mkdir build

dima@shorty:/tmp/mbelib$ cd build

dima@shorty:/tmp/mbelib/build$ cmake ..
...

dima@shorty:/tmp/mbelib/build$ make
...
Linking C static library libmbe.a

#+end_example

OK. We built =mbelib=, now we can build =dsd=. Same as before, except we tweak
the =Makefile= to find and use the library we just built, and to use the
statically-linked version so that we don't need to mess with RPATHs.

#+begin_example

dima@shorty:/tmp$ git clone https://github.com/szechyjs/dsd
...

dima@shorty:/tmp$ cd dsd

dima@shorty:/tmp/dsd$ git tag -l
v1.3
v1.4.1
v1.6.0

dima@shorty:/tmp/dsd$ git reset --hard v1.6.0
HEAD is now at 5d147c9 version 1.6.0

dima@shorty:/tmp/dsd$ perl -p -i -e 's{/usr/local/include}{/tmp/mbelib/}g; s{-lmbe}{/tmp/mbelib/build/libmbe.a}' Makefile

dima@shorty:/tmp/dsd$ make
...
gcc -O2 -Wall -o dsd dsd_main.o dsd_symbol.o dsd_dibit.o dsd_frame_sync.o dsd_file.o dsd_audio.o dsd_serial.o dsd_frame.o dsd_mbe.o dsd_upsample.o p25p1_hdu.o p25p1_ldu1.o p25p1_ldu2.o p25p1_tdulc.o p25_lcw.o x2tdma_voice.o x2tdma_data.o dstar.o nxdn_voice.o nxdn_data.o dmr_voice.o dmr_data.o provoice.o -L/usr/local/lib -lm /tmp/mbelib/build/libmbe.a 

#+end_example

*** Decoding the stream

Now we can think about listening in. The overall data flow is

- Tune in, demodulate the narrow-band FM signal into a 48KHz-sampled signal
- Use =dsd= to decode this 48KHz-sampled signal to produce 8KHz-sampled audio

**** FM

There are several basic tools one can use for this. I'd prefer to use the
commandline =rtl_sdr= or =rtl_fm= from the [[https://packages.debian.org/sid/rtl-sdr][rtl-sdr]] Debian package. The issue I
ran into was that we're tuning into a relatively narrow-band signal, so the
tuning is sensitive, and small tuning errors make you miss the signal you want
entirely. RTL-SDR is a cheap device, and its tuning inaccuracy alone is enough
to break this. There exists an [[https://github.com/steve-m/kalibrate-rtl][RTL-SDR calibration tool]] to compensate for the
hardware inaccuracy, but I still wasn't able to successfully tune into the
frequencies, as defined in the LAPD channel list linked above. I didn't push on
this very hard, so this could very well be my fault.

So instead of the commandline tools, I ended up GQRX. Pretty much all the LAPD
frequencies are in the 484MHz range or the 506MHz range. I set the tuner into
the right neighborhood, then the FFT waterfall plot in GQRX visually shows you
which frequencies are active. You can roughly tune in simply by looking at the
plot, and you can fine-tune by listening to the demodulated signal, trying to
find the characteristic digital buzz and no static. There are multiple
digital-sounding channels and multiple types of encoding are present (sound
different). You can play around to find a signal that =dsd= knows how to decode.
Note that since we're now looking for channels empirically, we compensate for
tuning inaccuracies, but the LAPD frequency list becomes useless, and we don't
even know what specifically we're listening to.

The GQRX window looks like this:

[[file:files/SDR/gqrx_dsd.png]]

We're clearly listening to an active transmission: we're tuned to the channel
indicated by the red line, and the waterfall plot shows intermittent activity
there. The signal is intermittent because the transmitter is only active when
there's data to send, i.e. when the human talking into the radio is pressing the
button.

**** P25

We now need to get the data out of GQRX and into =dsd=. =dsd= wants to get its
input from (and send its output to) =/dev/audio=. Even if my input was coming
from a sound device, it wouldn't be =/dev/audio= on my box. That's a holdover
from some ancient system that ALSA doesn't provide by default, and I want to
avoid it if possible. Turns out =dsd= just looks at raw samples, so we can
simply send it appropriately-formatted bits (16 bits per sample, little endian,
48KHz sample rate). GQRX has several export capabilities, one of them being raw
UDP output. This is perfect for this application, and I turn on that GQRX mode
by pressing the appropriate button (bottom of the screenshot; two computers are
pictured).

We now have raw 48KHz samples coming out on UDP port 7355. We can make a named
pipe, or better yet, we can pass the data to =dsd= on standard input:

#+begin_src sh
socat UDP-RECV:7355 - | ./dsd -i /dev/stdin
#+end_src

Almost done. We can now tune interactively with GQRX and decode the demodulated
FM data on the fly with =dsd=. =dsd= says lots of stuff about signals it's
receiving. When successfully decoding audio, I see things like this:

#+begin_example
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  466 src:   180359 tg:     1  LDU1  e:========================
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  467 src:   180359 tg:     1  LDU2  e:=========
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  466 src:   180359 tg:     1  LDU1  e:=====
Sync:  +P25p1     mod: GFSK inlvl:  7% nac:  466 src:   180359 tg:     1  LDU2  e:======R================R=========
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  466 src:   180359 tg:     1  LDU1  e:==================
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  466 src:   180359 tg:     1  LDU2  e:===
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  466 src:   180359 tg:     1  LDU1  e:=====================
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  466 src:  1228951 tg:     1  LDU2  e:========
Sync:  +P25p1     mod: C4FM inlvl:  7% nac:  466 src:  1228951 tg:     1  LDU1  e:====
Sync:  +P25p1     mod: GFSK inlvl:  7% nac:  466 src:   180359 tg:     1  LDU2  e:==================
#+end_example

We still can't hear the results because =dsd= doesn't write them anywhere
useful. We can send those to ALSA with a named pipe and =aplay=:

#+begin_src sh
# In one shell
mkfifo /tmp/pipe
socat UDP-RECV:7355 - | ./dsd -i /dev/stdin -o /tmp/pipe

# In another shell, in parallel
aplay -r 8000 -f S16_LE -t raw -c 1 < /tmp/pipe
#+end_src

So this setup works for me. Now one can go back, and fix stuff; stuff like
inaccurate tuning. It'd be nice to automatically tune into valid channels, of
better yet to follow the trunking signals, but that's more work than I'm willing
to put into this.


*** Commandline-only listening (no GQRX)

Once we find a channel we like, using GQRX to interactively tune the radio (as
described above), we can run the whole pipeline with one command (possibly
zsh-only):

#+begin_src sh
./dsd -f1 -mc -i <(rtl_fm -F1 -o4 -g32 -f 484.7918M -s48000 -) -o >(aplay -r 8000 -f S16_LE -t raw -c 1)
#+end_src

Here 484.7918Mhz is the frequency I found by poking around with GQRX. =-F1 and
-o4= are tuning parameters (possibly highly suboptimal). The =-f1 -mc= options
to =dsd= indicate what signal should be expected; this would vary if listening
to something other than LAPD. Seems to work. And the total CPU comsumption is
about 1/3 of what gqrx requires.

** DONE Org-mode for invoices                                         :emacs:
   CLOSED: [2014-10-01 Wed 10:11]

Like many people I work as a contractor, so I need to keep track of my hours and
to generate monthly invoices. Org-mode already has a [[http://orgmode.org/manual/Clocking-work-time.html][nice way to track hours]] and
a way to [[http://orgmode.org/manual/Exporting.html][export to a PDF]]. What's missing is a reasonable way to bridge those two
functions, to generate invoices, and the way I do this is described in this
post.

The main approach here is to maintain a =timecard.org= file that contains all
the timing information. As I work, I clock in and out of tasks in this file.
Then when an invoice needs to be prepared I simply export this file as a PDF,
and I get out a finished invoice. Before I get into the details, here's a sample
=timecard.org=:

#+BEGIN_SRC org :exports code

 #+STARTUP: showall
 #+LaTeX_CLASS_OPTIONS: [letterpaper,10pt]
 #+LATEX_HEADER: \usepackage[letterpaper,tmargin=0.5in,bmargin=1.0in,lmargin=1.0in,rmargin=1.0in,headheight=0in,headsep=0in,footskip=0.0in]{geometry}
 #+LATEX_HEADER: \usepackage{lmodern}
 #+LATEX_HEADER: \usepackage[labelformat=empty,textformat=empty]{caption}
 #+LATEX_HEADER: \parindent 0in
 #+LATEX_HEADER: \parskip 0.1in
 #+LATEX_HEADER: \setlength\LTleft{0pt} \setlength\LTright\fill
 #+OPTIONS: toc:nil num:nil 
 #+AUTHOR:
 #+DATE:
 #+TITLE: INVOICE


 #+CONSTANTS: rate=100.0

 #+BEGIN_LATEX
 \thispagestyle{empty}
 #+END_LATEX


 * Invoice number: 1
 * Invoice date: [2014-09-01 Mon]

 | / | <                    | >                     |
 |---+----------------------+-----------------------|
 |   | *Contractor*         | *Client*              |
 |---+----------------------+-----------------------|
 |   | Billy Bob Johnson    | WidgetWorks Inc       |
 |   | 21 N. First Ave      | 12 Main St.           |
 |   | Widgettown, CA 91234 | Gadgetville, CA 91235 |
 |   | william@isp.net      |                       |
 |---+----------------------+-----------------------|

 #+NAME: summary
 | / |            |             |
 |   | *Rate*     | *Total due* |
 |   | $100.00    | _$941.67_   |
 | ^ | ratetarget | totaltarget |
 #+TBLFM: $ratetarget=$rate;$%.2f::$totaltarget=remote(clocktable, "@II$>");_$%.2f_


 *Please make checks payable to _William Johnson_*

 #+TBLNAME: clocktable
 #+BEGIN: clocktable :maxlevel 3 :tcolumns 4 :scope file :block 2014-08 :narrow 60
 #+CAPTION: Clock summary at [2014-09-01 Mon 09:44], for August 2014.
 | <60>                                                         |        |      |      |           |
 | Headline                                                     | Time   |      |      | Amount($) |
 |--------------------------------------------------------------+--------+------+------+-----------|
 | *Total time*                                                 | *9:25* |      |      |    941.67 |
 |--------------------------------------------------------------+--------+------+------+-----------|
 | Tasks                                                        | 9:25   |      |      |    941.67 |
 | \__ Foo the bar                                              |        | 3:20 |      |    333.33 |
 | \_____ Implementing foo                                      |        |      | 3:20 |    333.33 |
 | \__ Frobnicate the baz                                       |        | 6:05 |      |    608.33 |
 #+TBLFM: @3$5..@>$5=vsum($2..$4)*$rate;t::@2$5=string("Amount($)")
 #+END:



 * Tasks                                                            :noexport:
 ** Foo the bar
 *** Meeting about the nature of bar
     CLOCK: [2014-09-07 Sun 00:24]--[2014-09-07 Sun 01:00] =>  0:36
 *** Implementing foo
     CLOCK: [2014-08-28 Thu 19:40]--[2014-08-28 Thu 23:00] =>  3:20
 ** Frobnicate the baz
     CLOCK: [2014-08-25 Mon 10:55]--[2014-08-25 Mon 17:00] =>  6:05


 * local lisp stuff                                                 :noexport:
 Local Variables:
 eval: (progn
   (set (make-local-variable 'org-time-clocksum-format)
        '(:hours "%d" :require-hours t :minutes ":%02d" :require-minutes t))
   (setq org-latex-tables-centered nil
         org-latex-default-table-environment "longtable")
   (local-set-key
    (kbd "<f5>")
    (lambda ()
      (interactive)
      ;;
      ;; Bump up the invoice number
      (beginning-of-buffer)
      (re-search-forward "Invoice number: \\([0-9]+\\)")
      (let ((n (string-to-number (match-string 1))))
        (kill-region (match-beginning 1) (match-end 1))
        (insert (format "%d" (1+ n))))
      ;;
      ;; Set the invoice date
      (beginning-of-buffer)
      (re-search-forward "Invoice date: *")
      (kill-region (point)
                   (save-excursion
                     (end-of-line) (point)))
      (org-insert-time-stamp (current-time) nil t)
      ;;
      ;;
      ;; Update the main clock table
      (beginning-of-buffer)
      (search-forward "#+BEGIN: clocktable")
      ;;
      ;; Here an advice is needed to make sure the Amount column is added
      ;; This advice is made unnecessary by this patch:
      ;; http://lists.gnu.org/archive/html/emacs-orgmode/2014-10/msg00002.html
      (unwind-protect
          (progn
            (defadvice org-table-goto-column
                (before
                 always-make-new-columns
                 (n &optional on-delim force)
                 activate)
              "always adds new columns when we move to them"
              (setq force t))
            ;;
            (org-clocktable-shift 'right 1))
        ;;
        (ad-deactivate 'org-table-goto-column))
      ;;
      ;; Update the summary table
      (beginning-of-buffer)
      (search-forward "| totaltarget")
      (org-table-recalculate t))))
 End:

#+END_SRC

As described in the [[http://orgmode.org/manual/Clocking-work-time.html][manual]], the time-tracking in org works with the user
producing an outline of tasks and then clocking in and out of them. Org can then
generate a table to summarize the hours spent on various tasks in a particular
period.

The =timecard.org= has

- some Latex commands to determine how the invoices look
- some specific information for /that/ invoice (invoice number, date, totals)
- the clock-table that contains the time and $ summaries
- the task outline that the user clocks in and out of
- some emacs lisp to update all these things in unison

The contracting rate is defined in a =#+CONSTANTS= at the top of the file. The
task outline itself is /not/ exported, but the clock-table it produces is. The
clock-table that org makes contains only timing information and nothing about
money, so I have a formula in that table that adds a column about how much each
task cost.

Every month before generating a PDF the data all needs to be updated, and the
emacs-lisp at the bottom does that. It's a bit more complicated than I'd like it
to be, but it works well. It exports the new clock-table, updates the totals,
the data, invoice number, etc. Normally I simply hit =F5=, and this invokes the
lisp and updates everything. The end-result currently looks like [[file:files/invoice/invoice.pdf][this]].

Not the prettiest thing in the world, but it serves the purpose just fine. One
could fine-tune the org exporter far more to generate prettier invoices if they
care enough to do so.

** DONE ZNC and ERC                                             :emacs:tools:
   CLOSED: [2014-10-08 Wed 14:16]

I use [[http://www.gnu.org/software/emacs/manual/erc.html][ERC]] in Emacs as my IRC client. An extremely common issue I have is that
when my laptop disconnects from the IRC server (due to the laptop going to sleep
or due to it losing a network connection), the IRC communication that happened
during that time is lost. People had this issue since the dawn of time, and a
solution exists: IRC bouncers. You run this on a dedicated server, and it acts
as a proxy between the real IRC server and your machine with its flaky network.
When you connect to the bouncer, it spews everything you missed at you. I set up
a bouncer yesterday, and it was surprisingly painful. This post summarizes the
procedure.

I'm using the [[http://www.znc.in][ZNC]] bouncer. There are others, and I'm not using this one for any
particular reason. I installed it from Debian/sid, so I'm running version
=1.4-1+b2=. Debian/wheezy has a much older package (=0.206-2=); I don't know if
it would be hugely problematic to use this older ZNC.

ZNC refuses to run as root, so I made an arbitrary user for it: =zncman=. The
procedure you're supposed to follow is

- use the =znc= binary to interactively generate a configuration
- use interactive IRC commands to configure servers and such

This is a pain in the butt. A slightly easier method:

- log in as =zncman=
- use the interactive configurator (=znc --makeconf=) to generate an arbitrary
  configuration
- kill =znc=
- edit the configuration
- restart =znc=

The configuration syntax is nominally described at
http://en.znc.in/wiki/Configuration, but this document is poor at best. This is
probably a big reason why upstream doesn't recommend editing this file. It's
much easier, though. Here's my =~/.znc/configs/znc.conf= file. It's pretty clear
what needs to be customized:

#+BEGIN_EXAMPLE
// WARNING
//
// Do NOT edit this file while ZNC is running!
// Use webadmin or *controlpanel instead.
//
// Altering this file by hand will forfeit all support.
//
// But if you feel risky, you might want to read help on /znc saveconfig and /znc rehash.
// Also check http://en.znc.in/wiki/Configuration

AnonIPLimit = 10
ConnectDelay = 5
#LoadModule = webadmin
LoadModule = fail2ban
MaxBufferSize = 500
ProtectWebSessions = true
ServerThrottle = 30
Skin = _default_
StatusPrefix = *
Version = 1.4

<Listener listener0>
        AllowIRC = true
#       AllowWeb = true
        IPv4 = true
        IPv6 = true
        Port = 5555
        SSL = false
</Listener>

<User dima>
        Admin = true
        Allow = *
        AltNick = dima_
        AppendTimestamp = true
        AutoClearChanBuffer = true
        Buffer = 50000
        ChanModes = +stn
        DenyLoadMod = false
        DenySetBindHost = false
        Ident = dima
        JoinTries = 10
#       LoadModule = webadmin
        MaxJoins = 0
        MaxNetworks = 1
        MultiClients = true
        Nick = dima
        PrependTimestamp = true
        QuitMsg = ZNC - http://znc.in
        RealName = Got ZNC?
        TimestampFormat = [%H:%M:%S]

        <Network oftc>
                Nick = dima5
                FloodBurst = 4
                FloodRate = 1.00
                IRCConnectEnabled = true
                Server = irc.oftc.net 6667
        </Network>

        <Network freenode>
                Nick = dima5
                FloodBurst = 4
                FloodRate = 1.00
                IRCConnectEnabled = true
                Server = irc.freenode.net 6667
        </Network>

        <Pass password>
                Hash = xxxxx
                Method = yyyyy
                Salt = zzzzz
        </Pass>
</User>

#+END_EXAMPLE

So with that file, ZNC runs an IRC server on port 5555 and the ZNC user is
called =dima=. ZNC itself talks to oftc and freenode using the Nick =dima5= for
both. The Debian package for ZNC is not currently integrated into the init
system, so you start the daemon with just =znc=. Clearly this doesn't happen on
every boot, and you have to set that up yourself. I haven't bothered yet, but
it'll come up at some point.

When logging into the ZNC IRC server, you have to be very clean on what is the
ZNC nick and what is the IRC server nick. Same with passwords. On top of that,
the ZNC password contains the ZNC nick, the target server and the ZNC password.
So in my case I start up ERC thusly:

#+BEGIN_SRC emacs-lisp
  (erc :server "my_znc_server.com" :port 5555 :nick "dima5" :password "dima/freenode:zncpassword")
  (erc :server "my_znc_server.com" :port 5555 :nick "dima5" :password "dima/oftc:zncpassword")
#+END_SRC

This all does the right thing. Currently there's an issue in that
=erc-autojoin-channels-alist= now sees both the freenode and oftc connection as
the same one (since it uses the hostname to differentiate, and the hostname is
now =my_znc_server.com= for both). Otherwise, this works fine.

** DONE Mu4e and desktop notifications                  :emacs:tools:desktop:
   CLOSED: [2014-12-12 Fri 20:54]

[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Even better notifications")){/lisp}][Some previous posts]] talked about desktop notifications via the X11 urgency hint.
Here I talk specifically about how I handle mail notifications specifically.

I use [[http://www.djcbsoftware.nl/code/mu/mu4e.html][mu4e]] to read mail in Emacs. This is great for reading mail, but it leaves
some tasks to external tools (I consider this a good thing):

- interacting with mail servers instead of a local [[http://en.wikipedia.org/wiki/Maildir][maildir]]
- automatically receiving mail

The former is generally done with [[http://offlineimap.org][offlineimap]] or [[http://isync.sourceforge.net/mbsync.html][mbsync]]. For the latter,
everybody seems to do something different.

To handle main receiving and notifications I use the [[http://en.wikipedia.org/wiki/IMAP_IDLE][IMAP IDLE]] command to handle
event-based (/not/ timer-based) receiving of mail, and I use the X11 UrgencyHint
(described for instance here: [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "X11 urgency hint and notifications")){/lisp}][X11 urgency hint and notifications]]) to notify me
of incoming messages. As any other UrgencyHint-based system, support from one's
window manager is crucial. I use [[http://notion.sourceforge.net][notion]] as my WM, which has /fantastic/ support
for this, so this all works /really/ well for me.

If I want event-based notifications, I run this =mailalert.pl= script:

#+begin_src perl
#!/usr/bin/perl
use strict;
use warnings;
use feature qw(say);

use Mail::IMAPClient;
use IO::Stty;
use X11::Protocol;
use X11::Protocol::WM;
use X11::WindowHierarchy;
use IO::Socket::SSL 'SSL_VERIFY_PEER';

use Getopt::Euclid;

use autodie;

use Parallel::ForkManager;

my $passwd = getPassword();

my %imapOptions    =
  ( Server         => 'mail.XXXXX.com',
    User           => 'USERNAME',
    Password       => $passwd,
    Ssl            => 1,
    Socketargs     => [SSL_verify_mode => SSL_VERIFY_PEER],
    Debug          => 0,
    Uid            => 1,
    Keepalive      => 1,
    Reconnectretry => 3,
 );

my $folders = getFolders( \%imapOptions );

say STDERR "Got folder list. Idling on each";

my $pm = new Parallel::ForkManager(1000);
for my $folder (@$folders)
{
  my $pid = $pm->start and next;

  my $imap = Mail::IMAPClient->new(%imapOptions) or die "Couldn't connect to imap for folder '$folder'";
  $imap->select($folder)                         or die "Couldn't select '$folder'";

  my %ids_saw;
  while(1)
  {
    my @unseen = $imap->unseen;
    my @newsubjects;
    for my $id( @unseen )
    {
      next if $ids_saw{$id};
      $ids_saw{$id} = 1;
      push @newsubjects, $imap->subject($id);
    }

    if( @newsubjects )
    {
      alert();
      my $date = `date`;
      chomp $date;
      say "$date; Pid $pid Saw unread email in '$folder':";
      print join('', map {$_ //= ""; "  $_\n"} @newsubjects);

      sleep(60*$ARGV{'--min'}) if $ARGV{'--min'};
    }

    my $IDLEtag     = $imap->idle           or die "idle failed: $@ for folder '$folder'\n";
    my $idle_result = $imap->idle_data(250) or die "idle_data failed: $@ for folder '$folder'\n";
    $imap->done($IDLEtag)                   or die "idle done failed: $@ for folder '$folder'\n";;
  }

  $pm->finish;
}
$pm->wait_all_children;



sub getPassword
{
  say "Enter password: ";

  my $stty_old = IO::Stty::stty(\*STDIN,'-g');
  IO::Stty::stty(\*STDIN,'-echo');
  $passwd = <>;
  IO::Stty::stty(\*STDIN,$stty_old);

  chomp $passwd;

  return $passwd;
}

sub getFolders
{
  my $opts = shift;

  my $imap = Mail::IMAPClient->new(%$opts) or die "Couldn't connect to imap";
  say STDERR "Connected. Getting folder list.";

  my $folders = $imap->folders
    or die "List folders error: ", $imap->LastError, "\n";
  $imap->disconnect;

  return $folders;
}

sub alert
{
  # I got mail! Alert the user

  # I try to set urgency on a mu4e window if there is one. Otherwise, I set the
  # urgency on the mailalert window itself

  my $x = X11::Protocol->new()
    or die "Couldn't open X11 display";

  # by default, set it to the mailalert ID
  my $xid = $ENV{WINDOWID};

  my @mu4e = x11_filter_hierarchy( filter => qr/emacs.*mu4e/ );
  if( @mu4e )
  {
    # found some mu4e windows. Alert the first one
    $xid = $mu4e[0]{id};

    # there's a mu4e window already. Get the mail
    system(qw(emacsclient -a '' -e), '(when (get-buffer "*mu4e-main*") (mu4e-update-mail-and-index t))');
  }

  X11::Protocol::WM::change_wm_hints( $x, $xid,
                                      urgency => 1 );
}


__END__

=head1 NAME

mailalert.pl - IMAP IDLE mail checker

=head1 SYNOPSIS

 ./mailalert.pl
 ... sits there until mail comes in

=head1 DESCRIPTION

This tool uses IMAP IDLE to wait for new email without polling. When a message
comes in, it tries to find the mail window and set the urgency hint on it

=head1 OPTIONAL ARGUMENTS

=over

=item --min <minimum_interval>

If given, do not alert the user more often than this many minutes.

=for Euclid:
  minimum_interval.type: int > 0

=back

=head1 AUTHOR

Dima Kogan, C<< <dima@secretsauce.net> >>

#+end_src

This is (clearly) a perl script; all the dependencies are available in Debian:

- libgetopt-euclid-perl
- libmail-imapclient-perl
- libio-stty-perl
- libx11-protocol-perl
- libx11-protocol-other-perl
- libx11-windowhierarchy-perl

To use it, fill in the IMAP server and username details. The script then queries
one for the password when it is executed. This script connects to the IMAP
server, and waits for new mail on all folders. When new mail arrives to any
folder, it calls =alert()=, which does 2 things:

- Finds the mu4e Emacs window, and sets the UrgencyHint on it
- Tells Emacs to contact the server and download, reindex the mail

The server operations are handled by evaluating

#+begin_src emacs-lisp
(mu4e-update-mail-and-index t)
#+end_src

as Emacs Lisp. This is a mu4e function that carries out the obvious operations.
Mu4e is told how to receive the mail by a bit of configuration the Emacs init
files. In my case:

#+begin_src emacs-lisp
(setq mu4e-get-mail-command "offlineimap -q")
#+end_src

This works for me. The only real downside is that due to the way IMAP is
implemented, only a single folder can be monitored for updates at a time. The
script above thus makes a connection to the server for each folder that exists.
If you have a lot of folders, this is quite a few simultaneous connections! A
consequence of this is that my script is silly, and looks at each folder in a
separate fork, with the forks not talking to each other. Thus if mail arrives to
more than one folder at the same time (as happens often when you initially start
the script), multiple =(mu4e-update-mail-and-index t)= are made at the same
time. Fortunately, this simply generates a warning, and doesn't do anything
bad, so my script can remain silly, and I can get on with my life.

** DONE Word-oriented navigation in emacs-lisp                        :emacs:
   CLOSED: [2015-01-01 Thu 18:03]

I recently hit a [[http://debbugs.gnu.org/17558][bug]] in a recent build of GNU Emacs. The crux of the matter was
that word-relative functions such as =forward-word= now respect =subword-mode=
and =superword-mode=. This is good for interactive use, but can have unintended
consequences in existing code, which is what happened with ERC: ERC expected
=forward-word= to move consistently, but now it behaves differently if
=subword-mode= is active.

The solution seems to be to forgo =forward-mode= and friends for interactive
use, and to use more low-level functions in programs. For instance, I replaced
=(upcase-word 1)= with

#+BEGIN_SRC emacs-lisp
 (skip-syntax-forward "^w")
 (let*
     ((word-start (point))
      (word-end
       (progn (skip-syntax-forward "w") (point))))
   (upcase-region word-start word-end))
#+END_SRC

Way more verbose than I would like, but it works.

** DONE User-space introspection with Linux perf                  :tools:dev:
   CLOSED: [2015-01-28 Wed 19:42]

The Linux kernel comes with a performance counter/profiler: [[https://perf.wiki.kernel.org/index.php/Main_Page][=perf=]]. This was
originally a kernel-only tool, but it can now do some userspace stuff as well.
=perf= is very powerful, but annoyingly under-documented. Plenty was written on
the topic, so here I'll mention only some hangups and strange behaviors I've
observed trying to use it. If I figure out enough of this, this could be used to
improve the docs.

For the below I'm using =perf= version 3.16.0 from Debian package
=linux-tools-3.16= version 3.16-2 on an amd64 box running a Debian vanilla
kernel 3.16-3-amd64 package version 3.16.5-1.

*** Basic userspace introspection

Let's say I have a tiny test program: =tst.c=:

#+BEGIN_SRC C
#include <stdio.h>
#include <stdlib.h>

int func(int xxx)
{
    int zzz = xxx;

    printf("zzz: %d\n", zzz);
    return zzz+5;
}
int main(int argc, char* argv[])
{
    int i=0;
    for( i=0; i<10; i++)
        printf("yyy: %d\n", func(argc + i));

    return 0;
}
#+END_SRC

One can run it, and get the obvious results:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ gcc -g -o tst tst.c && ./tst

zzz: 1
yyy: 6
zzz: 2
yyy: 7
zzz: 3
yyy: 8
zzz: 4
yyy: 9
zzz: 5
yyy: 10
zzz: 6
yyy: 11
zzz: 7
yyy: 12
zzz: 8
yyy: 13
zzz: 9
yyy: 14
zzz: 10
yyy: 15
#+END_EXAMPLE

Building with =-g= is significant since it allows =perf= to read the DWARF
information to know about symbols and variables.

The main command to to manipulate arbitrary probes, such as userspace ones, is
=perf probe=. The main command to get information from each probe hit
individually (instead of as an aggregate) is =perf script=. Let's do some basic
probing: let's see all the function returns from =func=:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ sudo perf probe -x tst --add 'out=func%return $retval'                         

Added new event:
  probe_tst:out        (on func%return in /tmp/tst with $retval)

You can now use it in all perf tools, such as:

        perf record -e probe_tst:out -aR sleep 1


dima@shorty:/tmp$ sudo perf record -g -e probe_tst:out -aR ./tst      
zzz: 1
yyy: 6
zzz: 2
yyy: 7
zzz: 3
yyy: 8
zzz: 4
yyy: 9
zzz: 5
yyy: 10
zzz: 6
yyy: 11
zzz: 7
yyy: 12
zzz: 8
yyy: 13
zzz: 9
yyy: 14
zzz: 10
yyy: 15
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.712 MB perf.data (~31090 samples) ]


dima@shorty:/tmp$ sudo perf report --stdio                                              

Failed to open /tmp/perf-20159.map, continuing without symbols
# Samples: 10  of event 'probe_tst:out'
# Event count (approx.): 10
#
# Children      Self  Command  Shared Object                 Symbol
# ........  ........  .......  .............  .....................
#
   100.00%     0.00%      tst  libc-2.19.so   [.] __libc_start_main
                |
                --- __libc_start_main

   100.00%   100.00%      tst  tst            [.] main             
                |
                --- main
                    __libc_start_main



dima@shorty:/tmp$ sudo perf script         

Failed to open /tmp/perf-20159.map, continuing without symbols
tst 20159 [001] 629384.650622: probe_tst:out: (400506 <- 400561) arg1=0x6
                  400561 main (/tmp/tst)
            7fda0633eb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20159 [001] 629384.650653: probe_tst:out: (400506 <- 400561) arg1=0x7
                  400561 main (/tmp/tst)
            7fda0633eb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20159 [001] 629384.650675: probe_tst:out: (400506 <- 400561) arg1=0x8
                  400561 main (/tmp/tst)
            7fda0633eb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

....
#+END_EXAMPLE

So things basically work. We made a probe called =out= to fire when =func=
returns (=func%return=). We asked to record the actual return value at that time
(=$retval=). The we looked at the aggregate call graph with =perf report=.
Finally, we used =perf script= to look at detailed output for each time we hit
this probe. In particular, we get the return value, given to us as =arg1=.

*** Pitfall: incorrect reading of function argument

#+begin_o_blog_alert info Update
I hit this issue again, and looked into it. A [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Perf misreporting of user-space function arguments")){/lisp}][newer post]] describes the findings.
#+end_o_blog_alert

Let put in a probe at the start of =func=, and print out the value of the
argument:

#+BEGIN_EXAMPLE

dima@shorty:/tmp$ sudo perf probe -x tst --add 'in=func xxx'                                     

Added new event:
  probe_tst:in         (on func in /tmp/tst with xxx)

You can now use it in all perf tools, such as:

        perf record -e probe_tst:in -aR sleep 1


dima@shorty:/tmp$ sudo perf record -g -e probe_tst:in -aR ./tst      
zzz: 1
yyy: 6
zzz: 2
yyy: 7
....


dima@shorty:/tmp$ sudo perf script         

Failed to open /tmp/perf-20159.map, continuing without symbols
tst 20159 [001] 629384.650566: probe_tst:in: (400506) xxx=0
                  400506 func (/tmp/tst)
            7fda0633eb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20159 [001] 629384.650639: probe_tst:in: (400506) xxx=1
                  400506 func (/tmp/tst)
            7fda0633eb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20159 [001] 629384.650661: probe_tst:in: (400506) xxx=2
                  400506 func (/tmp/tst)
            7fda0633eb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)
....

#+END_EXAMPLE


This is wrong! It says the passed-in value is 0 then 1 then 2. Actually it's 1
then 2 then 3. Let's put in two more probes to examine this:

- Another probe at =func=, but looking at the =di= register. The calling
  convention dictates that this is where the argument would go
- A probe in the middle of =func=, after =zzz= was set. At that point, I'd like
  to look at both =xxx= and =zzz=

We ask =perf= about what it thinks line numbers are in function =func= (line
numbers here are relative to the start of the function not the start of the
file):

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ sudo perf probe -x tst -L 'func'  

<func@/tmp/tst.c:0>
      0  int func(int xxx)
      1  {
      2      int zzz = xxx;
         
      4      printf("zzz: %d\n", zzz);
      5      return zzz+5;
      6  }
         int main(int argc, char* argv[])
         {
             int i=0;
#+END_EXAMPLE

So I place my probe on line 4:

#+BEGIN_EXAMPLE

dima@shorty:/tmp$ sudo perf probe -x tst --add 'inreg=func %di'

Added new event:
  probe_tst:inreg      (on func in /tmp/tst with %di)

You can now use it in all perf tools, such as:

        perf record -e probe_tst:inreg -aR sleep 1


dima@shorty:/tmp$ sudo perf probe -x tst --add 'mid=func:4 xxx zzz'                           

Added new event:
  probe_tst:mid        (on func:4 in /tmp/tst with xxx zzz)

You can now use it in all perf tools, such as:

        perf record -e probe_tst:mid -aR sleep 1


dima@shorty:/tmp$ sudo perf record -g -e probe_tst:inreg,probe_tst:mid -aR ./tst      

zzz: 1
yyy: 6
zzz: 2
yyy: 7
....


dima@shorty:/tmp$ sudo perf script         

dima@shorty:/tmp$ sudo perf script
Failed to open /tmp/perf-20335.map, continuing without symbols
tst 20335 [000] 629951.439663: probe_tst:inreg: (400506) arg1=0x1
                  400506 func (/tmp/tst)
            7f61a293fb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20335 [000] 629951.439681: probe_tst:mid: (400517) xxx=1 zzz=1
                  400517 func (/tmp/tst)
            7fffffffe000 [unknown] (/tmp/perf-20335.map)
            7f61a293fb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20335 [000] 629951.439720: probe_tst:inreg: (400506) arg1=0x2
                  400506 func (/tmp/tst)
            7f61a293fb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20335 [000] 629951.439725: probe_tst:mid: (400517) xxx=2 zzz=2
                  400517 func (/tmp/tst)
            7fffffffe000 [unknown] (/tmp/perf-20335.map)
            7f61a293fb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20335 [000] 629951.439733: probe_tst:inreg: (400506) arg1=0x3
                  400506 func (/tmp/tst)
            7f61a293fb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)

tst 20335 [000] 629951.439738: probe_tst:mid: (400517) xxx=3 zzz=3
                  400517 func (/tmp/tst)
            7fffffffe000 [unknown] (/tmp/perf-20335.map)
            7f61a293fb45 __libc_start_main (/lib/x86_64-linux-gnu/libc-2.19.so)
....
#+END_EXAMPLE

Aha. So looking at the register =di= instead of the variable =xxx= makes it
work. Or I can look at the variable a little later; that works too. This is an
easy pitfall to get caught in. I'll try to investigate more and file a bug.

*** Pitfall: rebuilding

When using =perf= to introspect one's own code, you hit another issue early.
Suppose we just did all the probing above. Now we rebuild the program under test
(with or without modifications), and probe again:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ gcc -g -o tst tst.c

dima@shorty:/tmp$ sudo perf record -g -e probe_tst:inreg,probe_tst:mid -aR ./tst

zzz: 1
yyy: 6
zzz: 2
yyy: 7


dima@shorty:/tmp$ sudo perf report --stdio                                                 

Error:
The perf.data file has no samples!
#+END_EXAMPLE

Whoa! This just worked. What happened? It turns out =perf= records the build-id
that a particular probe corresponds to. So when you rebuild, the build-id
changes, and the probe no longer triggers. I haven't yet seriously looked into
disabling and/or fixing this. The easiest workaround is to simply remove all the
probes with

#+BEGIN_EXAMPLE
sudo perf probe -x tst --del "*"
#+END_EXAMPLE

Then when you re-add the probes, they will use the new build-id. This /really/
needs better handling upstream as well. I'll patch when I get the time.

** DONE Gnuplot for numpy                             :tools:python:data:dev:
   CLOSED: [2015-01-31 Sat 20:24]

#+begin_o_blog_alert info Follow-up posts
I did much more work on this: [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Gnuplot for numpy. For real this time")){/lisp}][Gnuplot for numpy. For real this time]]
#+end_o_blog_alert

I've been using [[http://pdl.perl.org][PDL]] for numerical computation. It works ok, but is crufty and
incomplete in many ways. I'm thus starting to look at numpy as an alternative,
as much as I hate switching languages. Such transitions are full of dealing with
people's NIH ambitions, and one has to pick one's battles. One battle I refuse
to concede is switching away to numpy's preferred NIH plotting system:
matplotlib. I've been using gnuplot for something like 20 years at this point,
and I'm not moving.

Python land appears to have a reasonably functional gnuplot interface:
[[http://gnuplot-py.sourceforge.net/][=gnuplot.py=]]. It works, but is very clunky to use with numpy. So I wrote
[[https://github.com/dkogan/python_gplot][=python_gplot=]], a wrapper around =gnuplot.py= that provides an interface very
similar to my original design for [[https://github.com/dkogan/PDL-Graphics-Gnuplot][PDL::Graphics::Gnuplot]]. Examples (ASCII plots
just as a demo):

#+BEGIN_EXAMPLE
dima@shorty:~/projects/python_gplot$ cat test.py

#!/usr/bin/python2

from numpy import *
from gplot import *

x = arange(100)
y = x ** 2


gplot( x,y, cmd="set terminal dumb" )

g2 = python_gplot("set terminal dumb")
g2.plot({'with': 'lines'},
        y, x)
g2.plot( (x-100, y),
         ({'with': 'lines'},
          -x,y) )




dima@shorty:~/projects/python_gplot$ ./test.py



 100.000 +-+----+-----+------+-----+------+------+-----+------+-----+----+-+
         +      +     +      +     +      +      +     +      +   ++++++   +
  90.000 +-+                                               ++++-++       +-+
         |                                           ++-+++                |
  80.000 +-+                                    +++++                    +-+
         |                                 +++++                           |
  70.000 +-+                          +++++                              +-+
         |                        ++++                                     |
  60.000 +-+                  ++++                                       +-+
  50.000 +-+              ++++                                           +-+
         |            ++++                                                 |
  40.000 +-+        +++                                                  +-+
         |       +++                                                       |
  30.000 +-+   +++                                                       +-+
         |   ++                                                            |
  20.000 +-++                                                            +-+
         |++                                                               |
  10.000 +-+                                                             +-+
         +      +     +      +     +      +      +     +      +     +      +
   0.000 +-+----+-----+------+-----+------+------+-----+------+-----+----+-+
         0     1000  2000   3000  4000   5000   6000  7000   8000  9000  10000




 10000.000 +-+----------+------------+-----------+------------+----------+A+
           ++++         +            +           +            +         AAA+
  9000.000 +-+++                                                       AA+-+
           |   +++                                                   AAA   |
  8000.000 +-+   +++                                               AAA   +-+
           |       +++                                           AAA       |
  7000.000 +-+       +++                                       AAA       +-+
           |           +++                                   AAA           |
  6000.000 +-+            ++                               AA            +-+
  5000.000 +-+              +++                         AAA              +-+
           |                  +++                     AAA                  |
  4000.000 +-+                   +++               AAA                   +-+
           |                       +++           AAA                       |
  3000.000 +-+                        +++     AAA                        +-+
           |                             +AAAA                             |
  2000.000 +-+                         AAAA ++++                         +-+
           |                       AAAA         ++++                       |
  1000.000 +-+                AAAAA                 +++++                +-+
           +          AAAAAAAA       +           +       ++++++++          +
     0.000 AAAAAAAAAAA--+------------+-----------+------------+----------+-+
         -100          -80          -60         -40          -20           0

                                                                                                  


 10000.000 +-+---+------+-----+------+-----+-----+------+-----+------+---+A+
           +     +      +     +      +     +     +      +     +      +  AAA+
  9000.000 +-+                                                         AA+-+
           |                                                         AAA   |
  8000.000 +-+                                                     AAA   +-+
           |                                                     AAA       |
  7000.000 +-+                                                 AAA       +-+
           |                                                 AAA           |
  6000.000 +-+                                             AA            +-+
  5000.000 +-+                                          AAA              +-+
           |                                          AAA                  |
  4000.000 +-+                                     AAA                   +-+
           |                                     AAA                       |
  3000.000 +-+                                AAA                        +-+
           |                              AAAA                             |
  2000.000 +-+                         AAAA                              +-+
           |                       AAAA                                    |
  1000.000 +-+                AAAAA                                      +-+
           +     +    AAAAAAAA+      +     +     +      +     +      +     +
     0.000 AAAAAAAAAAA--+-----+------+-----+-----+------+-----+------+---+-+
           0     10     20    30     40    50    60     70    80     90   100
#+END_EXAMPLE

*** Brief description

Here you can plot into a global =gnuplot= instance using the =gplot()= function.
Or you can create a new instance by instantiating =class python_gplot=. In
either case, you can pass in plot parameters for a single set of data /or/ a
tuple of such parameters if we want to plot multiple pieces of data on the same
plot.

Each set of plot parameters is a list that's an =ndarray= of =y= values or =x=,
=y=. Optionally, the first element of this list can be a =dict= that's passed to
=gnuplot.py= unmodified.

Lastly, arbitrary =gnuplot= commands can be given with

#+BEGIN_SRC python
gplot( ...., cmd="gnuplot command!!!")
#+END_SRC

or in a constructor:

#+BEGIN_SRC python
g = python_gplot("gnuplot command!!!")
#+END_SRC

*** Explained examples

Let's say you have two =ndarray= objects: =x= and =y=.

In its most simple form you can plot =y= versus =x=:

#+BEGIN_SRC python
gplot( x,y )
#+END_SRC

If you want to pick a different terminal, say:

#+BEGIN_SRC python
gplot( x,y, cmd="set terminal dumb" )
#+END_SRC

If you just want to plot =y= against integers, it's even simpler:

#+BEGIN_SRC python
gplot( y )
#+END_SRC

If you want to make such a plot but with points /and/ lines:

#+BEGIN_SRC python
gplot( {'with': 'linespoints'}, y )
#+END_SRC

If you want to plot two curves on the same plot:

#+BEGIN_SRC python
gplot( (x,y),
       ({'with': 'linespoints'}, y,x) )
#+END_SRC

*** Bugs

This is an extremely early alpha. I think it works, but haven't used it very
much yet. I'll improve this as I need features. 3D plotting will happen at some
point (maybe it works already). Also, this is my first python effort, and the
code quality will improve. Documentation is extremely lacking right now. This
will likewise improve as I use this.

*** License

Copyright 2015, Dima Kogan. Distributed under the terms of GNU LGPL, like
=gnuplot.py=

** DONE Bike headlight circuitry                                    :data:EE:
   CLOSED: [2015-02-20 Fri 20:57]

#+begin_o_blog_alert info Follow-up posts
I redid the circuitry: [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Bike headlight circuit continued")){/lisp}][Bike headlight circuit continued]] and [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Bike headlight circuit. Again")){/lisp}][Bike headlight circuit. Again]]
#+end_o_blog_alert

I just bought a bright headlight for my bicycle: [[http://www.dorcy.com/p-594-41-4001-3aa-led-bicycle-light-personal-light.aspx][Dorcy 41-4001]]. It's fairly
bright and beefy, which is good. It also runs on 3 AA alkaline batteries, which
isn't so much. I want to convert this headlight to run on a Li-ion cell instead.
This would give it a longer runtime and make it rechargeable.

*** Overview

Li-ion cells produce a higher voltage than alkaline ones (nominally 3.7V instead
of 1.5V), so this isn't necessarily a drop-in replacement. The headlight uses
the 3 AA cells in series to get 4.5V nominal. The headlight description says

#+BEGIN_EXAMPLE
LED: USA Made CREE XML (T6) High Power White LED
Power Driven:5.0Watt (3.10~3.15V 1500~1600MA ± 5%)
#+END_EXAMPLE

The [[http://www.cree.com/LED-Components-and-Modules/Products/XLamp/Discrete-Directional/XLamp-XML][LED manufacturer description]] lists a typical forward voltage of 3.1V and a
maximum current of 3A, so this is all consistent.

To replace the power source I need to know how the driving circuit works. I
naively assumed that there's a switching regulator being used, feeding back on
the current to drive the LED efficiently and to maintain a constant current (and
thus constant light output) in a range of voltages. To test this theory I ran
the light from my benchtop power supply instead of the AA batteries. I expected
to see the input current track the input voltage inversely (keeping the input
power roughly constant). Instead, I saw them moving together, with the light
brightness varying noticeably with input voltage. This is what you'd see if
there was a simple resistor used to limit the LED current. Seriously???

*** LED characterization

First, I gather what I know: the LED characteristics themselves. These are on
page 5 of the datasheet linked [[http://www.cree.com/LED-Components-and-Modules/Products/XLamp/Discrete-Directional/XLamp-XML][here]]. I extracted the [[file:files/Dorcylight/diode.dat][i-V data]], and the i-V curve
of the LED under nominal conditions looks like this ([[file:files/Dorcylight/diode.gp][source]]):

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/cree_characteristics.svg]]

*** Headlight body characterization

The headlight is composed of

- the main housing (contains the LED and the battery pack)
- battery cap (contains ON switch)

The initial assumption was that everything we care about lives in the housing,
so I characterized that first. I scanned a range of input voltages, taking note
of the input currents drawn. Let me assume a trivial resistor-diode driver
circuit and try to fit my [[file:files/Dorcylight/iv_body.dat][observed input i-V data]] ([[file:files/Dorcylight/iv_body.gp][source]])

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/iv_body.svg]]

I didn't go to the nominal triple-alkaline voltage (4.5V) because my power
supply can't produce more than 3A. The fit isn't perfect, so there's likely
something else going on. The fitted resistor value is 0.28 Ohms! Could this
really be true? The alligator clips could have resistances in this range.
Furthermore, the LED datasheet says that it produces 280 lumens at 0.7A, but we
get this at a very low voltage /and/ the headlight is rated for 220 lumens.
/This/ is a highly inconsistent piece of data. Something else is going on.

*** Headlight body /and/ battery cap characterization

What if the battery cap is more than a dumb switch? It's beefy and sealed, so
there /could/ be something in there. I hooked everything up cables, and scanned
the voltages again. Once again, let me assume a trivial resistor-diode driver
circuit and try to fit my [[file:files/Dorcylight/iv_full.dat][observed input i-V data]] ([[file:files/Dorcylight/iv_full.gp][source]])

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/iv_full.svg]]

Much better! The inferred resistance is 1.9 Ohms. Low, but I can believe it. The
body was /just/ the LED, and the resistances were my (poor) connections.
Alkalines generally start at around 1.6V, then decay to 1.0V or so as you use
them. Ballpark average: 1.3V, so 3 of those are at ~4V. The data we just
gathered puts this current draw at 600mA, which is a bit below 700mA; this makes
sense since the rated light output is 220 lumens, a bit lower than the 280
lumens we're supposed get at 700mA.

*** Musings

I'm now confident I know what's going on. I'm satisfied that Li-ion won't break
anything, but it will be dim. Li-ion batteries have varying discharge curves;
generally they start at 4.2V or so, then drop to 3.0V-3.5V as you use them. This
will result in a dramatic drop in the current and thus light intensity.

This is just lame. Without regulation the light gets dimmer as the batteries are
used up /and/ it's just inefficient. Taking their nominal operating point of 4V
you get 3.1V in the LED and you throw away 0.9V in the resistor for an
efficiency of ~75%. In this scheme the efficiency gets worse at higher voltages
(brighter light). I guess you could do worse, but it's just dumb.

My particular rechargable plan won't produce great results. I guess I can lower
the resistance by installing something in parallel. Maybe I will, but I will
complain the whole time.

** DONE Bike headlight circuit continued                                 :EE:
   CLOSED: [2015-03-14 Sat 02:34]

#+begin_o_blog_alert info Follow-up posts
I finally laid out a PCB: [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Bike headlight circuit. Again")){/lisp}][Bike headlight circuit. Again]]
#+end_o_blog_alert

In the [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Bike headlight circuitry")){/lisp}][last post]] I complained about the poor regulation of a flashlight I
bought. Apparently I was willing to dump unreasonable amounts of time to right
this wrong, so I bought some parts, hacked them in and I now have a regulated
flashlight driven by a big Li-ion rechargable cell.

The battery is a single 3500 mAh MNKE IMR 26650 cell. Apparently these are
popular in the vaping community. Ok then.

Very little is required to add regulation. I'm using an [[http://www.st.com/web/en/catalog/sense_power/FM142/CL1854/SC1575/PF253333][LED2000]] driver IC to do
all the work. This is a buck switching regulator that feeds back on current in
order to drive an LED with a particular current. The [[file:files/Dorcylight/schematics.pdf][circuit]] I built is the
reference design for this IC. I added a TVS diode to absorb the voltage spikes
that could result if the LED is disconnected as it is being driven; this is a
potentially poor mechanical connection, so this is important. I added a PTC
resistor to protect against short circuits. Furthermore, I'm using a switch
controller IC and a MOSFET to interact with the on/off button. The circuit is
not at all interesting.

What /is/ interesting is the construction. I never built a board, so this is all
free-formed and then encased in hot glue at the end. I'm using 22-gauge magnet
wire for everything. This is nicely thick and low impedance, but stripping it is
a pain in the butt. In fact, this whole free-forming exercise was really
time-consuming, and I don't feel particularly good about the results. The thing
works right now, but I can easily imagine that it'll stop working at any point.
At that point, I could make a PCB to construct something more reliable, but I've
already put way too much time into this, so simply buying another light would be
the answer.

Before I realized that the button assembly in the cap could be opened, all the
circuitry was meant to go in the gap between the battery and the housing:

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/R0023853.jpg.scaled.jpg]]
#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/R0023862.jpg.scaled.jpg]]

After I figured out that I could get to the on/off button circuitry, I put
everything into the cap:

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/R0023864.jpg.scaled.jpg]]

Then encase in hot glue, and done:

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/R0023867.jpg.scaled.jpg]]
#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/R0023868.jpg.scaled.jpg]]

** DONE Linux tracepoints for 32-bit code on a 64-bit kernel      :tools:dev:
   CLOSED: [2015-04-03 Fri 15:28]

So I was trying to use [[http://www.sysdig.org][sysdig]] to see what a Windows application running under
[[http://www.winehq.org][wine]] was doing, and sysdig was telling me nothing about it. A [[https://github.com/draios/sysdig/issues/278][bug report]] and
some investigating yielded the answer: Linux tracepoints do not work for 32-bit
processes running on a 64-bit kernel. As a trivial example, you can build tst.c:

#+BEGIN_SRC C
#include <stdio.h>
#include <stdlib.h>

#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>

int main(int argc, char* argv[])
{
    while(1)
    {
        int fd = open("/dev/null", O_RDONLY);
        close(fd);
        sleep(1);
    }

    return 0;
}
#+END_SRC

natively: =gcc -o tst tst.c=, and sysdig will see the 3 syscalls here just fine.
But building it in 32-bit mode with =gcc -m32 -o tst tst.c= makes sysdig blind.
One doesn't even have to use sysdig. I tried to use tracepoints through the
interface in =/sys= with the same results: events are seen without =-m32=, but
cannot be seen with it:

#+BEGIN_EXAMPLE

root@shorty:/home/dima# cd /sys/kernel/debug/tracing

root@shorty:/sys/kernel/debug/tracing# echo 'syscalls:sys_enter_open' >> /sys/kernel/debug/tracing/set_event 

root@shorty:/sys/kernel/debug/tracing# echo 'common_pid == 16211' > events/syscalls/sys_enter_open/filter

root@shorty:/sys/kernel/debug/tracing# cat trace_pipe
^C

#+END_EXAMPLE

This may or may not be easy to fix, but this rabbithole probably runs deep, so
I'm stopping here.

** DONE Steepest streets in Los Angeles                            :data:GIS:
   CLOSED: [2015-05-03 Sun 00:05]

A century ago, city planners didn't have the restraint that modern planners do,
and as a result some older LA neighborhoods have some exceptionally steep
streets. Random people on the internet usually claim that some combination of
Fargo St, Baxter St (both in Silver Lake) and Eldred St (climbing Mt Washington
from Highland Park) are the steepest, with Eldred barely taking the crown. I've
never seen any justification of this, and being skeptical of things random
people on the internet say, I decided to check.

The basic strategy is the same as the [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Using DEMs to get GPX elevation profiles")){/lisp}][last time]], but using real tools. I want to
get a detailed-enough DEM, and to sample it along given street paths (from [[http://www.openstreetmap.org][OSM]]).
The SRTM data I used the last time isn't detailed enough: the higher-res SRTM
data has cell about 30 meters wide. LA county has a [[http://egis3.lacounty.gov/dataportal/2011/01/26/2006-10-foot-digital-elevation-model-dem-public-domain/][publicly-available dataset]]
with a higher resolution: cells about 3m wide. This should be sufficient.

After downloading the 2GB file, the DEM can be read into [[http://www.qgis.org][QGIS]] by opening the
=dem_10ft/hdr.adf= file as a raster layer. Note that it takes QGIS a bit of time
to load the whole thing. The =Quick OSM= plugin works reasonably well to import
OSM data into QGIS (can be installed through the Plugin manager in the =Plugins=
directory in QGIS). Once the DEM and the street geometry are loaded, we can
generate the elevation profile. There are several plugins that do this. With
some coaxing, I was able to get the "Profile tool" plugin to do what I needed
here.

*** Results

I checked Eldred, Fargo and Baxter. The term "steepest street" is
poorly-defined, so based on a completely arbitrary measure or "steepest over a
reasonable length" we have

- Eldred and Fargo tied for 1st
- Baxter 3rd

The sampled elevation-vs-horizontal travel data is here (everything in ft):

- [[file:files/la_hills/eldred.dat][Eldred]]
- [[file:files/la_hills/fargo.dat][Fargo]]
- [[file:files/la_hills/baxter.dat][Baxter]]

The elevation profiles for the 3 streets in their steepest sections looks like
this (Baxter, Fargo starting at the 2 fwy, and going East; [[file:files/la_hills/profiles_full.gp][source]]):

#+ATTR_HTML: :width 80%
[[file:files/la_hills/profiles_full.svg]]

We can see that as you travel West from Ave 50, Eldred St loses elevation, and
then regains it quickly, getting more steep as it goes. We can also see that
while the steepest section of Fargo is a bit steeper than Baxter, Baxter keeps
going and has several more slightly-less-steep bumps.

If we plot the steepest sections of all 3 streets on top of one another, we get
this ([[file:files/la_hills/profiles_relative.gp][source]]):


#+ATTR_HTML: :width 80%
[[file:files/la_hills/profiles_relative.svg]]

Once again, Fargo is steeper than Baxter. The steepest section of Fargo is very
similar to Eldred, but Fargo retains is steepness longer than Eldred does.

One could try to split hairs about whether Fargo or Baxter are steeper, but
based on this data, they're even for all purposes.

Clearly this all is only valid if the source data is valid. It looks mostly
reasonable. The DEM is detailed enough to show each street in a little valley,
and this aligns well with Eldred. Fargo and Baxter lie in an area with a natural
valley, but the street alignment ignores it cutting at an angle, which is
consistent with the USGS topo. The absolute heights are fairly consistent with
the USGS topo. I believe these results.

It's also possible that there's some other candidate steep street out there.
Some random internet person claimed 28th Street in San Pedro had a steep
section, but none was comparable to these 3. This is good enough for me and I'm
stopping here.

** DONE Poles of Inaccessibility in the San Gabriel Mountains :hiking:data:GIS:
   CLOSED: [2015-05-06 Wed 17:04]


#+begin_o_blog_alert info Updates
I visited the furthest Pole and placed a register there:
[[http://sangabrielmnts.myfreeforum.org/about7040.html]]. Do visit and sign in!

Also I ran a [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Pole of road inaccessibility of the contiguous US")){/lisp}][similar analysis for the contiguous US]]
#+end_o_blog_alert

So I was out hiking with a friend, and a question came up about where the least
accessible point of the San Gabriel Mountains was, with "accessible" defined as
having a road or trail nearby. I decided to answer this question. The resulting
code is in a fresh repository: [[https://github.com/dkogan/inaccessibility]].

This is called the [[http://en.wikipedia.org/wiki/Pole_of_inaccessibility][Pole of Inaccessibility]]: a point that is as far away as
possible from a given set of objects. Locations of such poles are known for the
most landlocked spot on earth or most far away from land. Here we limit
ourselves to the San Gabriel Mountains, and try to stay away from roads and
trails.

*** Approach

**** Input data processing

[[http://www.openstreetmap.org][OpenStreetMap]] has open data I can use to map out all the roads and trails. This
is the input dataset.

For 2D geometry, the best approach to compute the Pole of Inaccessibility
appears to be to construct a [[http://en.wikipedia.org/wiki/Voronoi_diagram][Voronoi diagram]] of the geometry we're trying to
stay away from, and to find the Voronoi vertex corresponding to the
furthest-away point.

Our world is not 2D. Instead, it has varying elevation sitting on top of an
ellipsoid. The grand purpose here is to compute a location that hardy people can
visit and to tell everybody they did it, so extreme accuracy is not required.
Thus I claim that assuming the world is locally-flat and using the
Voronoi-diagram-based method is sufficient. So I construct a plane that best
describes my query area and project all my input points to this plane. I use a
plane that is tangent to the Earth's surface at the center of the query area.
This clearly wouldn't work if trying to find the pole of inaccessibility of
something as large as an ocean, for instance, but it works here.

To compute the tangent plane, I assume the Earth is spherical. As I move along
the tangent plane away from the point of tangency, the elevation error grows:

 E = sqrt(R_{earth}^{2} + d^{2}) - R_{earth}

The San Gabriels are about 80km across, and the tangent plane sits in the
middle, so at worst d = 40km and the error is about 125m. That's plenty good
enough. Plot ([[file:files/inaccessibility/plot_flat_earth_error.gp][source]]):

[[file:files/inaccessibility/plot_flat_earth_error.svg]]

I ignore the ellipsoid shape of the Earth outright. I ignore the topography as
well, since including it in my distance metrics would require a fancier
algorithm than making a Voronoi diagram, and it would make the notion of
"inaccessibility" more ambiguous.

**** Pole of Inaccessibility computation

I want to use the most basic Voronoi algorithm, so I represent my input as a set
of points only; no line segments. To get reasonable accuracy, I make sure to
sample each road at least every 100m.

Now that I have my set of dense-enough points in 2D, I construct the Voronoi
diagram. Without constraints the furthest-away point would be infinitely far off
to one side, so generally people constrain the solution to lie within the convex
hull of the input points. This means that the Pole of Inaccessibility lies
either on a Voronoi vertex or at an intersection of a Voronoi edge and the
convex hull of the input. In my case there are generally more roads at the edges
of my query area that in the interior (less stuff in the mountains than in the
flats), so I simply assume that the Pole of Inaccessibility is not on the convex
hull. This simplifies my implementation since I simply ignore all the Voronoi
vertices that are outside of the query region.

So I need to look at every Voronoi vertex, check the distance between it and an
adjacent input point, and return the vertex with the largest such distance.

*** Implementation

Each step in the process lives in its own program. This simplifies
implementation and makes it easy to work on each piece separately.

**** Data import

First we query OSM. This is done with the =query.sh= script. It takes in corners
of the query area, constructs the query, sends it off to the server, and stores
the result. =query.sh= takes 4 arguments; lat0, lon0, lat1, lon1, and stores its
output in a file called =query_$lat0_$lon0_$lat1_$lon1.json=. The query uses the
[[http://wiki.openstreetmap.org/wiki/Overpass_API/Overpass_QL][OSM Overpass query language]]. By default I simply look at all the roads, trails
(everything with a =highway= tag):

#+BEGIN_EXAMPLE
[out:json];

way ["highway"] ($lat0,$lon0,$lat1,$lon1);

(._;>;);

out;
#+END_EXAMPLE

If I want to only consider roads in my computation (allow trails), then I can
exclude trails from the query:

#+BEGIN_EXAMPLE
[out:json];

way ["highway"] ["highway" != "footway" ] ["highway" != "path" ] ($lat0,$lon0,$lat1,$lon1);

(._;>;);

out;
#+END_EXAMPLE

Sample invocation:

#+BEGIN_EXAMPLE
$ ./query.sh 34.1390884 -118.4944153 34.5020298 -117.5852966

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 28.3M    0 28.3M    0   138  72803      0 --:--:--  0:06:48 --:--:--  138k


$ ls -lh query*

-rw-r--r-- 1 dkogan dkogan 29M May  6 04:12 query_34.1390884_-118.4944153_34.5020298_-117.5852966.json
#+END_EXAMPLE

**** Data massaging

Next, I take the lat/lon pairs, map them to the tangent plane and make sure the
data is sufficiently dense. This is done by the =massage_input.pl= script. It
takes in the =query_....json= file we just obtained, and generates a
=points_$lat0_$lon0_$lat1_$lon1.dat= file that is a list of (x,y) tuples in my
plane. There's a small header of 4 values, representing the bounds of my data so
that I can reject outlying vertices, as described earlier.

Sample invocation:

#+BEGIN_EXAMPLE
$ ./massage_input.pl query_34.1390884_-118.4944153_34.5020298_-117.5852966.json


$ ls -lh points*

-rw-r--r-- 1 dkogan dkogan 4.3M May  6 04:20 points_34.1390884_-118.4944153_34.5020298_-117.5852966.dat
#+END_EXAMPLE

**** Pole of Inaccessibility computation

Now we can compute the Voronoi diagram. I use [[http://www.boost.org/doc/libs/1_57_0/libs/polygon/doc/index.htm][boost::polygon]] to do this. I had
concerns that this step would be prohibitively slow, but the algorithm and this
implementation are quick-enough such that this "just works".

The =points_....dat= file is inputs on standard input. Note that this is
different from the other tools that read a file on the commandline instead.

For each Voronoi vertex I get an arbitrary neighboring edge, and an arbitrary
neighboring cell. The distance between the vertex and the cell center is
identical for any such edge, cell by definition of a Voronoi vertex. I keep
track of the cell with the largest distance between the vertex and the cell
center, and I report the vertex with the largest such distance as my Pole of
Inaccessibility.

Sample invocation:

#+BEGIN_EXAMPLE
$ ./voronoi < points_34.1390884_-118.4944153_34.5020298_-117.5852966.dat 

furthest point center, surrounding points:
25541 -78
25308 4259
21223 -543
26931 -4192
distance: 4342.873206
#+END_EXAMPLE

Bam! So the Pole of Inaccessibility is about 4.3 km from the nearest trail/road.
The coordinates here are in my 2D tangent plane, which isn't super useful. Now I
convert them to lat/lon and I'm done.

**** Unmapping the planar coordinates

I do this with the massaging script as before simply by passing the coords in on
the commandline:

#+BEGIN_EXAMPLE
$ ./massage_input.pl query_34.1390884_-118.4944153_34.5020298_-117.5852966.json 25541 -78 25308 4259 21223 -543 26931 -4192

34.3206972918426,-117.761740671673
34.3597096140465,-117.764149633831
34.3165155855012,-117.808770212857
34.2837076589351,-117.746734473897
#+END_EXAMPLE

OK. Done.

*** Results

I did this three times

- avoiding all roads, trails
- avoiding all roads
- avoiding all paved, driveable roads

All Poles of Inaccessibility are above the East Fork of the San Gabriel River,
by Ross Mountain and Iron Mountain:

| pole         | lat,lon            | distance to nearest (m) |
|--------------+--------------------+-------------------------|
| roads,trails | 34.3204, -117.7617 |                    4343 |
| roads only   | 34.3108, -117.7474 |                    5677 |
| paved roads  | 34.2992, -117.7167 |                    8157 |

This is shown nicely on the map:

http://caltopo.com/m/4N7A

[[file:files/inaccessibility/poles.png]]

Looks like the bounding spots for the roads,trails point are the road up to
South Mt Hawkins, the PCT on top of Mt. Baden-Powell and the trail at the Bridge
to Nowhere.

The bounding spots for the roads-only point is the same road up to South Mt
Hawkins, the Cabin Flat Campground and Shoemaker Canyon Road.

** DONE Gnuplot for numpy. For real this time         :tools:python:data:dev:
   CLOSED: [2015-07-02 Thu 15:05]

#+begin_o_blog_alert info Follow-up posts
I added broadcasting support: [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "gnuplotlib and broadcasting in numpy")){/lisp}][gnuplotlib and broadcasting in numpy]]
#+end_o_blog_alert

In the [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Gnuplot for numpy")){/lisp}][last post]] I talked about a gnuplot backend for numpy to make it possible
to live without matplotlib. I just did quite a bit of work to port my [[https://github.com/dkogan/PDL-Graphics-Gnuplot][PDL
plotting library]] to python and numpy. The result: [[https://github.com/dkogan/gnuplotlib][gnuplotlib]]. This is a fairly
direct port, so the API and the design goals are similar. The module tries to be
a thin shim between python and gnuplot, so if the user wants to do something
fancy, they should read the gnuplot docs to figure out how to get the fancy
thing done, and then communicate it to gnuplotlib. I see this as a good thing.
Some basic examples from the main documentation:

#+BEGIN_SRC python
 import numpy      as np
 import gnuplotlib as gp
 from scipy.constants import pi

 x = np.arange(101) - 50
 gp.plot(x**2)
 [ basic parabola plot pops up ]


 g1 = gp.gnuplotlib(title = 'Parabola with error bars',
                    _with = 'xyerrorbars')
 g1.plot( x**2 * 10, np.abs(x)/10, np.abs(x)*5,
          legend    = 'Parabola',
          tuplesize = 4 )
 [ parabola with x,y errobars pops up in a new window ]


 x,y = np.ogrid[-10:11,-10:11]
 gp.plot( x**2 + y**2,
          title     = 'Heat map',
          cmds      = 'set view map',
          _with     = 'image',
          tuplesize = 3)
 [ Heat map pops up where first parabola used to be ]


 theta = np.linspace(0, 6*pi, 200)
 z     = np.linspace(0, 5,    200)
 g2 = gp.gnuplotlib(_3d = True)
 g2.plot( (np.cos(theta),  np.sin(theta), z),
          (np.cos(theta), -np.sin(theta), z))
 [ Two 3D curves together in a new window: spirals ]
#+END_SRC

See the main docs for more. During the course of this exercise I now have a
fully-formed opinion of python from a perl perspective (meh). And one of numpy
from a PDL perspective (also meh). But at least they're now "standard" for some
meaning of that word, so we'll see.

** DONE Validating a CRC computation on an AVR                       :dev:EE:
   CLOSED: [2015-08-01 Sat 16:38]

Recently I implemented a self-reprogramming feature for an AVR XMEGA-E5 cpu.
Naturally one wants to have some validation logic to make sure that what was
burned to the flash is right. This is a common want, so Atmel implemented in
hardware a chunk of logic to compute the CRC checksum of a block of flash
memory. This is nice. What is less nice is their documentation, which is lacking
to put it nicely. To use their hardware CRC, I need to make sure it matches a
CRC that I compute in software. This was needlessly difficult, and hopefully
this will serve as a guide for somebody in the future.

The AVR supports a CRC16 checksum (CCITT) and a CRC32 checksum (IEEE 802.3). I
use the bigger 32-bit checksum here.

The *XMEGA E MANUAL* describes in section "29.11.2.6" how to compute the CRC of
a block of flash memory. It says

#+BEGIN_EXAMPLE
The CRC checksum will be available in the NVM DATA register
#+END_EXAMPLE

This is a lie. On my partiular CPU, there are 24 bits worth of NVM.DATA, so this
cannot be true for a 32-bit checksum. The checksum /really/ ends up in the
CRC.CHECKSUM registers.

In Section 24.3 the manual says

#+BEGIN_EXAMPLE
when CRC-32 polynomial is used, the final checksum read is bit reversed and complemented

#+END_EXAMPLE

I'm not 100% sure what's going on here; I /think/ that "bit reversed and
complemented" simply means "binary NOT", but I'm not certain. In any case, there
also seems to be a difference in behavior if we're checksumming flash memory
(=CRC_SOURCE_FLASH_gc=) or if we're looking at =CRC.DATAIN= input
(=CRC_SOURCE_IO_gc=).

By default, the CRC module resets from zero. However to match other CRC32
implementations, it is apparently necessary to reset from 1 instead
(=CRC_RESET_RESET1_gc=). Oh, and this reset must happen /before/ the CRC module
is turned on.

Section 24.7.2 says

#+BEGIN_EXAMPLE
When running CRC-32 and appending the checksum at the end
of the packet (as little endian), the final checksum should be 0x2144df1c, and
not zero. However, if the checksum is complemented before it is appended (as
little endian) to the data, the final result in the checksum register will be
zero
#+END_EXAMPLE

This is a lie also. The constant 0x2144df1c is indeed there. However if you
append a complemented (meaning binary NOT instead of a 2-complement) CRC then
you get 0xFFFFFFFF and not 0.

So in the end, if I have a flash of size =SIZE_APP_FLASH_BYTES=, then I compute
the CRC32 of the first =SIZE_APP_FLASH_BYTES-4= bytes, and append a binary NOT
of this CRC to the end. I can then validate by making sure that this CRC matches
0xFFFFFFFF. On the AVR:

#+BEGIN_SRC C

  bool CRC_matches(void)
  {
      CRC.CTRL |= CRC_RESET_RESET1_gc;
      CRC.CTRL  = CRC_CRC32_bm | CRC_SOURCE_FLASH_gc;
      nvm_issue_flash_range_crc(0, SIZE_APP_FLASH_BYTES-1);
      nvm_wait_until_ready();
      while( CRC.STATUS & CRC_BUSY_bm ) ;

      return
        CRC.CHECKSUM0 == '\xFF' &&
        CRC.CHECKSUM1 == '\xFF' &&
        CRC.CHECKSUM2 == '\xFF' &&
        CRC.CHECKSUM3 == '\xFF';
  }

#+END_SRC

When preparing this image, I have perl pad, compute the CRC, binary-NOT and
append:

#+BEGIN_SRC perl
use Digest::CRC 'crc32';

sub prepareImage
{
    my ($rawimage) = @_;

    my $Npad = $SIZE_APP_FLASH_BYTES-4 - length($rawimage);
    my $ff   = pack('C', 0xFF);
    my $pad  = $ff x $Npad;

    my $crc = crc32($rawimage . $pad);
    return $rawimage . $pad . pack('V', 0xFFFFFFFF & ~$crc);
}

#+END_SRC

** DONE Rendering OpenStreetMap tiles in gnuplot         :tools:data:GIS:dev:
   CLOSED: [2015-08-13 Thu 00:39]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "osmgnuplot update: correct plots over a large geographical area")){/lisp}][osmgnuplot update: correct plots over a large geographical area]]
#+end_o_blog_alert

I use gnuplot heavily to plot things. I also use OpenStreetMap heavily to map
other things. Sometimes I want to draw a map using OSM tiles, and to plot
something on top of it. Gnuplot has the core functionality, but some glue code
needs to be written to actually make this available. I just wrote this glue
code, and it lives in a new repository: https://github.com/dkogan/osmgnuplot

The repository contains the documentation and usage examples. The [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Least convenient location in Los Angeles (from Koreatown)")){/lisp}][next post]] uses
this extensively to visualize mapping data.

** DONE Least convenient location in Los Angeles (from Koreatown) :biking:data:GIS:
   CLOSED: [2015-08-16 Sun 00:10]

Talking to a friend, a question came up about finding the point in LA's road
network that's most inconvenient to get to, with /inconvenient/ being a vague
notion describing a closed residential neighborhood full of dead ends; the
furthest of these dead ends would be most inconvenient indeed. I decided to
answer that question. All the code lives in a new repo:
https://github.com/dkogan/culdesacs

I want /inconvenient/ to mean

#+BEGIN_QUOTE
Furthest to reach via the road network, but nearest as-the-crow-flies.
#+END_QUOTE

Note that this type of metric is not a universal one, but is relative to a
particular starting point. This makes sense, however: a location that's
inconvenient from one location could be very convenient from another.

This metric could be expressed in many ways. I keep it simple, and compute a
relative inefficiency coefficient:

[[file:files/culdesacs/coeff_define.png]]

# o-blog v1 can only export equations as mathjs, but I want a JS-less page. I
# just rendered the below (C-c C-x C-l) and copied the image manually
# \[
# c_\mathrm{inefficiency} \equiv \frac{d_\mathrm{road} - d_\mathrm{direct}}{d_\mathrm{direct}}
# \]

Thus the goal is to find a location within a given radius of the starting point
that maximizes this relative inefficiency.

*** Approach

I use [[http://www.openstreetmap.org][OpenStreetMap]] for the road data. This is all aimed at bicycling, so I'm
looking at all roads except freeways and ones marked private. I /am/ looking at
footpaths, trails, etc.

Once I have the road network, I run [[https://en.wikipedia.org/wiki/Dijkstra's_algorithm][Dijkstra's Algorithm]] to compute the shortest
path from my starting point to every other point on the map. Then I can easily
compute the inefficiency for each such point, and pick the point with the
highest inefficiency. I use OSM nodes as the "points". It is possible that the
location I'm looking for is inbetween a pair of nodes, but the nodes will really
be close enough. Also, the "distance" between adjacent nodes can take into
account terrain type, elevation, road type and so on. I ignore all that, and
simply look at the distance.

*** Implementation

Each step in the process lives in its own program. This simplifies
implementation and makes it easy to work on each piece separately.

**** Data import

First I query OSM. This is done with the =query.pl= script. It takes in the
center point and the query radius. The query uses the [[http://wiki.openstreetmap.org/wiki/Overpass_API/Overpass_QL][OSM Overpass query
language]]. I use this simple query, filling in the center point and radius:

#+BEGIN_EXAMPLE
[out:json];

way
 ["highway"]
 ["highway" !~ "motorway|motorway_link" ]
 ["access" !~ "private" ]
 ["access" !~ "no" ]
 (around:$rad,$lat,$lon);

(._;>;);

out;
#+END_EXAMPLE

Sample invocation:

#+BEGIN_EXAMPLE
$ ./query.pl --center 34.0690448,-118.292924 --rad 20miles
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  .....

$ ls -lhrt *(om[1])
-rw-r--r-- 1 dima dima 81M Aug 14 00:44 query_34.0690448_-118.292924_20miles.json

#+END_EXAMPLE

**** Data massaging

Now I need to take the OSM query results, and manipulate them into a form
readable by the Dijkstra's algorithm solver. This is done by the
=massage_input.pl= script. This script does nothing interesting, but it doesn it
inefficiently, so it's CPU and RAM-hungry and takes a few minutes. Sample
invocation:

#+BEGIN_EXAMPLE
$ ./massage_input.pl query_34.0690448_-118.292924_20miles.json > query_34.0690448_-118.292924_20miles.net
#+END_EXAMPLE

***** Neighbor list representation

An implementation choice here was how to represent the neighbor list for a node.
I want the main computation (next section) to be able to query this very
quickly, and I don't want the list to take much space, and I don't want to
fragment my memory with many small allocations. Thus I have a single contiguous
array of integers =neighbor_pool=. Each node has a single integer index into
this pool. At this index the =neighbor_pool= contains a list of node indices
that are neighbors of the node in question. A special node index of -1 signifies
the end of the neighbor list for that node.

**** Inefficiency coefficient computation

I now feed the massaged data to Dijkstra's algorithm implemented in =compute.c=.
I need a priority queue where elements can be inserted, removed and updated.
Apparently most heap implementations don't have an 'update' mechanism, so it
took a little while to find a working one. I ended up using [[https://en.wikipedia.org/wiki/B-heap][phk's b-heap]]
implementation from the [[https://www.varnish-cache.org/trac/browser/lib/libvarnish/binary_heap.c][varnish source tree]]. It stores arbitrary pointers
(64-bit on my box); 32-bit indices into a pool would be more efficient, but this
is fast enough.

Sample invocation:

#+BEGIN_EXAMPLE
$ ./compute < query_34.0690448_-118.292924_20miles.net > query_34.0690448_-118.292924_20miles.out

$ head -n 2 query_34.0690448_-118.292924_20miles.out
34.069046 -118.292923 0.000000 0.000000
34.070034 -118.292931 109.863564 109.863564
#+END_EXAMPLE

The output is all nodes, sorted by the road distance to the node. The columns
are lat,lon,d_{road},d_{direct}.

***** Distance from latitude/longitude pairs

One implementation note here is how to compute the distance between two
latitude/longitude pairs. The most direct way is to convert each
latitude/longitude pair into a unit vector, compute the dot product, take the
arccos and multiply by the radius of the Earth. This requires 9 trigonometric
operations and relies on the arccos of a number close to 1, which is inaccurate.
One could instead compute the arcsin of the magnitude of the cross-product, but
this requires even more computation. I want something simpler:

[[file:files/culdesacs/latlon_distance_approx.png]]

# As before, this is a render of...
#
# \begin{align*}
# d &= R_\mathrm{earth} \alpha \\
# \\
# \theta &\equiv \mathrm{longitude} \\
# \phi &\equiv \mathrm{latitude} \\
# \\
# \cos(\alpha) &= \vec v_0 \cdot \vec v_1 = \\
# &= \left[ \begin{matrix}
#  \cos(\theta_0) \cos(\phi_0) \\
#  \sin(\theta_0) \cos(\phi_0) \\
#  \sin(\phi_0) \\
# \end{matrix} \right] \cdot
# \left[ \begin{matrix}
#  \cos(\theta_1) \cos(\phi_1) \\
#  \sin(\theta_1) \cos(\phi_1) \\
#  \sin(\phi_1)
# \end{matrix} \right] = \\
# &= \cos(\phi_0) \cos(\phi_1) \left( \cos(\theta_0) \cos(\theta_1) + \sin(\theta_0) \sin(\theta_1) \right) + \sin(\phi_0) \sin(\phi_1) = \\
# &= \cos(\phi_0) \cos(\phi_1) \cos(\Delta_\theta) + \sin(\phi_0) \sin(\phi_1) \\
# \\
# \cos(\Delta_\theta) & \approx 1 - {\Delta_\theta^2 \over 2} \rightarrow \\
# \cos(\alpha) & \approx \cos(\phi_0) \cos(\phi_1) + \sin(\phi_0) \sin(\phi_1) - {\Delta_\theta^2 \over 2} \cos(\phi_0) \cos(\phi_1) = \\
# &= \cos(\Delta_\phi) - {\Delta_\theta^2 \over 2} \cos(\phi_0) \cos(\phi_1) \approx \\
# & \approx 1 - {\Delta_\phi^2 \over 2} - {\Delta_\theta^2 \over 2} \cos(\phi_0) \cos(\phi_1) \\
# \\
# \cos(\alpha) & \approx 1 - {\alpha^2 \over 2} \rightarrow \\
# \alpha^2 & \approx \Delta_\phi^2 + \Delta_\theta^2 \cos(\phi_0) \cos(\phi_1) \\
# \\
# \alpha & \approx \sqrt{\Delta_\phi^2 + \Delta_\theta^2 \cos(\phi_0) \cos(\phi_1)}
# \end{align*}


This is nice and simple. Is it sufficiently accurate? This python script tests
it:

#+BEGIN_SRC python
import numpy as np
lat0,lon0 = 34.0690448,-118.292924  # 3rd/New Hampshire
lat1,lon1 = 33.93,-118.4314         # LAX

lat0,lon0,lat1,lon1 = [x * np.pi/180.0 for x in lat0,lon0,lat1,lon1]

Rearth = 6371000

v0 = np.array((np.cos(lat0)*np.cos(lon0), np.cos(lat0)*np.sin(lon0),np.sin(lat0)))
v1 = np.array((np.cos(lat1)*np.cos(lon1), np.cos(lat1)*np.sin(lon1),np.sin(lat1)))

dist_accurate = np.sqrt( (lat0-lat1)**2 + (lon0-lon1)**2 * np.cos(lat0)*np.cos(lat1) ) * Rearth
dist_approx   = np.arccos(np.inner(v0,v1)) * Rearth

print dist_accurate
print dist_approx
print dist_accurate - dist_approx
#+END_SRC

Between Koreatown and LAX there's quite a bit of difference in both latitude and
longitude. Both methods say the distance is about 20km, with a disagreement of
3mm. This is plenty good enough.

*** Results

I want to find the least convenient location from the intersection of New
Hampshire and 3rd street in Los Angeles within 20 miles or so.

All the inconvenience coefficients computed from this data look like this
([[file:files/culdesacs/all_coefficients.gp][source]], [[file:files/culdesacs/query_34.0690448_-118.292924_20miles.out][data]]):

[[file:files/culdesacs/all_coefficients.png]]

As expected, straight roads away from the starting point are highly efficient,
and we see this in the Vermont corridor, 3rd street, and to a lesser extent 6th
street going East. Also as expected, routes that do not align with the street
grid are inefficient: we see inefficiency bumps going NW and SW from the
starting point. Eastward from the start the street grid tilts, so things are
more complicated in that direction.

We also see that hilly neighborhoods stand out: winding streets are not
efficient. The Santa Monica mountains, Verdugos, Mount Washington and the San
Gabriel Mountains clearly stand out. One can also make out the river paths in
the bottom-right as ridges of inconvenience: their limited access nature makes
it necessary to travel longer distance to get to them.

Onwards. The output of =compute= is sorted by road distance from the start. I
prepend the coefficient of inconvenience, re-sort the list and take 50 most
inconvenient locations by invoking

#+BEGIN_EXAMPLE
$ <query_34.0690448_-118.292924_20miles.out
   awk '$4 {printf "%f %f %f %f %f\n",($3-$4)/$4,$1,$2,$3,$4}' |
   sort -n -k1 -r | head -n 50
#+END_EXAMPLE

The output is this:

| Inconvenience |  Latitude |   Longitude | Road distance (m) | Direct distance (m) |
|---------------+-----------+-------------+-------------------+---------------------|
|      1.142052 | 34.068104 | -118.290382 |        549.216980 |          256.397583 |
|      1.139839 | 34.071629 | -118.288956 |        994.499390 |          464.754242 |
|      1.139147 | 34.068066 | -118.290436 |        542.721497 |          253.709305 |
|      1.136799 | 34.068130 | -118.290329 |        554.962891 |          259.716919 |
|      1.127631 | 34.068031 | -118.290466 |        537.980652 |          252.854279 |
|      1.120537 | 34.068153 | -118.290253 |        562.437012 |          265.233337 |
|      1.106771 | 34.067982 | -118.290504 |        531.442017 |          252.254257 |
|      1.103518 | 34.068169 | -118.290184 |        568.985352 |          270.492218 |
|      1.083344 | 34.067940 | -118.290527 |        526.321899 |          252.633179 |
|      1.079027 | 34.068176 | -118.290100 |        576.762024 |          277.419189 |
|      1.041816 | 34.067883 | -118.290543 |        519.805908 |          254.580200 |
|      1.034252 | 34.070259 | -118.291237 |        418.454498 |          205.704315 |
|      1.019096 | 34.071594 | -118.287888 |       1097.392212 |          543.506653 |
|      0.974731 | 34.068214 | -118.289680 |        617.407532 |          312.654022 |
|      0.970095 | 34.068176 | -118.289719 |        611.899475 |          310.593842 |
|      0.917598 | 34.068111 | -118.289383 |        656.267517 |          342.234131 |
|      0.910048 | 34.068165 | -118.289383 |        650.329041 |          340.477783 |
|      0.902491 | 34.068214 | -118.289383 |        644.814758 |          338.931915 |
|      0.770809 | 34.067570 | -118.290543 |        485.023560 |          273.899414 |
|      0.760711 | 34.068214 | -118.288643 |        712.981384 |          404.939484 |
|      0.753344 | 34.068214 | -118.288597 |        717.197876 |          409.045654 |
|      0.750541 | 34.033188 | -118.279716 |       7297.569824 |         4168.751465 |
|      0.747349 | 34.031826 | -118.279968 |       7526.415039 |         4307.333008 |
|      0.743357 | 34.067772 | -118.289474 |        606.347107 |          347.804382 |
|      0.741902 | 34.067787 | -118.289436 |        610.249084 |          350.334900 |
|      0.740024 | 34.067749 | -118.289505 |        602.555115 |          346.291290 |
|      0.739944 | 34.031769 | -118.279823 |       7511.619141 |         4317.161621 |
|      0.738388 | 34.031582 | -118.280746 |       7499.802734 |         4314.228516 |
|      0.737889 | 34.067795 | -118.289398 |        613.863831 |          353.223755 |
|      0.737716 | 34.031742 | -118.279800 |       7507.977051 |         4320.601562 |
|      0.736297 | 34.031372 | -118.280258 |       7550.486816 |         4348.613770 |
|      0.735083 | 34.068108 | -118.288734 |        693.459473 |          399.669403 |
|      0.734607 | 34.067730 | -118.289520 |        600.010803 |          345.905945 |
|      0.732851 | 34.031685 | -118.279747 |       7499.933105 |         4328.088379 |
|      0.730817 | 34.067795 | -118.289352 |        618.080322 |          357.103241 |
|      0.730543 | 34.031654 | -118.279732 |       7496.259766 |         4331.739746 |
|      0.728622 | 34.031628 | -118.279724 |       7493.208496 |         4334.787109 |
|      0.727123 | 34.067707 | -118.289536 |        597.103455 |          345.721344 |
|      0.726802 | 34.031601 | -118.279724 |       7490.239258 |         4337.637207 |
|      0.724309 | 34.031563 | -118.279739 |       7485.770508 |         4341.315430 |
|      0.723138 | 34.067791 | -118.289307 |        622.318115 |          361.153992 |
|      0.722826 | 34.031540 | -118.279755 |       7482.862793 |         4343.366211 |
|      0.722384 | 34.094849 | -118.236145 |      10273.032227 |         5964.425293 |
|      0.721979 | 34.094719 | -118.235779 |      10309.708008 |         5987.128906 |
|      0.721011 | 34.094639 | -118.235474 |      10339.187500 |         6007.625977 |
|      0.720812 | 34.094620 | -118.235405 |      10345.856445 |         6012.193359 |
|      0.720105 | 34.031498 | -118.279778 |       7477.742188 |         4347.258789 |
|      0.720078 | 34.094543 | -118.235138 |      10371.867188 |         6029.880859 |
|      0.719789 | 34.031509 | -118.279755 |       7475.278809 |         4346.624512 |
|      0.719616 | 34.095020 | -118.236320 |      10248.023438 |         5959.484863 |


There are 3 clusters of data. All the stuff < 500m away from the start is mostly
degenerate and uninteresting ([[file:files/culdesacs/montage_34.0691_-118.290_300m_16.gp][source]]):

[[file:files/culdesacs/atstart.png]]

Most of the points are in walkways in Shatto Recreation Center. They're all so
close to the start that any inefficiency is exaggerated by the small direct
distance. I make the rules, so I claim these aren't the least convenient point.


Next we have the points about 4.2km away as the crow flies ([[file:files/culdesacs/montage_34.0324_-118.280_50m_18.gp][source]]):

[[file:files/culdesacs/atsaintjamespark.png]]

These all appear in an improperly-mapped group of sidewalks around Saint James
park: http://www.openstreetmap.org/#map=18/34.03173/-118.27892.

Here the sidewalks appear as separate ways that don't connect with the roads
they abut. So according to the data, connecting to the network of sidewalks can
only happen in one location, making these appear less convenient than they
actually are. (I think these should be removed from OSM, but it looks like the
OSM committee people think that mapping sidewalks as separate ways is
acceptable. OK; it'll be fixed eventually in some way).

The next cluster of data is about 6km away as the crow flies ([[file:files/culdesacs/montage_34.094719_-118.235779_300m_16.gp][source]]):

[[file:files/culdesacs/attayloryard.png]]

These are all at the road connecting to the Metrolink maintenance facility at
Taylor Yard: http://www.openstreetmap.org/#map=17/34.09371/-118.23463. This
makes sense! This location is on the other side of the LA river from Koreatown,
so getting here requires a lengthy detour to the nearest bikeable bridge. The
nearest one (Riverside Drive) is 2.5km by road away, but this is in the opposite
direction from Koreatown. The nearest one in the other direction is Fletcher
Drive, 3.8km by road.

So the least convenient point from New Hampshire / 3rd is at lat/lon
34.094849,-118.236145. This location is 10.3km away by road, but only 6.0km as
the crow flies, for an inconvenience coefficient of 0.72.

** DONE A Kernel Debugging Story                                  :tools:dev:
   CLOSED: [2015-09-05 Sat 14:50]

I was asked by a client to investigate an issue they observed where the
performance of some hardware became dramatically degraded after a kernel update
on the server this hardware was connected to. This device is an external unit
that communicates with the server using an FTDI usb-serial converter. The
program running on the server that talks to this device is a proprietary binary
blob I'll call the =frobnicator=. This blob came from some vendor, and the
client did not have access to sources, or had any real idea about how this
=frobnicator= communicates or what it expects.

For years they were using a stock Linux 3.5 kernel shipped by Ubuntu with no
issues. But upgrading to a stock 3.13 kernel was apparently causing the device
to exhibit glitches, and generally misbehave.

*** Initial investigation

When asked to look into this, I first manipulated the server in various ways to
see how the issue manifests, and what triggers it. I could see that

- The =frobnicator= reports communication integrity (CI) values, and those were
  noticeably reduced on the newer kernel, which would explain the degraded
  performance
- Raising the system CPU load correlated with higher CI degradation, so a loose
  theory could be resource contention, where other tasks are taking resources
  away from the =frobnicator=
- periodically, errors would appear in the frobnicator log file
  =/var/log/frobnicator.log=, and these corresponded with periods of poor
  performance

Now that there was a concrete event that coincided with the poor performance,
this could be traced with a tool such as [[http://www.sysdig.org][sysdig]]. The hope was that one would see
what happened leading up to the reported error, and investigate that. So; first
I recorded a log

#+BEGIN_EXAMPLE
$ sudo sysdig -w frobnicator.scap proc.name = frobnicator
#+END_EXAMPLE

Then found instances where it writes complaints into its log:

#+BEGIN_EXAMPLE
$ sysdig -r frobnicator.scap fd.name contains frobnicator.log

371858 12:01:43.587694261 0 frobnicator (21343) < open fd=10(<f>/var/log/frobnicator.log) name=frobnicator.log(/var/log/frobnicator.log) flags=15(O_APPEND|O_CREAT|O_RDWR) mode=0 
371859 12:01:43.587713415 0 frobnicator (21343) > fstat fd=10(<f>/var/log/frobnicator.log) 
371860 12:01:43.587714435 0 frobnicator (21343) < fstat res=0 
371863 12:01:43.587748893 0 frobnicator (21343) > write fd=10(<f>/var/log/frobnicator.log) size=50 
371864 12:01:43.587779337 0 frobnicator (21343) < write res=51 data=[254542.588] ERROR! OH NO! COULDN'T COMMUNICATE!!! 
371865 12:01:43.587780740 0 frobnicator (21343) > close fd=10(<f>/var/log/frobnicator.log) 
371866 12:01:43.587781852 0 frobnicator (21343) < close res=0 
371872 12:01:43.587824754 0 frobnicator (21343) < open fd=10(<f>/var/log/frobnicator.log) name=frobnicator.log(/var/log/frobnicator.log) flags=15(O_APPEND|O_CREAT|O_RDWR) mode=0 
371873 12:01:43.587831903 0 frobnicator (21343) > fstat fd=10(<f>/var/log/frobnicator.log) 
371874 12:01:43.587832417 0 frobnicator (21343) < fstat res=0 
371877 12:01:43.587838779 0 frobnicator (21343) > write fd=10(<f>/var/log/frobnicator.log) size=48 
371878 12:01:43.587843915 0 frobnicator (21343) < write res=40 data=[254542.588] NOOO!! SOMETHING ELSE WENT WRONG!!! 
371879 12:01:43.587844635 0 frobnicator (21343) > close fd=10(<f>/var/log/frobnicator.log) 
371880 12:01:43.587845018 0 frobnicator (21343) < close res=0 
#+END_EXAMPLE

And then looked at what happened leading up to it, searching for anything
noteworthy:

#+BEGIN_EXAMPLE
$ sysdig -r frobnicator.scap evt.num \>= $((371858-10000)) and evt.num \<= 371858

...
365555 12:01:43.584116007 0 frobnicator (21343) > write fd=7(<f>/dev/ttyUSB2) size=7 
365556 12:01:43.584120946 0 frobnicator (21343) < write res=7 data=....... 
365557 12:01:43.584121497 0 frobnicator (21343) > fcntl fd=7(<f>/dev/ttyUSB2) cmd=5(F_SETFL) 
365558 12:01:43.584121792 0 frobnicator (21343) < fcntl res=0(<f>/dev/pts/3) 
365559 12:01:43.584123089 0 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
365560 12:01:43.584124530 0 frobnicator (21343) < read res=0 data= 
365561 12:01:43.584125440 0 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
365562 12:01:43.584125967 0 frobnicator (21343) < read res=0 data= 
365563 12:01:43.584126441 0 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
365564 12:01:43.584126830 0 frobnicator (21343) < read res=0 data= 
..... lots more hammering read() that returns 0
371853 12:01:43.587643084 0 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
371854 12:01:43.587643466 0 frobnicator (21343) < read res=0 data= 
371855 12:01:43.587651900 0 frobnicator (21343) > stat 
371856 12:01:43.587660887 0 frobnicator (21343) < stat res=0 path=frobnicator.log(/var/log/frobnicator.log) 
371857 12:01:43.587665751 0 frobnicator (21343) > open 
371858 12:01:43.587694261 0 frobnicator (21343) < open fd=10(<f>/var/log/frobnicator.log) name=frobnicator.log(/var/log/frobnicator.log) flags=15(O_APPEND|O_CREAT|O_RDWR) mode=0 
#+END_EXAMPLE

So we wrote something to the device at =/dev/ttyUSB2=, then hammered the port
waiting for a reply, then gave up reading the port without getting any data, and
complained into the log. (This hammering is embarrassing. Apparently the authors
of =frobnicator= have never heard of =select= or =poll=. This yet again supports
my theory that most proprietary software is proprietary not to protect some
brilliant IP from theft, but rather to avoid embarrassment). So presumably some
timeout expired; the =frobnicator= likely expected to get a reply from the
hardware before a certain amount of time elapsed, and this took too long. How
long /did/ it take?

#+BEGIN_EXAMPLE
$ echo $((.587651900 - .584120946))
0.0035309540000000306
#+END_EXAMPLE

So after about 3.5ms, we gave up. The serial device is set at 115200 baud. Let's
say you need 10-bits-worth of time to transmit a single byte (this is about
right probably because of the start and stop bits). To write 7 bytes and get at
least 1 byte back you need

#+BEGIN_EXAMPLE
$ echo $(( 8.*10/115200 ))
0.00069444444444444447
#+END_EXAMPLE

0.7ms just for the communication. So I guess it's not crazy to want a reply
within 3.5ms. Presumably it doesn't fail this way every time? Do we ever read
anything from USB2? Under what circumstances?

#+BEGIN_EXAMPLE
$ sysdig -r frobnicator.scap evt.type = read and fd.name contains USB2 and evt.rawres \!= 0 | head -n 10

10190 12:01:41.803108311 1 frobnicator (21343) < read res=1 data=. 
16753 12:01:41.834119153 1 frobnicator (21343) < read res=1 data=. 
23252 12:01:41.865108212 1 frobnicator (21343) < read res=1 data=. 
29817 12:01:41.896112925 1 frobnicator (21343) < read res=1 data=. 
42142 12:01:41.959126061 1 frobnicator (21343) < read res=1 data=. 
46319 12:01:41.989105762 1 frobnicator (21343) < read res=1 data=. 
52241 12:01:42.020106289 3 frobnicator (21343) < read res=1 data=. 
58206 12:01:42.051112845 3 frobnicator (21343) < read res=1 data=. 
64759 12:01:42.082126350 3 frobnicator (21343) < read res=1 data=. 
71562 12:01:42.113106478 3 frobnicator (21343) < read res=1 data=. 


$ sysdig -r frobnicator.scap evt.type = write and fd.name contains USB2 and evt.num \< 42142 | tail -n1

37288 12:01:41.957115614 1 frobnicator (21343) < write res=7 data=....... 


$ sysdig -r frobnicator.scap evt.num \>= 37288 and evt.num \<= 42142

37288 12:01:41.957115614 1 frobnicator (21343) < write res=7 data=....... 
37289 12:01:41.957116233 1 frobnicator (21343) > fcntl fd=7(<f>/dev/ttyUSB2) cmd=5(F_SETFL) 
37290 12:01:41.957116440 1 frobnicator (21343) < fcntl res=0(<f>/dev/pts/3) 
37291 12:01:41.957117828 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
37292 12:01:41.957118640 1 frobnicator (21343) < read res=0 data= 
37293 12:01:41.957119433 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
37294 12:01:41.957119972 1 frobnicator (21343) < read res=0 data= 
37295 12:01:41.957120373 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
37296 12:01:41.957120901 1 frobnicator (21343) < read res=0 data= 
... again. we hammer the read()
42133 12:01:41.959120974 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
42134 12:01:41.959121368 1 frobnicator (21343) < read res=0 data= 
42135 12:01:41.959121769 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
42136 12:01:41.959122160 1 frobnicator (21343) < read res=0 data= 
42137 12:01:41.959122719 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
42138 12:01:41.959123119 1 frobnicator (21343) < read res=0 data= 
42139 12:01:41.959123690 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
42140 12:01:41.959124311 1 frobnicator (21343) < read res=0 data= 
42141 12:01:41.959124846 1 frobnicator (21343) > read fd=7(<f>/dev/ttyUSB2) size=32767 
42142 12:01:41.959126061 1 frobnicator (21343) < read res=1 data=. 
#+END_EXAMPLE

So that time, we wrote to =/dev/ttyUSB2=, but did receive a response of exactly
one byte. The delay there was

#+BEGIN_EXAMPLE
$ echo $((.959126061 - .957115614 ))
0.0020104469999999708
#+END_EXAMPLE

about 2ms. OK. That's reasonable too. So it very well could be that we complain
if the response comes after 3.5ms. What's the distribution of these response
times?

#+BEGIN_EXAMPLE
$ sysdig -t r -r frobnicator.scap fd.name contains frobnicator.log or \
  \( fd.name contains ttyUSB2 and \
    \( \( evt.type = read and evt.dir = '<' and evt.rawres \!= 0\) or \
       \( evt.type = write and evt.dir = '>' \) \) \) | \
  perl -anE 'if(/write.*USB2/) { $t0 = $F[1]; }
             elsif(defined $t0 && /read|frobnicator.log/ ) { $d = $F[1] - $t0; say $d*1000; $t0 = undef; }' | \
  feedgnuplot --histo 0 --binwidth 0.1 --with boxes --set 'style fill solid border -1' \
              --title "Histogram of /dev/ttyUSB2 reply times" --xlabel "Reply (ms)" -ylabel Count
#+END_EXAMPLE

OK, so we look at the time interval between a write() to =/dev/ttyUSB2= and
either when we read() something from it, or touch =frobnicator.log= in any way.
Looks like the usual time is 1.5ms or so, and we give up at 3.5ms, as expected:

[[file:files/kernel_frobnicator/response_histogram.svg]]

I did the same thing on a working machine running the known-good 3.5 kernel: no
=frobnicator.log= error messages were observed and a response histogram made
this way is clean, and does not ever break 3.5ms. So this is consistent with the
theory that these 3.5ms timeouts are involved in the poor performance we are
seeing.

I should say that we are running vanilla Linux kernel from Ubuntu. It appears
that the =frobnicator= is expecting realtime performance from this kernel, but
*this realtime performance was never guaranteed*. So it's unlikely that the
kernel is doing anything more "wrong" now than it was before, and the
=frobnicator= just has unreasonable expectations. Anyway...

*** Initial kernel tracing

We see that an expected response from the USB device does not arrive, or arrives
too late. Many things could be causing this. I was hoping this is purely a
software issue, so I dug into the kernel. This isn't something that =sysdig= can
do, so I switched to =perf= here. I placed lots of probes into the path from the
time when we write() to =/dev/ttyUSB2= to the time when we either receive a
response or give up. Eventually I ended up with this =recordall= script:

#+BEGIN_SRC sh
#!/bin/bash

set -e

VMLINUX=/usr/lib/debug/boot/vmlinux-3.13.0-58-generic
USBSERIALDIR=/usr/lib/debug/lib/modules/3.13.0-58-generic/kernel/drivers/usb/serial
USBSERIAL=$USBSERIALDIR/usbserial.ko
FTDI=$USBSERIALDIR/ftdi_sio.ko
SOURCE=/chattel/linux

sudo perf probe --del '*' || true # let this fail

sudo perf probe -k $VMLINUX -s $SOURCE -m $FTDI      --add 'ftdi_process_read_urb urb->actual_length urb->status urb->transfer_buffer_length'
sudo perf probe -k $VMLINUX -s $SOURCE -m $FTDI      --add 'ftdi_process_packet len'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'tty_insert_flip_string_fixed_flag flag size'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'tty_schedule_flip'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'tty_schedule_flip_ret=tty_schedule_flip:6 %ax'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'flush_to_ldisc'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'receive_buf'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'n_tty_receive_buf2'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'n_tty_receive_buf_common'
sudo perf probe -k $VMLINUX -s $SOURCE               --add 'copy_from_read_buf'
sudo perf probe -k $VMLINUX -s $SOURCE               --add tty_flush_to_ldisc


sudo perf record \
     -esched:sched_{kthread_stop,kthread_stop_ret,wakeup,wakeup_new,switch,migrate_task,process_free,process_exit,wait_task,process_wait,process_fork,process_exec,stat_wait,stat_sleep,stat_iowait,stat_blocked,stat_runtime,pi_setprio,process_hang} \
     -eirq:irq_handler_{entry,exit} \
     -eirq:softirq_{entry,exit} \
     -e cs \
     -e syscalls:sys_enter_write --filter "fd==7" \
     -e syscalls:sys_enter_read  --filter "fd==7" \
     -e syscalls:sys_enter_ppoll \
     -e syscalls:sys_exit_ppoll \
     -e syscalls:sys_exit_read  --filter "ret==0 || ret==1" \
     -eprobe:ftdi_process_read_urb \
     -eprobe:ftdi_process_packet \
     -eprobe:tty_insert_flip_string_fixed_flag \
     -eprobe:tty_schedule_flip{,_ret} \
     -eprobe:copy_from_read_buf \
     -eprobe:{flush_to_ldisc,receive_buf{,_1}} \
     -eprobe:n_tty_receive_buf{2,_common} \
     -eprobe:tty_flush_to_ldisc \
     -eworkqueue:workqueue_{queue_work,activate_work,execute_start,execute_end} \
     -Ra sleep 1; sudo chown user:user perf.data

perf script | ./script_filter.pl > /tmp/`uname -n`.fg
#+END_SRC

This assumes that kernel debug symbols are available, which is one area where
Ubuntu is actually better than Debian:
[[https://wiki.ubuntu.com/Debug%20Symbol%20Packages]]. Note that the probes look at
read() and write(), some USB stuff, some TTY stuff and some process scheduling
stuff. I was adding probes one at a time as I was chasing the failure, and this
is what was there at the end; I didn't just decided to probe this set of things
from the start. After we set up the dynamic tracepoints with =perf probe=, we
record 1 second of trace data with =perf record=; this writes a binary
=perf.data= file, that can be read by =perf report= (to get statistics) and
=perf script= (to look at each individual trace).

Once we have a trace, a visualization script is used to convert it into
something that [[http://github.com/dkogan/feedgnuplot][=feedgnuplot=]] can visualize. Modulo small visualization tweaks,
=script_filter.pl= looks like this:

#+BEGIN_SRC perl
#!/usr/bin/perl

use warnings;
use feature ':5.10';


$i = 0;
my $machine = `uname -n`;
chomp $machine;
say "#!/usr/bin/feedgnuplot --dom --datai --auto --with 'points ps 2' --style cpu 'with labels' --rangesize cpu 2 --set 'key Left'";

my $arg = $ARGV[0];
$print_orig_if = qr/$arg/ if defined $arg;

while(<STDIN>)
{
    chomp;

    next unless /^ /;

    my @F = split;
    ($proc,$cpu,$t,$id, $arg) = @F[0,2,3,4,5];
    $cpu =~ s/\[(.*?)\]/$1/g; $cpu = 0 + $cpu;
    $id =~ s/probe//;
    $id =~ s/syscalls://;
    $id =~ s/workqueue://;
    $id =~ s/irq://;
    $id =~ s/sched://;
    $id =~ s/://g;
    $t =~ s/://g;

    next if $id =~ /^sys_/ && $proc ne 'frobnicator';

    # id mangling
    if ( $id eq 'sys_exit_read' )
    {
        $id .= ";ret=$arg";
    }

    my ($post) = /$id(.*)/;
    if ($post)
    {
        my @args = $post =~ /[0-9a-zA-Z_]+ *= *[0-9a-zx\.]+/g;
        for my $arg (@args)
        {
            $arg =~ s/[ \.]//g;
            $id .= ";$arg";
        }
    }



    if ( $id eq 'sys_exit_read;ret=0x0' && $proc eq 'frobnicator')
    {
        if ($last_exit_read_0)
        { $t0 = undef; }
        else
        { $last_exit_read_0 = 1; }
    }
    elsif ( $id !~ /n_tty_read|r49|tty_put_user|read_/ and $proc eq 'frobnicator')
    {
        $last_exit_read_0 = 0;
    }

    if (/sys_enter_write.*count: [0x]*7$/)
    {
        $t0 = $t;
        $i++;
    }

    if ( defined $t0 )
    {
        $d = ($t-$t0) / 1e-3;

        if( $id !~ /^ftdi|tty|workqueue_queue_work|workqueue_activate_work|irq/ && $proc !~ /frobnicator|swapper/ )
        {
            $id .= ";proc=$proc";
        }

        say "$i $id $d";

        my $icpu = $i + 0.1 + $cpu/50;
        $cpu .= ";$proc" if $id =~/actual_length=3/;
        say "$icpu cpu $d $cpu";

        say STDERR "processed: '$i(t0=$t0) $id $d', orig line: $_" if (defined $print_orig_if && $id =~ $print_orig_if);
    }

    if( $id eq 'sys_exit_read;ret=0x1' && $proc eq 'frobnicator')
    {
        $t0 = undef;
    }
}
#+END_SRC

This isn't nicely written at all; it's just a one-off made for this
investigation, and is a one-liner that got out of hand. The tracing procedure
was to run =./recordall= on a machine while it's exhibiting the troublesome
behavior, then execute the self-plotting data file it creates:
=/tmp/MACHINE.fg=. Initially, the results look like this:

[[file:files/kernel_frobnicator/perf_overview.png]]

(raster plot because of a /very/ high point count). The x-axis is each attempt
to write to =/dev/ttyUSB2= and to get a response from it. The y-axis is time, in
ms. The initial =write(/dev/ttyUSB2)= is at t=0, at the bottom of each stack.
Each type of event is plotted with a different symbol. Here I omit the legend
because there're way too many different symbols, and too much stuff going on.
Each sequence ends when we either received data from =/dev/ttyUSB2= or if we
gave up. Giving up means we stop calling read() on the port. Even with this
super busy plot we see that most cycles complete after 1-2ms, with a few ending
a bit after 3.5ms, as we have seen previously. These cycles are the problem. The
plot looks like a solid wall of events primarily because of the hammering read()
that's happening: we constantly try to read() the data, and get 0 bytes back.
Much more can be inferred if we focus on various sections of this data.

If we make the same plot, but focus /only/ on the FTDI stuff, we get this:

[[file:files/kernel_frobnicator/perf_ftdi.svg]]

OK. Much less going on. FTDI USB packets are supposed to come in every 1ms, and
we can see this: =ftdi_process_read_urb= runs in response to the USB interrupt,
and we see stuff happen every 1ms or so. Each FTDI USB packet is supposed to
contain 2 bytes of status, followed by a payload. Thus to get our 1-byte payload
into read(), we must get a USB packet that's 3 bytes long. The /great/ news here
is that we consistently see 3-byte FTDI payloads come in when they're supposed
to, even on cycles where the 3.5ms deadline was missed. For instance, the plot
above showed that cycles 11, 18 and 19 each missed the 3.5ms deadline, but in
this plot we see a 3-byte FTDI packet (blue star, orange square) arrive in time
(at about 2.1ms, 1.95ms and 1.9ms respectively). This means that the hardware
/is/ receiving the data in time, so no need to check the write() latencies,
think about buffering issues, or trace stuff in hardware. Whew. Let's see what
/success/ looks like. If we zoom in on the end of the first stack, we get this:

[[file:files/kernel_frobnicator/perf_zoom_success.svg]]

Here I show the events and which CPU they were executing on. Again, it took a
while to settle on this particular data to plot. As before, we see that the
application repeatedly calls read() which returns with 0: it's asking for data
we do not yet have. In the meantime, we see the 3-byte FTDI packet come in
(=ftdi_process_read_urb= with =actual_length=3=). This results in calls to
various functions, eventually making it to the TTY subsystem:
=tty_insert_flip_string_fixed_flag=. We =tty_schedule_flip= for later
processing; this calls =workqueue_queue_work= and =workqueue_activate_work=.
Then after a bit of time we see this deferred task run:
=workqueue_execute_start= runs with the payload function: =flush_to_ldisc=. This
does more stuff, and eventually we see read() return 1. To clarify, the
corresponding raw events coming out of =perf script= are:

#+BEGIN_EXAMPLE
         swapper     0 [000] 93115.013218: probe:ftdi_process_read_urb: (ffffffffa05868c0) actual_length=3 status=0 transfer_buffer_length=200
         swapper     0 [000] 93115.013219: probe:ftdi_process_packet: (ffffffffa05865a0) len=3
         swapper     0 [000] 93115.013220: probe:tty_insert_flip_string_fixed_flag: (ffffffff814723a0) flag=0 size=1
         swapper     0 [000] 93115.013221: probe:tty_schedule_flip: (ffffffff81472090)
         swapper     0 [000] 93115.013222: workqueue:workqueue_queue_work: work struct=0xffff88081e452810 function=flush_to_ldisc workqueue=0xffff88082ec0ac00 req_cpu=256 cpu=0
         swapper     0 [000] 93115.013223: workqueue:workqueue_activate_work: work struct 0xffff88081e452810
         swapper     0 [000] 93115.013227: probe:tty_schedule_flip_ret: (ffffffff814720bb) arg1=1
     kworker/0:1   142 [000] 93115.013234: workqueue:workqueue_execute_start: work struct 0xffff88081e452810: function flush_to_ldisc
     kworker/0:1   142 [000] 93115.013235: probe:flush_to_ldisc: (ffffffff81472500)
     kworker/0:1   142 [000] 93115.013236: probe:receive_buf: (ffffffff814725b0)
     kworker/0:1   142 [000] 93115.013237: probe:n_tty_receive_buf2: (ffffffff8146efa0)
     kworker/0:1   142 [000] 93115.013238: probe:n_tty_receive_buf_common: (ffffffff8146edc0)
     kworker/0:1   142 [000] 93115.013239: workqueue:workqueue_execute_end: work struct 0xffff88081e452810

     frobnicator 24483 [001] 93115.013238: syscalls:sys_enter_read: fd: 0x00000007, buf: 0x01f417c4, count: 0x00007fff
     frobnicator 24483 [001] 93115.013240: probe:copy_from_read_buf: (ffffffff8146c1f0)
     frobnicator 24483 [001] 93115.013241: probe:copy_from_read_buf: (ffffffff8146c1f0)
     frobnicator 24483 [001] 93115.013242: syscalls:sys_exit_read: 0x1
#+END_EXAMPLE

That makes sense. What about a failing case? Previously we saw cycle 11 time out
at 3.5ms. We do see a 3-byte USB packet arrive, so what's different about the
sequence in cycle 11 that makes things break, compared to cycle 1 where things
work? The events look like this:

[[file:files/kernel_frobnicator/perf_zoom_failure.svg]]

This generally looks similar to what we saw before. The difference is that while
the deferred task /is/ scheduled by =workqueue_queue_work= and
=workqueue_activate_work=, it does /not/ run in time. This is very consistent in
all the traces I've been looking at, and we have found our failure mechanism.

*** CPU allocation and the scheduler

Now that we know that process scheduling is somehow responsible, let's look at
the success and failure plots again, and pay attention to what the scheduler is
doing and what the various CPUs are doing.

In the success shown above, we see that on CPU 1 =frobnicator= is doing its
hammering read(), again and again. In the meantime, CPU 0 is servicing an
interrupt:

- IRQ 16 fires. It's apparently handling both USB (ehci) and ethernet (eth0).
- Both of these are handled
- then the USB is further processed by the soft IRQ handler, which (eventually)
  calls =ftdi_process_read_urb=. The events indicate that the /swapper/ process
  is what CPU 0 was doing before it was interrupted. This is an internal kernel
  *idle* process: i.e. the CPU was not doing anything.
- As before we see the scheduling of the deferred task:
  =workqueue_activate_work=.
- When this task is scheduled we see some scheduler events: =sched_stat_sleep=
  and =sched_wakeup=
- Then the soft IRQ handler exits, and the scheduler runs to decide what CPU 0
  should do next; we see more scheduler events: =sched_stat_wait= and
  =sched_switch=
- The scheduler makes the deferred task run: =workqueue_execute_start= in the
  =kworker/0:1= kernel process. The =0= in the process name means CPU 0, so the
  CPU handling the interrupt is the same as the CPU handling the deferred task.
  I see this consistently even as I change the CPU servicing the interrupt

The raw events for CPU 0 corresponding to the successful task switch are:

#+BEGIN_EXAMPLE
         swapper     0 [000] 93115.013222: workqueue:workqueue_queue_work: work struct=0xffff88081e452810 function=flush_to_ldisc workqueue=0xffff88082ec0ac00 req_cpu=256 cpu=0
         swapper     0 [000] 93115.013223: workqueue:workqueue_activate_work: work struct 0xffff88081e452810
         swapper     0 [000] 93115.013224: sched:sched_stat_sleep: comm=kworker/0:1 pid=142 delay=865313 [ns]
         swapper     0 [000] 93115.013225: sched:sched_wakeup: comm=kworker/0:1 pid=142 prio=120 success=1 target_cpu=000
         swapper     0 [000] 93115.013227: probe:tty_schedule_flip_ret: (ffffffff814720bb) arg1=1
         swapper     0 [000] 93115.013229: irq:softirq_exit: vec=6 [action=TASKLET]
         swapper     0 [000] 93115.013231: sched:sched_stat_wait: comm=kworker/0:1 pid=142 delay=0 [ns]
         swapper     0 [000] 93115.013232: sched:sched_switch: prev_comm=swapper/0 prev_pid=0 prev_prio=120 prev_state=R ==> next_comm=kworker/0:1 next_pid=142 next_prio=120
     kworker/0:1   142 [000] 93115.013234: workqueue:workqueue_execute_start: work struct 0xffff88081e452810: function flush_to_ldisc
#+END_EXAMPLE

=sched_stat_wait= says =delay=0= meaning that the =kworker/0:1= tasks did not
wait at all to be scheduled. As soon as it was ready to run, it was allowed to.
Great.

OK. What about the failure case? The raw events look like this:

#+BEGIN_EXAMPLE
     frobnicator 24483 [002] 93115.322925: syscalls:sys_enter_write: fd: 0x00000007, buf: 0x01f15cf4, count: 0x00000007
            ...
            perf 27484 [000] 93115.324019: workqueue:workqueue_queue_work: work struct=0xffff88081e452810 function=flush_to_ldisc workqueue=0xffff88082ec0ac00 req_cpu=256 cpu=0
            perf 27484 [000] 93115.324020: workqueue:workqueue_activate_work: work struct 0xffff88081e452810
            perf 27484 [000] 93115.324022: sched:sched_stat_sleep: comm=kworker/0:1 pid=142 delay=992381 [ns]
            perf 27484 [000] 93115.324023: sched:sched_wakeup: comm=kworker/0:1 pid=142 prio=120 success=1 target_cpu=000
            perf 27484 [000] 93115.324024: probe:tty_schedule_flip_ret: (ffffffff814720bb) arg1=1
            perf 27484 [000] 93115.324027: irq:softirq_exit: vec=6 [action=TASKLET]

            ... CPU 0 services the USB interrupt many more times. None of the scheduled tasks run until...
                                                
            perf 27484 [000] 93115.326706: sched:sched_stat_runtime: comm=perf pid=27484 runtime=2856449 [ns] vruntime=482196990 [ns]
            perf 27484 [000] 93115.326708: sched:sched_stat_wait: comm=kworker/0:1 pid=142 delay=2683863 [ns]
            perf 27484 [000] 93115.326709: sched:sched_switch: prev_comm=perf prev_pid=27484 prev_prio=120 prev_state=S ==> next_comm=kworker/0:1 next_pid=142 next_prio=120
            perf 27484 [000] 93115.326710: cs:  ffffffff8176300a [unknown] ([kernel.kallsyms])
     kworker/0:1   142 [000] 93115.326712: workqueue:workqueue_execute_start: work struct 0xffff88081e452810: function flush_to_ldisc
#+END_EXAMPLE

That time the hammering read() is on CPU 2 and once again the interrupt is
serviced by CPU 0. When it was interrupted, CPU 0 was not idle, however: it was
running the =perf= process (used to gather the data we're looking at). Here, the
deferred scheduling happens as in the successful case; we see =sched_stat_sleep=
and =sched_wakeup=, but the scheduled process doesn't run, at least not
immediately.

It is significant that the CPU being interrupted was idle in the successful
case: it had nothing else to do other than to service our interrupt. Here it
/did/ have something to do: when the IRQ handler exited, the CPU went back to
=perf=. Eventually =perf= releases the CPU and our task can run. Here the cycle
started at t=93115.322925, the reply was received and the task was scheduled at
t=93115.324019 (1.1ms later). However the CPU wasn't released until
t=93115.326712, which is 3.8ms after the start, and past our 3.5ms deadline.
=sched_stat_wait= even tells us that the task was sitting there for 2.68ms
waiting for the CPU.

This scenario is consistent in all the traces I collected. The interrupt and the
deferred tasks are handled by one particular CPU, while userspace applications
are free to use any CPU they want. If our USB data happens to be delivered at a
time when some user process is using the interrupt-servicing CPU, then
contention and delays can result. If we increase the system CPU load, then the
machine is spending more time crunching its userspace work, and interrupts are
more likely to fire at an inopportune time. I.e. this is all very consistent
with the observed behavior.

This feels like a fairly run-of-the-mill issue that shouldn't have been exposed
by a kernel upgrade. Why did this work on the older Linux 3.5 boxes at all? It
appears that the machines that the client is running Linux 3.13 on service the
USB interrupt on CPU 0, and this CPU may be used by any user process also. For
whatever reason, when these machines boot into Linux 3.5 instead, the USB
interrupt is serviced on CPU 12, and in practice this core is only used by the
=frobnicator= process. This mapping isn't enforced by any setting anywhere, but
it is what I'm observing. Perhaps there's some heuristic to put user processes
on the same CPU core that services most of the interrupts delivering that
process's data. While waiting for data, the =frobnicator= process spends all its
time calling read(), which in turn goes directly into the kernel. This hammering
read() behavior likely doesn't take priority over the workqueue in the kernel,
and our data can be delivered in a timely fashion. So it appears that on the
older kernel there was some sort of de-facto CPU segregation going on that
allowed the workqueue tasks to run on an uncontested CPU. I.e. we were getting
lucky.

*** Mitigation

We have a CPU contention issue. Potential fixes:

1. Make sure the CPU servicing the interrupt is never busy when an interrupt
   fires
2. Set up the process priorities to prioritize the workqueue above other user
   processes

**** Segregation

The older kernel effectively implements the first option above: the CPU
servicing the interrupt is never doing anything intensive, and can turn around
our data quickly. We can mimic this by using the =taskset= command to explicitly
set the CPU affinities of each process on the machine to stay off the CPU
servicing the USB interrupt. This CPU is controlled by the
=/proc/irq/N/smp_affinity_list= file where =N= is the IRQ number (16 on the box
the traces came from). As expected, this resolves the =frobnicator= performance
issues. This however is quite a big hammer: a whole CPU core becomes unavailable
for any other system work. So it's a good validation of the debugging so far,
but I want a better solution

**** Prioritization

As seen before, during a failure we have:

- CPU is running process =x=
- USB interrupt comes in
- Interrupt schedules deferred task
- Interrupt exits, scheduler runs, CPU goes back to =x=

We want the scheduler to make a different choice: instead of going back to the
process that was running previously, it should service our workqueue /first/.
This can be done by bumping the priority of the process servicing the workqueue.
The tricky thing is that this process is /not/ the =frobnicator=, but a
=kworker/a:b= process. As seen from the traces, on that box this process was
=kworker/0:1=. Setting a very high realtime priority to that process makes the
problem go away also:

#+BEGIN_EXAMPLE
$ chrt -p -r 99 $(pgrep kworker/0:1)
#+END_EXAMPLE

Something like this is a much better solution, since it doesn't sacrifice any
resources, and simply prioritizes the work that has tighter timing requirements
than the other tasks.

This exact command isn't a perfect final solution either, however. I don't like
touching the priority of an internal kernel process, since a later kernel change
could break the way this works. This change will have the desired effect /only/
if the =kworker= process does not change. It very well could change if the
=smp_affinity_list= contains multiple CPUs, or if something touches that file.
And the =kworker= doesn't necessary have to run on the same CPU as the
interrupt, it just happens that way; a later kernel could change that. I don't
see any kernel options to control this stuff in a better way, however. Linux 3.9
introduced higher-priority kworkers (=kworker/0:1H= for instance), but as far as
I can tell I'd need to patch the kernel to make the FTDI stack use them. That's
inherently maintenance-full. So in the end the recommendation was to

- disable the =irqbalance= daemon, which was setting the =smp_affinity_list= as
  the machine was running
- set the =smp_affinity_list= to an arbitrary, single CPU
- raise the priority of the corresponding =kworker= process

This seems to work. More system testing is required in case =irqbalance= was
actually doing something useful that people weren't aware of (this comes by
default on Ubuntu, and it's not obvious if it's a net positive or not). Any
kernel update could break something, and more testing would be required, but
that was probably assumed from the start anyway. OK. Done.

** DONE Bike headlight circuit. Again                                    :EE:
   CLOSED: [2015-09-18 Fri 22:14]

[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Bike headlight circuit continued")){/lisp}][The last time]] I dealt with this stupid headlight, I made things work with some
ridiculous construction techniques. Well, those construction techniques finally
made something die. I still had a full set of parts, and the schematics in an
EDA tool, so I laid out a PCB to try again with more reasonable construction
techniques. The schematics and layout are in their own [[https://github.com/dkogan/flashlight_regulator][git repo]]. The tools were
=gschem= and =pcb= from the gEDA project. Somebody told me to look at [[http://www.oshpark.com][oshpark]]
for the fabrication, and 3 copies of my board cost me $4. Deal. The fabbed
boards look nice:

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/shipped.jpg]]

And assembly wasn't a giant pain:

#+ATTR_HTML: :width 80%
[[file:files/Dorcylight/stuffed.jpg]]

If /this/ breaks for some reason, I /really/ should just buy another light.

** DONE Debugging GNU Emacs memory leaks (part 1) :tools:dev:emacs:memoryleaks:
   CLOSED: [2015-09-19 Sat 23:40]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Debugging GNU Emacs memory leaks (part 2)")){/lisp}][Debugging GNU Emacs memory leaks (part 2)]]
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Memory leak debugging tools")){/lisp}][Memory leak debugging tools]]
#+end_o_blog_alert

*** Overview

Like many people I run my emacs as a long-running daemon. This allows for the
clients to start quickly, and for state to be preserved as long as the emacs
daemon is running. This works great. However, in the last year or so I've had a
very rough time doing this: something in emacs leaks memory, eats all the RAM on
my machine and at best I have to kill emacs, or at worst restart my whole
machine, since "swapping" can make it unresponsive. It's quite difficult to
debug this, since it's not obvious when memory is leaking in a long-running
process. On top of that, emacs is a lisp vm with its own GC, so it's not even
completely clear when the =free= happens, or if the memory is then returned to
the OS or not. To make it even worse, I couldn't create a reproducible test case
that would reliably leak memory quickly. If such a test existed, one could then
attempt to debug. It was only clear that during normal use memory consumption
would steadily increase. I asked on the =emacs-devel= mailing list a while back
without any obvious results:

https://lists.gnu.org/archive/html/emacs-devel/2015-02/msg00705.html

*** A leak plugged

Many months later I finally figured out how to make it leak on command, and the
results are described below and on the mailing list:

https://lists.gnu.org/archive/html/emacs-devel/2015-09/msg00619.html

Apparently starting up a daemon and then repeatedly creating/destroying a client
frame made the memory use consistently climb. The following =zsh= snippet
tickles this bug:

#+BEGIN_SRC bash
$ emacs --daemon

$ while true; do
    for i in `seq 10`; do
      timeout 5 emacsclient -a '' -c & ;
    done;
    sleep 10;
  done
#+END_SRC

The memory use could be monitored with this bit of =zsh=:

#+BEGIN_SRC bash
$ while true; do
    ps -h -p `pidof emacs` -O rss; sleep 1;
  done
#+END_SRC

The leak was visible both with =emacs -Q= (don't load any user configuration)
and with =emacs= (load my full configuration), but the leak was much more
pronounced if my configuration was loaded. I then bisected my configuration to
find the bit that was causing the leak, and I found it: =winner-mode=.

Apparently =winner-mode= keeps a list of all active frames, but it doesn't clean
dead frames off of this list. In a long-running daemon workflow frames are
created and destroyed all the time, so this list ends up keeping references to
data structures that are no longer active. This in turn prevents the GC from
cleaning up the associated memory. A simple patch to =winner-mode= fixes this,
and we can clearly see the results:

#+ATTR_HTML: :width 80%
[[file:files/emacs_leak_debugging/winner/winner.svg]]

So I fixed /a/ memory leak. It's not obvious that this is /the/ memory leak that
I'm feeling most. And clearly there are other leaks, since the memory
consumption is growing even with no configuration loaded at all. Still, we're on
our way.

** DONE Debugging GNU Emacs memory leaks (part 2) :tools:dev:emacs:memoryleaks:
   CLOSED: [2015-10-01 Thu 16:14]

#+begin_o_blog_alert info Follow-up posts
[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Memory leak debugging tools")){/lisp}][Memory leak debugging tools]]
#+end_o_blog_alert

I'm continuing to look for and plug memory leaks. The [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Debugging GNU Emacs memory leaks (part 1)")){/lisp}][previous effort]]
successfully plugged a leak, but there's much more to find here. I found some
more stuff, which is documented here. The corresponding bug report is

http://debbugs.gnu.org/cgi/bugreport.cgi?bug=21556

*** Preliminaries

I'm instrumenting all allocation/deallocation functions, and analyzing the data
with some simple (and not-yet-nice) tools to look for anomalies. I need to
generate backtraces, so I use [[https://perf.wiki.kernel.org/][=perf=]] as the main instrumentation tool. Generally
backtraces are computed from frame pointers, but all =-O= optimization levels in
=gcc= strip those out, so I build my own emacs with =-fno-omit-frame-pointer= to
keep them in place. It is possible to make backtraces with =perf= even without
frame pointers (using DWARF debug information), but this results in much larger
trace files. This extra load generally overwhelms my machine, and important data
ends up culled at data-collection time.

I'm using the lucid X11 widgets for my emacs.

I'm on Debian/sid, so debug symbols are generally split off into separate
packages. It appears that =perf= has a bug, and needs a patch to find these
properly. I did that and posted on LKML:
http://lkml.iu.edu/hypermail/linux/kernel/1509.0/04006.html

*** Test configuration

I set up emacs to continually leak memory. Here I do that by running

#+BEGIN_SRC bash
emacs -Q --daemon
while true; do timeout 1 emacsclient -a '' -c; sleep 1; done
#+END_SRC

This is similar to the setup in the previous post, but I create one client frame
at a time instead of 10 at a time. The sequence is 1-second-with-frame followed
by 1-second-without-frame; a new frame pops up every 2 seconds. This appears to
be sufficient as the memory leaks fairly quickly. Note that there are at least
two different leaks here. One of the leaks results from an X error:

http://debbugs.gnu.org/cgi/bugreport.cgi?bug=21509

Here I do not chase that leak, but look for the non-X-error-caused leaks. I
sample the memory consumption with this zsh snippet:

#+BEGIN_SRC bash
while (true) { ps -h -p `pidof emacs` -O rss; sleep 1 } | awk '{print $2;}'
#+END_SRC

and it looks like this:

#+ATTR_HTML: :width 80%
[[file:files/emacs_leak_debugging/first_client/first_client.svg]]

This leak is about 90kB/s or 180kB/frame! As this runs, I make a record of all
memory allocation operations. I use =perf= to do this:

#+BEGIN_SRC bash
perf probe --del '*'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'malloc=malloc bytes'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'malloc_ret=malloc%return $retval'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'calloc=calloc elem_size n'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'calloc_ret=calloc%return $retval'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'realloc=realloc oldmem bytes'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'realloc_ret=realloc%return $retval'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'aligned_alloc alignment bytes'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'aligned_alloc_ret=aligned_alloc%return $retval'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'posix_memalign alignment size'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'posix_memalign_ret=posix_memalign%return $retval'
perf probe -x /lib/x86_64-linux-gnu/libc-2.19.so --add 'free mem'

timeout 40 perf record -m512 -r50 -g --call-graph=fp -p `pidof emacs` \
                       -eprobe_libc:{free,{malloc,calloc,realloc}{,_ret}} \
                       -eprobe_libc:aligned_alloc{,_1,_ret} \
                       -eprobe_libc:posix_memalign{,_ret}
#+END_SRC

So I probe all the memory allocation/deallocation operations and log all
inputs/outputs. This creates a (very large) =perf.data= file. Backtraces are
available if frame pointers are available, so a =malloc()= called directly from
emacs has a backtrace (because I built with =-fno-omit-frame-pointer=). However
=malloc()= that goes through something like =libgtk= first does not. =perf= can
get overloaded and not write all the data, and I make sure to only keep full
data sets.

I =perf record= for 40 seconds. Since I make a new frame every 2 seconds, about
20 new clients were created in that time.

I now convert the log file to a human-readable one:

#+BEGIN_SRC bash
perf script > script
#+END_SRC

This makes a human-readable, but even larger file.

*** Analysis

I now run the =script= file through a =parse_script.pl= tool to follow all
allocations, and report any that have not been freed:

#+BEGIN_SRC perl
#!/usr/bin/perl

use strict;
use warnings;


use feature 'say';


my $Nbytes_allocated = 0;
my %allocated;

my ($prev_addr, $prev_ret, $prev_type, $prev_realloc0, $prev_realloc0_addr, $prev_realloc0_bytes);
my $allocating;


while(<>)
{
    next unless /probe_libc:([^:]+)/;

    my $type = $1;
    my $ret = $type =~ /_ret$/;
    $type =~ s/_ret$//;


    if ( $ret && !( !$prev_ret && $type eq $prev_type) &&
         !($prev_realloc0 && $prev_type eq 'malloc' && $prev_ret && $type eq 'realloc') ) {
        die "$type ret, but prev wasn't a corresponding !ret";
    }
    elsif ( !$ret && !$prev_ret &&
            !($prev_realloc0 && $prev_type eq 'realloc' && !$prev_ret && $type eq 'malloc') &&
            $. > 1) {
        die "$type !ret following another !ret";
    }
    elsif ( $prev_realloc0 && !($type eq 'malloc' || $type eq 'realloc'))
    {
        die "realloc(0, N) must be followed by malloc(N)";
    }
    elsif ( !$ret )
    {
        if ($type eq 'malloc' && /bytes=([0-9a-z]+)/)
        {
            $allocating = hex $1;
            if ( $prev_realloc0 && $allocating != $prev_realloc0_bytes )
            {
                die "realloc(0, N) must be followed by malloc(N)";
            }
        }
        elsif ($type eq 'calloc' && /elem_size=([0-9a-z]+).*n=([0-9a-z]+)/)
        {
            $allocating = (hex $1) * (hex $2);
        }
        elsif ($type eq 'aligned_alloc' && /bytes=([0-9a-z]+)/)
        {
            $allocating = hex $1;
        }
        elsif ($type eq 'realloc' && /oldmem=([0-9a-z]+).*bytes=([0-9a-z]+)/)
        {
            if ( hex($1) == 0 )
            {
                # realloc(0, xxx) is always mapped to a malloc apparently. I treat
                # this specially
                $prev_realloc0       = 1;
                $prev_realloc0_bytes = hex $2;
            }
            else
            {
                $allocating = hex $2;
                $prev_addr = $1;
            }
        }
        elsif ($type eq 'free' && /mem=([0-9a-z]+)/)
        {
            if ( hex($1) != 0)  # free(0) does nothing
            {
                if (!defined $allocated{$1})
                {
                    say "Unallocated free at $1. Line $.";
                }
                else
                {
                    $Nbytes_allocated -= $allocated{$1}{bytes};
                    delete $allocated{$1};
                }
            }

            $ret = 1;           # free has no free-ret so I set that now
        }
        else
        {
            say "Unknown !ret line: '$_'";
            exit;
        }
    }
    elsif ( $ret )
    {
        if ( !/arg1=([0-9a-z]+)/ )
        {
            die "Ret didn't get arg1";
        }

        my $addr = $1;

        if ( hex($addr) == 0 )
        {
            say "$type returned NULL. Giving up";
            exit;
        }
        elsif ( $type =~ /^(?:[cm]alloc|aligned_alloc)$/ )
        {
            if (defined $allocated{$addr})
            {
                say "Double alloc at $addr. Line $.";
            }
            else
            {
                $allocated{$addr}{bytes} = $allocating;
                $allocated{$addr}{line} = $.;
                $Nbytes_allocated += $allocating;
            }

            if ( $prev_realloc0 && $type eq 'malloc')
            {
                $prev_realloc0_addr = $addr;
            }
        }
        elsif ( $type eq 'realloc' )
        {
            if ( $prev_realloc0 )
            {
                if ( $addr ne $prev_realloc0_addr )
                {
                    die "realloc(0, N) must be followed by malloc(N); differing addr";
                }

                $prev_realloc0       = undef;
                $prev_realloc0_addr  = undef;
                $prev_realloc0_bytes = undef;
            }
            else
            {
                my $prev0 = (hex($prev_addr) == 0);

                if (!$prev0 && !defined $allocated{$prev_addr})
                {
                    say "realloc not alloced at $prev_addr. Line $.";
                    $prev0 = 1;
                }

                if ($addr ne $prev_addr && defined $allocated{$addr})
                {
                    say "Double realloc at $addr. Line $.";
                }

                if ( !$prev0 )
                {
                    $Nbytes_allocated -= $allocated{$prev_addr}{bytes};
                    delete $allocated{$prev_addr};
                }

                $allocated{$addr}{bytes} = $allocating;
                $allocated{$addr}{line} = $.;
                $Nbytes_allocated += $allocating;
            }
        }
        else
        {
            say "Unknown ret line: '$_'";
            exit;
        }


        $allocating = undef;
    }


    $prev_type = $type;
    $prev_ret = $ret;
}


$Nbytes_allocated /= 1e6;
say "Total allocated: $Nbytes_allocated MB";
say '';

for my $addr ( sort { $allocated{$a}{line} <=> $allocated{$b}{line}} keys %allocated )
{
    my ($bytes,$line) = ($allocated{$addr}{bytes},
                         $allocated{$addr}{line});
    say "Leaked " . sprintf('%5d', $bytes) . " bytes at line $line ($addr)";
}
#+END_SRC

Allocations reported as leaky by this tool aren't necessarily leaky; I sampled
for a finite amount of time, and maybe the memory hasn't been freed /yet/.
However anything that has been allocated near the beginning of the log file, and
hasn't been freed by the end is potentially a leak, and should be investigated.
I make a plot of leak size vs line number:

#+BEGIN_SRC bash
<leaks awk '$1=="Leaked" {print $6,$2}' | feedgnuplot --domain --points --xlabel 'script line number' --ylabel 'size leaked'
#+END_SRC

and the result looks like this:

#+ATTR_HTML: :width 80%
[[file:files/emacs_leak_debugging/first_client/leaks.svg]]

Note that it looks like we leak exactly 60984 bytes (can see that when zoom in)
at regular intervals, about 20 times. There are 20 frames created in this log,
so this is a strong indication that whatever happens with those allocations
happens every time we create (or destroy) a client frame. Looking at the
backtrace for those allocations, we see that they're all in

#+BEGIN_EXAMPLE
b62c XftFontOpenInfo (/usr/lib/x86_64-linux-gnu/libXft.so.2.3.2)
#+END_EXAMPLE

So we're leaking a font! Those are large, and plugging this leak would be /very/
good. This was called through a library so no backtrace was available. I trace
this again looking /just/ at these allocations, with full DWARF backtrace
tracking:

#+BEGIN_SRC bash
perf probe -x /usr/lib/x86_64-linux-gnu/libXft.so.2 --add XftFontOpenInfo
timeout 40 perf record -m512 -r50 -g --call-graph=dwarf -p `pidof emacs` -eprobe_libc:malloc --filter bytes==60984
#+END_SRC

Backtrace:

#+BEGIN_EXAMPLE
emacs-snapshot- 17249 [001] 681364.216285: probe_libc:malloc: (7fbff4f01020) bytes=0xee38
            7fbff4f01020 malloc (/lib/x86_64-linux-gnu/libc-2.19.so)
            7fbff9b9d62c XftFontOpenInfo (/usr/lib/x86_64-linux-gnu/libXft.so.2.3.2)
            7fbff9b9e4da XftFontOpenPattern (/usr/lib/x86_64-linux-gnu/libXft.so.2.3.2)
                  5c46b5 xftfont_open (/usr/bin/emacs-snapshot-lucid)
                  5767e8 font_open_entity (/usr/bin/emacs-snapshot-lucid)
                  576e5c font_load_for_lface (/usr/bin/emacs-snapshot-lucid)
                  57703a font_open_by_spec (/usr/bin/emacs-snapshot-lucid)
                  577095 font_open_by_name (/usr/bin/emacs-snapshot-lucid)
                  4d0390 x_default_font_parameter (/usr/bin/emacs-snapshot-lucid)
                  4d7f98 Fx_create_frame (/usr/bin/emacs-snapshot-lucid)
                  55f933 Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f350 funcall_lambda (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  560d13 Fapply (/usr/bin/emacs-snapshot-lucid)
                  55f831 Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  560bb0 Fapply (/usr/bin/emacs-snapshot-lucid)
                  560d8a apply1 (/usr/bin/emacs-snapshot-lucid)
                  55df4a internal_condition_case_1 (/usr/bin/emacs-snapshot-lucid)
                  5992e0 read_process_output (/usr/bin/emacs-snapshot-lucid)
#+END_EXAMPLE

So this is the leaky allocation path. What's the "normal" =free= path that's not
happening? At this point I've played with this enough to notice that these
issues only come up for the first frame. If I create a frame, leave it open, and
then use my loop to create/destroy the /second/ frame, then this leak does not
occur. Probing that situation I know what the proper =free= path is:

#+BEGIN_EXAMPLE
            7fbd19a85660 free (/lib/x86_64-linux-gnu/libc-2.19.so)
            7fbd1e730af4 XftFontManageMemory (/usr/lib/x86_64-linux-gnu/libXft.so.2.3.2)
            7fbd1e72cab7 _XftCloseDisplay (/usr/lib/x86_64-linux-gnu/libXft.so.2.3.2)
            7fbd1eb64e22 XCloseDisplay (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
            7fbd1f4dd3d5 [unknown] (/usr/lib/x86_64-linux-gnu/libXt.so.6.0.0)
            7fbd1f4ddffa XtCloseDisplay (/usr/lib/x86_64-linux-gnu/libXt.so.6.0.0)
                  4c44ce x_delete_terminal (/usr/bin/emacs-snapshot-lucid)
                  4b604b Fdelete_terminal (/usr/bin/emacs-snapshot-lucid)
                  423397 delete_frame (/usr/bin/emacs-snapshot-lucid)
                  55f928 Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  595823 exec_byte_code (/usr/bin/emacs-snapshot-lucid)
                  55f72b Ffuncall (/usr/bin/emacs-snapshot-lucid)
                  560bb0 Fapply (/usr/bin/emacs-snapshot-lucid)
                  560d8a apply1 (/usr/bin/emacs-snapshot-lucid)
                  55df4a internal_condition_case_1 (/usr/bin/emacs-snapshot-lucid)
                  5995cf exec_sentinel (/usr/bin/emacs-snapshot-lucid)
                  59b1d9 status_notify (/usr/bin/emacs-snapshot-lucid)
                  5a1cbf wait_reading_process_output (/usr/bin/emacs-snapshot-lucid)
#+END_EXAMPLE

Fine. Looking at the Xft sources, it looks like actual allocation of fonts
happens in =XftFontOpenInfo()=, past where we check for the font being
already-open. The actual =free()= happens in =XftFontDestroy()=. I put a probe
in each of these two places, then create/destroy a single client frame, and I
see this:

#+BEGIN_EXAMPLE
$ perf probe -x /usr/lib/x86_64-linux-gnu/libXft.so --add 'XftFontOpenInfo_alloc=XftFontOpenInfo:121'

$ perf probe -x /usr/lib/x86_64-linux-gnu/libXft.so --add XftFontDestroy

$ sudo perf record -i -p `pidof emacs` -eprobe_libXft:XftFont{OpenInfo_alloc,Destroy}
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.097 MB perf.data (7 samples) ]

$ perf script
emacs-snapshot- 16337 [000] 12141.927687: probe_libXft:XftFontOpenInfo_alloc: (7f7906641620)
emacs-snapshot- 16337 [000] 12141.961706: probe_libXft:XftFontOpenInfo_alloc: (7f7906641620)
emacs-snapshot- 16337 [000] 12141.968348: probe_libXft:XftFontOpenInfo_alloc: (7f7906641620)
emacs-snapshot- 16337 [000] 12142.010935: probe_libXft:XftFontOpenInfo_alloc: (7f7906641620)
emacs-snapshot- 16337 [001] 12142.531279: probe_libXft:XftFontDestroy: (7f7906640e40)
emacs-snapshot- 16337 [001] 12142.531471: probe_libXft:XftFontDestroy: (7f7906640e40)
emacs-snapshot- 16337 [001] 12142.531553: probe_libXft:XftFontDestroy: (7f7906640e40)
#+END_EXAMPLE

Aha. So with each frame we allocate 4 fonts, but destroy only 3. Looking at the
Xft sources, it looks like there are two different reference counters:
=info->num_unref_fonts= and =font->ref=. When a window is closed,
=XftFontManageMemory()= is called, and fonts are killed, based on
=info->num_unref_fonts=. However, this is updated in =XftFontClose()= based on
=font->ref=. So =XftFontClose()= /must/ be called by emacs and we can't assume
that a window closing will do that for us.

OK. Good to know. It looks like all the =free= calls are all coming from
=delete_frame=, which clears out everything in the font cache. How does this
font cache work? Presumably the leaky font is somehow not being added to the
cache, or somehow not being read from there properly. Let me look at the cache.
Most of the allocations come from =font_load_for_lface=. I use =gdb= to place a
breakpoint, and to look at the cache each time. When I create a frame, I should
see a new cache entry with each allocated font. The =emacs/src/.gdbinit= gdb
script has some useful macros, so in =font_load_for_lface= I can look at the
font cache with

#+BEGIN_EXAMPLE
pp f->output_data.x->display_info->name_list_element
#+END_EXAMPLE

The output of this goes to stderr, which the daemon helpfully redirects to
=/dev/null=. I can still see it with =sysdig= however. It has a buffering
problem that I get around with using =unbuffer=:

#+BEGIN_EXAMPLE
$ sudo unbuffer sysdig -s 60000 proc.pid=`pidof emacs` -c stderr | perl -ne 's/\r/\n/g; s/\n$//; print STDERR $_'     

(":0.0" (x 1) (xft 1))
(":0.0" (x 1)
  (xft 1
    (#<font-spec xft nil Monospace nil iso8859-1 nil nil nil nil nil nil nil ((:name . "monospace-10"))> .
      [#<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 bold oblique normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf" . 0) (:name . "monospace-10"))>
       #<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 bold normal normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf" . 0) (:name . "monospace-10"))>
       #<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 normal normal normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf" . 0) (:name . "monospace-10"))>
       #<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 normal oblique normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf" . 0) (:name . "monospace-10"))>])))
(":0.0" (x 1) (xft 1))
(":0.0" (x 1)
  (xft 1
    (#<font-spec xft unknown DejaVu\ Sans\ Mono nil iso8859-1 nil nil nil nil nil nil nil nil> .
      [#<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 bold oblique normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf" . 0))>
       #<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 bold normal normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf" . 0))>
       #<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 normal normal normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf" . 0))>
       #<font-entity xft unknown DejaVu\ Sans\ Mono nil iso10646-1 normal oblique normal 0 nil 100 0 ((:font-entity "/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf" . 0))>])))
#+END_EXAMPLE

Aha! So I created a frame, 4 fonts were opened as usual, but for some reason
between the 2nd and 3rd font open, the cache was cleared! Since the cache stores
the fonts that eventually get deallocated, these fonts end up leaking! Where
does this happen? Placing breakpoints into the code where it looks like the
clear could be happening doesn't appear to find the culprit, and I want a
watchpoint so that gdb can tell me when the cache is touched improperly. I need
a watchpoint on an emacs lisp object, which isn't something you can do
explicitly, but I can semi-manually find the raw pointer, and set a watchpoint
on it. gdb session:


#+BEGIN_EXAMPLE
[ breakpoint on font_load_for_lface as before]

[ opened frame. breakpoint hit the first time]

(gdb) p f->output_data.x->display_info->name_list_element
$228 = 24121555

(gdb) xlist
$229 = 0x10cc824
Lisp_String
$230 = (struct Lisp_String *) 0x10cc820
":0.0"
---
$231 = 0x17011d3
Lisp_Cons
$232 = (struct Lisp_Cons *) 0x17011d0
{
  car = 0xc4e0, 
  u = {
    cdr = 0x17011c3, 
    chain = 0x17011c3
  }
}
---
$233 = 0x17011a3
Lisp_Cons
$234 = (struct Lisp_Cons *) 0x17011a0
{
  car = 0xc750, 
  u = {
    cdr = 0x1701193, 
    chain = 0x1701193
  }
}
---
nil

(gdb) p $->u.cdr
$235 = 24121747

(gdb) xpr
Lisp_Cons
$236 = (struct Lisp_Cons *) 0x1701190
{
  car = 0x6, 
  u = {
    cdr = 0x0, 
    chain = 0x0
  }
}

(gdb) watch ((struct Lisp_Cons *) 0x1701190)->u.cdr
Hardware watchpoint 24: ((struct Lisp_Cons *) 0x1701190)->u.cdr

[ disable font_load_for_lface breakpoint ]
(gdb) disa 6

(gdb) commands 24
Type commands for breakpoint(s) 24, one per line.
End with a line saying just "end".
>bt 5
>c
>end

(gdb) c
Continuing.
Hardware watchpoint 24: ((struct Lisp_Cons *) 0x1701190)->u.cdr

Old value = 0
New value = 24122275
XSETCDR (c=24121747, n=24122275) at lisp.h:1194
#0  0x000000000054cdad in XSETCDR (c=24121747, n=24122275) at lisp.h:1194
#1  0x000000000060eb2a in font_list_entities (f=0x19b7d20, spec=21168293) at font.c:2786
#2  0x00000000006107f7 in font_find_for_lface (f=0x19b7d20, attrs=0x7ffcdb167570, spec=21052973, c=-1) at font.c:3261
#3  0x0000000000610b80 in font_load_for_lface (f=0x19b7d20, attrs=0x7ffcdb167570, spec=21052973) at font.c:3334
#4  0x0000000000610f72 in font_open_by_spec (f=0x19b7d20, spec=21052973) at font.c:3428
#5  0x0000000000610fe5 in font_open_by_name (f=0x19b7d20, name=17336596) at font.c:3439

Lisp Backtrace:
"x-create-frame" (0xdb1678a0)
"x-create-frame-with-faces" (0xdb167dd8)
0x12b9d80 PVEC_COMPILED
"apply" (0xdb168450)
"frame-creation-function" (0xdb1689f0)
"make-frame" (0xdb168f40)
"make-frame-on-display" (0xdb1694a8)
"server-create-window-system-frame" (0xdb169a78)
"server-process-filter" (0xdb169ff8)
Hardware watchpoint 24: ((struct Lisp_Cons *) 0x1701190)->u.cdr

Old value = 24122275
New value = 0
0x00000000005d2172 in compact_font_cache_entry (entry=24121763) at alloc.c:5313
#0  0x00000000005d2172 in compact_font_cache_entry (entry=24121763) at alloc.c:5313
#1  0x00000000005d221b in compact_font_caches () at alloc.c:5339
#2  0x00000000005d2742 in garbage_collect_1 (end=0x7ffcdb166830) at alloc.c:5515
#3  0x00000000005d2e1d in Fgarbage_collect () at alloc.c:5720
#4  0x000000000054eb21 in maybe_gc () at lisp.h:4515
#5  0x00000000005f638c in Ffuncall (nargs=3, args=0x7ffcdb166988) at eval.c:2584

Lisp Backtrace:
"Automatic GC" (0x0)
"map-keymap" (0xdb166990)
"keymap-canonicalize" (0xdb166f38)
"x-create-frame" (0xdb1678a0)
"x-create-frame-with-faces" (0xdb167dd8)
0x12b9d80 PVEC_COMPILED
"apply" (0xdb168450)
"frame-creation-function" (0xdb1689f0)
"make-frame" (0xdb168f40)
"make-frame-on-display" (0xdb1694a8)
"server-create-window-system-frame" (0xdb169a78)
"server-process-filter" (0xdb169ff8)
Hardware watchpoint 24: ((struct Lisp_Cons *) 0x1701190)->u.cdr

#+END_EXAMPLE


Bam! The font cache was unexpectedly (to me) touched in
=compact_font_cache_entry=. Looking at the font cache before and after this call
makes it clear that this is the call that's erroneously clearing the font cache.
Apparently it's part of the mark/sweep GC that runs in the sweep phase. For
whatever reason the leaked font isn't marked, so the GC is clearing it out from
the cache. This seems wrong, since simply taking it out of the cache will result
in it not being freed later, but that's irrelevant: this font is not unused, so
it should have been marked. Looking at the font marking business, the issue
becomes clear: each font entity object contains a list of fonts. In
=compact_font_cache_entry= we look for marks on the entity and not in any
individual font. Apparently it is possible for an unmarked entity to contain a
marked font, and this case wasn't being checked. And adding this check to
=compact_font_cache_entry= makes this leak go away entirely.

A plot showing the memory usage whilt creating/destroying the first frame before
and after the patch, shifted to start at 0:

#+ATTR_HTML: :width 80%
[[file:files/emacs_leak_debugging/first_client/both.svg]]

The steps are bug http://debbugs.gnu.org/cgi/bugreport.cgi?bug=21509, which now
dominates the memory leaking during this particular test. Between the steps the
memory consumption is now more or less flat, which indicates that this patch is
effective. Furthermore, during my actual use of emacs (as opposed to the testing
use here) I always have multiple frames open, so in real life bug 21509 does not
apply for me at all. There could be other smaller leaks that happen here, but
until 21509 is fixed, they're difficult to see. Done!


#+begin_o_blog_alert info Update
I discovered that bug 21509 can be worked around by turning off the scroll bar.
Here's the same memory usage plot as above, but without bug 21509 confounding
things:

#+ATTR_HTML: :width 80%
[[file:files/emacs_leak_debugging/first_client/both.noscrollbar.svg]]

Looks like the patch drops the leak from 57kB/s (114 kB/frame) to 12kB/s
(24kB/frame). Still work to be done, but this is very good.
#+end_o_blog_alert

** DONE Memory leak debugging tools             :tools:dev:emacs:memoryleaks:
   CLOSED: [2015-10-05 Mon 23:28]

I consolidated the memory-leak-finding tools from [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Debugging GNU Emacs memory leaks (part 1)")){/lisp}][here]] and [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Debugging GNU Emacs memory leaks (part 2)")){/lisp}][here]] into their own
repository: https://github.com/dkogan/memory_leak_instrumentation. It's written
primarily for emacs, but the tools can be easily adapted for anything. The
documentation is copied here:

*** Overview

This is a set of tools used to find memory leaks in long-running applications.
These tools use [[http://perf.wiki.kernel.org/][=perf=]] to instrument all memory allocations/deallocations. These
traces can then be analyzed to find allocated memory that was not properly
cleaned up.

This was written to find memory leaks in emacs daemon sessions. The resulting
bugs and mailing list posts:

- https://lists.gnu.org/archive/html/emacs-devel/2015-09/msg00814.html
- http://debbugs.gnu.org/cgi/bugreport.cgi?bug=21509
- http://debbugs.gnu.org/cgi/bugreport.cgi?bug=21556
- http://debbugs.gnu.org/cgi/bugreport.cgi?bug=21623

Some of the tools are emacs-specific, but some are not. Note that this is all
fairly rough, and the user would want to understand how each tool works to be
able to use it effectively. This is also not well-documented yet, but the links
above, and the text below show examples.

*** Tools
**** Non-emacs-specific

***** =reademacsvar.sh=
Exports some shell variables that all the scripts use to do their thing. Mostly
this is paths, etc.

***** =plotmem.sh=
Used to generate a realtime plot of memory usage of a particular process. This
lets us see the leaks as they happen.

***** =probes.sh=
Creates =perf= probes that we care about. This is all allocation/deallocations,
and anything else we like.

***** =record_alloc.sh=
Runs =perf record= to record all allocations/deallocations.

***** =parse_script.pl=
Reads the output of =perf script=, following all allocations. The output is a
list of all unexpected memory operations (calling =free()= on a pointer that
didn't come from =malloc()= for instance) and a list of all potentially leaky
memory (anything that was allocated but not freed). If we started logging some
time after the process has started, and finished logging before the process has
finished then both of these will have false positives:

- We could see a =free()= of memory that was allocated before we started logging
- We can report a leak for something that was =free()=-ed after we stopped
  logging

So take all output with a grain of salt, and use your best judgement

***** =plotleaks.sh=
Takes the output of =parse_script.pl=, and makes a plot of potential leak sizes
vs input line number. This is useful to quickly see the leaks. For instance,
let's say the emacs session we're tracing leaks 3072 bytes each time a new frame
is created, and that we created 10 frames in a row while running =perf record=.
The plot this script produces would then show 10 points at 3072 evenly spaced
through time. Leaks at the start of the session are most likely to be true ones
(there was time to =free()= the memory), so I generally follow up anything that
leaked lots of memory at the start.

***** =follow_alloc.pl=
Reads the output of =perf script=, and filters out all memory operations that do
not refer to a particular allocation size. This is useful to focus on particular
allocations identified by =plotleaks.sh=. So if =plotleaks.sh= shows lots of
leaks of size 3072, we use this to cut down the log to show only the leaks we
care about.

**** Emacs-specific

***** =daemon.sh=
Starts up a new emacs daemon.

***** =client.sh=
Creates a new client frame.

***** =kill.sh=
Kills the emacs daemon.

***** =loopclient.sh=
Repeatedly creates/destroys a client frame.

***** =show_stderr.sh=
Shows the STDERR output of a process (emacs in this case). This is useful
because the emacs daemon redirects its STDERR to =/dev/null=, but gdb printing
commands such as =pr= and =pp= write to STDERR, and we want to see this output.

*** General notes

When running =perf record=, the =perf= process can be overloaded and drop events
as a result (an error message says this). A larger buffer can help (=-m= option)
at the expense of using more RAM. It also helps to make smaller logs (fewer
things to record, =fp= backtrace generation instead of =DWARF=; see below).

=perf= can generate backtraces in two ways:
- Using the frame-pointer. This is the preferred method, but it only works for
  functions that have a frame pointer. All gcc optimization levels strip this
  out, so rebuild with =-fno-omit-frame-pointer= if possible.
- Using DWARF debug information. This doesn't require frame pointers, but needs
  debug info. Another down side is that this generates much bigger =perf= logs,
  and =perf= is more likely to drop events. Currently =perf= has a bug in that
  it's not able to read the split debug information in Debian packages, so you
  need this patch: http://lkml.iu.edu/hypermail/linux/kernel/1509.0/04006.html

If the binary application being instrumented is rebuilt, probes into that
application need to be deleted and re-added.

*** Example

I'm observing that current build of emacs I'm using leaks memory. I can see this
because I repeatedly create/destroy client frames in one terminal window:

#+BEGIN_EXAMPLE
$ ./daemon.sh
Starting Emacs daemon.

$ ./loopclient.sh   
Waiting for Emacs...
Waiting for Emacs...
Waiting for Emacs...
...
#+END_EXAMPLE

And in another terminal window I look at memory consumption using
=./plotmem.sh=. The plot looks like this:

#+ATTR_HTML: :width 80%
[[file:files/memory_leak_instrumentation/memory.svg]]

The memory use is climbing, so we have a leak. I make a memory consumption log
while =./loopclient.sh= is running:

#+BEGIN_EXAMPLE
$ ./record_alloc.sh 
^C[ perf record: Captured and wrote 36.197 MB perf.data (165038 samples) ]

$ sudo perf script > script
#+END_EXAMPLE

Now I analyze the log, and plot the leaks

#+BEGIN_EXAMPLE
$ ./parse_script.pl < script > leaks

$ ./plotleaks.sh leaks
#+END_EXAMPLE

The leaks look like this:

#+ATTR_HTML: :width 80%
[[file:files/memory_leak_instrumentation/leaks.svg]]

The large leak at the end is a false positive: it just hasn't been freed yet.
Zooming-in to the smaller leaks at the start, I see this:

#+ATTR_HTML: :width 80%
[[file:files/memory_leak_instrumentation/leaks_zoomed.svg]]

So there's a recurring leak of about 3000 bytes. Zooming in more, I see that
these are all leaks of exactly 3072 bytes. Let me trace these leaks
specifically:

#+BEGIN_EXAMPLE
$ ./follow_alloc.pl 3072 < script > script.3072
#+END_EXAMPLE

Looking through this filtered log, I see that the leaks all follow a pattern:

#+BEGIN_EXAMPLE
Line: 124809 Refcount: 1. enter emacs-tst 31381 [001] 609235.683003: probe_libc:malloc_ret: (7f7d99f5e020 <- 7f7d9eac1af0) arg1=0x34d4200
                   24af0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
                 2400009 [unknown] ([unknown])

Line: 124818 Refcount: 0. exit emacs-tst 31381 [001] 609235.683015: probe_libc:free: (7f7d99f5e660) mem=0x34d4200
                   7c660 free (/lib/x86_64-linux-gnu/libc-2.19.so)
        676942363846585f [unknown] ([unknown])

Line: 124916 Refcount: 1. enter emacs-tst 31381 [001] 609235.683321: probe_libc:malloc_ret: (7f7d99f5e020 <- 7f7d9eac1af0) arg1=0x34d4200
                   24af0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
                 2400009 [unknown] ([unknown])

                         ...

Line: 1572738 Refcount: 2. enter emacs-tst 31381 [000] 609237.688972: probe_libc:malloc_ret: (7f7d99f5e020 <- 7f7d9eac1af0) arg1=0xedc980
                   24af0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
                 2400009 [unknown] ([unknown])

Line: 1572747 Refcount: 1. exit emacs-tst 31381 [000] 609237.688984: probe_libc:free: (7f7d99f5e660) mem=0xedc980
                   7c660 free (/lib/x86_64-linux-gnu/libc-2.19.so)
        676942363846585f [unknown] ([unknown])

Line: 1572845 Refcount: 2. enter emacs-tst 31381 [000] 609237.689323: probe_libc:malloc_ret: (7f7d99f5e020 <- 7f7d9eac1af0) arg1=0xedc980
                   24af0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
                 2400009 [unknown] ([unknown])

                         ...

Line: 3000071 Refcount: 3. enter emacs-tst 31381 [000] 609239.698236: probe_libc:malloc_ret: (7f7d99f5e020 <- 7f7d9eac1af0) arg1=0x34eb190
                   24af0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
                 2400009 [unknown] ([unknown])

Line: 3000080 Refcount: 2. exit emacs-tst 31381 [000] 609239.698250: probe_libc:free: (7f7d99f5e660) mem=0x34eb190
                   7c660 free (/lib/x86_64-linux-gnu/libc-2.19.so)
        676942363846585f [unknown] ([unknown])

Line: 3000178 Refcount: 3. enter emacs-tst 31381 [000] 609239.698617: probe_libc:malloc_ret: (7f7d99f5e020 <- 7f7d9eac1af0) arg1=0x34eb190
                   24af0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
                 2400009 [unknown] ([unknown])

                         ...
                         ...
                         ...
#+END_EXAMPLE

So we allocate 3072 bytes in =_XQueryFont=, then we =free()= this, then we
allocate 3072 bytes again in =_XQueryFont=, and this second allocation leaks.
Then time passes as we do these 3 things again. Every time (presumably with each
frame) we leak 3072 bytes. Here the allocation was from =libX11.so=, which
omitted frame pointers and thus we don't have useful backtraces. I make another
trace using DWARF for backtraces. This is heavier, but I can limit logging to
=malloc(3072)= since I now know that's what I care about. This cuts down on the
data that =perf= needs to write:

#+BEGIN_EXAMPLE
$ source reademacsvar.sh

$ sudo perf record ${=RECORD_OPTS} -g --call-graph=dwarf -p ${EMACS_PID} -eprobe_libc:malloc --filter 'bytes==3072'

$ sudo perf script > script
#+END_EXAMPLE

Selecting the =malloc()= paths through =_XQueryFont= I get these:

#+BEGIN_EXAMPLE
emacs-tst  4165 [000] 612746.917886: probe_libc:malloc: (7fbbd7027020) bytes=0xc00
	    7fbbd7027020 malloc (/lib/x86_64-linux-gnu/libc-2.19.so)
	    7fbbdbb8aaf0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
	    7fbbdbb8b7aa XLoadQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
	    7fbbdbb8b54e _XF86LoadQueryLocaleFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
	    7fbbdbb965d0 XLoadFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
	    7fbbdc4f95e5 XtCvtStringToFont (/usr/lib/x86_64-linux-gnu/libXt.so.6.0.0)
	    7fbbdc4f606d [unknown] (/usr/lib/x86_64-linux-gnu/libXt.so.6.0.0)
	    7fbbdc4f6db7 XtCallConverter (/usr/lib/x86_64-linux-gnu/libXt.so.6.0.0)
	          52c818 x_term_init (/tmp/emacs-tst.patched)
	          5381e8 x_display_info_for_name (/tmp/emacs-tst.patched)
	          52d3b7 check_x_display_info (/tmp/emacs-tst.patched)
	          5345c5 Fx_create_frame (/tmp/emacs-tst.patched)
                         ...

emacs-tst  4165 [000] 612746.918039: probe_libc:malloc: (7fbbd7027020) bytes=0xc00
	    7fbbd7027020 malloc (/lib/x86_64-linux-gnu/libc-2.19.so)
	    7fbbdbb8aaf0 _XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
	    7fbbdbb8b2bd XQueryFont (/usr/lib/x86_64-linux-gnu/libX11.so.6.3.0)
	          52c851 x_term_init (/tmp/emacs-tst.patched)
	          5381e8 x_display_info_for_name (/tmp/emacs-tst.patched)
	          52d3b7 check_x_display_info (/tmp/emacs-tst.patched)
	          5345c5 Fx_create_frame (/tmp/emacs-tst.patched)
                         ...
#+END_EXAMPLE

Those 2 appear in clusters, which is consistent with the previous log. The first
is freed, the second leaks. In emacs, both come from =x_term_init()=. Looking at
the source, here are the relevant lines:

#+BEGIN_SRC C
    if (!XtCallConverter (dpy, XtCvtStringToFont, &d, 1, &fr, &to, NULL))
      emacs_abort ();
    if (x_had_errors_p (dpy) || !XQueryFont (dpy, font))
      XrmPutLineResource (&xrdb, "Emacs.dialog.*.font: 9x15");
    /* Do not free XFontStruct returned by the above call to XQueryFont.
       This leads to X protocol errors at XtCloseDisplay (Bug#18403).  */
    x_uncatch_errors ();
#+END_SRC

Oh my. So we leak this on purpose, and that bug report describes why.

*** License

All source released under the terms of the Lesser GNU General Public License,
version 3 or later: https://www.gnu.org/copyleft/lesser.html

** DONE gnuplotlib and broadcasting in numpy          :tools:python:data:dev:
   CLOSED: [2015-10-13 Tue 16:01]

#+begin_o_blog_alert info Follow-up posts
I added generic broadcasting support to numpy: [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "numpysane: a more reasonable numpy")){/lisp}][numpysane: a more reasonable numpy]]
#+end_o_blog_alert

Up to this point, a major part of [[https://github.com/dkogan/PDL-Graphics-Gnuplot][PDL::Graphics::Gnuplot]] that was not supported
in [[https://github.com/dkogan/gnuplotlib][=gnuplotlib=]] was proper [[http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html][broadcasting]] ("threading" in PDL-speak) support. What
is this? Suppose you have 3 1D vectors: =x=, =y= and =y2=, and you want to plot
two curves: =y= vs =x= and =y2= vs =x=. In =gnuplotlib= you can do

#+BEGIN_SRC python
 gp.plot( (x,y), (x,y2) )
#+END_SRC

But then you're repeating the =x=. It would be nicer to consolidate that. You
can join =y= and =y2= into a 2D numpy array:

#+BEGIN_SRC python
 gp.plot( x, np.vstack(y,y2) )
#+END_SRC

Interpreting the arguments in this way is what /broadcasting/ is, and I just
added support for this into =gnuplotlib=.

It would appear that this is something that [[http://pdl.perl.org][PDL]] does much better than numpy, in
general. While in PDL threading is a fundamental concept that's supported widely
across the whole system, in numpy broadcasting only really exists for
fundamental operations (addition, multiplication, etc). Thus numpy lacks an
equivalent of [[http://pdl.perl.org/?docs%3DCore&title%3DPDL::Core#thread_define][=thread_define=]], and I had to implement it myself. Note the
[[https://github.com/dkogan/gnuplotlib/blob/v0.7/gnuplotlib.py#L1471][implementation]] isn't pretty, but it's just complicated by nature.

Something numpy does better than PDL is the wider range of types that can live
in their arrays. I can thus put strings into arrays, and use broadcasting to
plot stuff with different labels and styles. This clearly requires an example.
Let's plot the [[https://en.wikipedia.org/wiki/Conchoid_of_de_Sluze][Conchoids of de Sluze]]:

#+BEGIN_SRC python
import numpy as np
import gnuplotlib as gp

theta = np.linspace(0, 2*np.pi, 1000)  # dim=(  1000,)
a     = np.arange(-4,3)[:, np.newaxis] # dim=(7,1)

gp.plot( theta,
         1./np.cos(theta) + a*np.cos(theta), # broadcasted. dim=(7,1000)

         _with  = 'lines',
         set    = 'polar',
         square = True,
         yrange = [-5,5],
         legend = a.ravel() )
#+END_SRC

This is a plot in polar coordinates. I use broadcasting to generate a numpy
array that has /all/ the =r= values for /all/ my curves in it. This array was
generated by a /single/ command. And I plot it against the /one/ array of
=theta= values, again in a /single/ command. I also use broadcasting to generate
labels. Generally these would be strings, and I can do that with numpy, but here
just printing the numerical value of the =a= parameter is sufficient. The
result:

#+ATTR_HTML: :width 80%
[[file:files/gnuplotlib/conchoids.svg]]

Another example:

#+BEGIN_SRC python
import numpy as np
import gnuplotlib as gp

x,y = np.mgrid[-10:11, -10:11]
z   = np.sqrt(x*x + y*y)
x  = x[:, 2:12]
z  = z[:, 2:12]

gp.plot((np.rollaxis( np.dstack((x,z)), 2,0),
         {'tuplesize': 3,
          'with': np.array(('points palette pt 7','points ps variable pt 6'))}),

        square = 1)

#+END_SRC

Here the dimensions of =x= and =z= end up as (21,10). We stack them together
into an array of dimensions (2,21,10). Furthermore, the =with= key option is
also an array, but with dimension (2,) and two strings in it, indicating how
each broadcast slice should be plotted. We thus plot =x= as a matrix with a
varying color and =z= as a matrix with a varying point size. The result:

#+ATTR_HTML: :width 50%
[[file:files/gnuplotlib/broadcasted.svg]]

Works OK, and this will be very useful.

** DONE Mapping new keys to a modifier with xmodmap                 :desktop:
   CLOSED: [2016-03-09 Wed 10:14]

So even now, after more than 15 years of using X11, I can apparently run into
user-facing corner cases that are weird and look like bugs. I just got an x230
ThinkPad to serve as a portable laptop where my trusty T61 is too bulky.

It has a particularly moronic keyboard. In particular, the menu key has been
replaced by PrintScreen, which is appreciated by exactly nobody:

http://www.google.com/search?q=x230+keyboard&tbm=isch

As before, I want to map this to =Super_R= to talk to my [[http://notion.sourceforge.net/][window manager]]. So I do
just that, naively:

#+BEGIN_SRC sh
xmodmap -e "keycode 107 = Super_R"
#+END_SRC

This appears to work in [[http://notion.sourceforge.net/][notion]], but in some ways it doesn't. It feels like this
key is not recognized as the =mod4= modifier at all times. After some debugging,
this was proved correct. The modifier status before /and/ after the =xmodmap=
invocation above looks like this:

#+BEGIN_EXAMPLE
$ xmodmap -pm | grep mod4

mod4        Super_L (0x85),  Super_R (0x86),  Super_R (0x87)
#+END_EXAMPLE

The values in () are keycodes. Note that the keycode I just added (107 == 0x6b)
is missing. Apparently when adding new modifier keys, you need to re-add the
modifier after setting up the new keycode. So what I was supposed to do above
was this:

#+BEGIN_SRC sh
xmodmap -e "remove mod4 = Super_R" -e "keycode 107 = Super_R" -e "add mod4 = Super_R"
#+END_SRC

With that I do see the new keycode in the =mod4= map, and things work correctly:

#+BEGIN_EXAMPLE
$ xmodmap -pm | grep mod4

mod4        Super_R (0x6b),  Super_L (0x85),  Super_R (0x86),  Super_R (0x87)
#+END_EXAMPLE

I don't doubt this is a well known behavior, but I really wish it was less
user-hostile and I didn't have to spend over an hour tracking this down.

** DONE Perf misreporting of user-space function arguments            :tools:
   CLOSED: [2016-03-24 Thu 21:27]

In a [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "User-space introspection with Linux perf")){/lisp}][previous post]] I noticed that [[http://perf.wiki.kernel.org][=perf=]] can misreport arguments to user-space
functions. I just hit this issue again, and dug into it a bit. Unsurprisingly,
it is a bug. It (probably) only affects unoptimized code, which could result in
it being undetected/unfixed for this long. I don't really know to fix it, and if
others don't either, then that's another reason. In any case, I sent the [[http://article.gmane.org/gmane.linux.kernel/2183882][report]]
to the mailing list, so we'll see what they say.

The bug:

Let's say I have a trivial C program =tst.c=:

#+BEGIN_SRC C
#include <stdio.h>
#include <stdlib.h>
int f(int x)
{
    return !printf("%d\n", x);
}
int main(int argc, char* argv[])
{
    return f(argc);
}
#+END_SRC

I ask =perf= to trap all calls to =f()= and to give me the value of the =x=
argument:

#+BEGIN_EXAMPLE
$ gcc-5 -g -o tst tst.c &&
  perf probe -x tst --add 'f x' &&
  perf record -eprobe_tst:f ./tst 2 3 4 &&
  perf script    

  .....

  tst 24626 [003] 98586.485680: probe_tst:f: (4004e6) x=0
#+END_EXAMPLE

Note that the value passed to =f()= was 4, but perf reported it as 0 instead.

If I look at what uprobes was actually asked to report, I see this:

#+BEGIN_EXAMPLE
$ cat /sys/kernel/debug/tracing/uprobe_events 

p:probe_tst/f /tmp/tst:0x00000000000004e6 x=-12(%sp):s32
#+END_EXAMPLE

The corresponding disassembly is:

#+BEGIN_EXAMPLE
$ objdump -d tst | awk '/<f>:/,/^$/'

00000000004004e6 <f>:
  4004e6:       55                      push   %rbp
  4004e7:       48 89 e5                mov    %rsp,%rbp
  4004ea:       48 83 ec 10             sub    $0x10,%rsp
  4004ee:       89 7d fc                mov    %edi,-0x4(%rbp)
  4004f1:       8b 45 fc                mov    -0x4(%rbp),%eax
  4004f4:       89 c6                   mov    %eax,%esi
  4004f6:       bf b4 05 40 00          mov    $0x4005b4,%edi
  4004fb:       b8 00 00 00 00          mov    $0x0,%eax
  400500:       e8 bb fe ff ff          callq  4003c0 <printf@plt>
  400505:       85 c0                   test   %eax,%eax
  400507:       0f 94 c0                sete   %al
  40050a:       0f b6 c0                movzbl %al,%eax
  40050d:       c9                      leaveq 
  40050e:       c3                      retq   
#+END_EXAMPLE

So uprobes was looking at the argument as a local stack variable. However the
trap was placed at the start of the function, where the stack variable wasn't
yet available (the argument is still in %di).

This doesn't happen with optimized code, because (at least in this simple
example) the variable is simply kept in %di, the DWARF data indicates this, and
perf picks that up.

Possible ways to address this:

1. place the trace after the local variables are set (4004f1 in this example)
2. Look at %di instead of the stack variable

I didn't see anything obvious in the DWARF data to tell us how exactly to do
either of these (but I can imagine I missed something). There's also a concern
that both these are vulnerable to the optimizer coming in, and making them not
work. I await the wisdom of the mailing list.

** DONE glibc malloc inefficiency                     :tools:dev:memoryleaks:
   CLOSED: [2016-04-08 Fri 17:35]

So despite the previous efforts ([[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Debugging GNU Emacs memory leaks (part 1)")){/lisp}][this]], [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Debugging GNU Emacs memory leaks (part 2)")){/lisp}][this]] and [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Memory leak debugging tools")){/lisp}][this]]) my emacs sessions are
still taking up too much memory, causing me trouble. I dumped more time into it,
and while I still think there're leaks somewhere, it's now clear that there's a
major confounding factor: the specifics of the =malloc= implementation in glibc
I'm using (currently version 2.22-5 from Debian).

Like everything else, =malloc= isn't magical. In particular, it can suffer from
memory fragmentation issues, as one would expect. Furthermore, when you call
=free=, the memory may or may not be given back to the kernel immediately.
Memory can be given back if the freed chunk is at the top of the data segment
allocated by the application. Otherwise, glibc would need to reshuffle things
around to place the unused space at the top. This reshuffling is costly, as is
the overhead to move the top of the data segment in either direction.

So the behavior of glibc is a tradeoff between execution speed and memory use:

- If glibc aggressively tries to release memory to the OS, it may be spending
  much time doing so, potentially for little benefit, since the application may
  want to immediately reallocate any released memory
- If glibc is very passive about releasing memory, then it will spend little
  time doing extra work, but the application's memory footprint will be larger
  than one would expect

There are functions in glibc to control this tradeoff:

- =mallopt= allows one to set some parameters to control the behavior, in
  particular =M_TRIM_THRESHOLD= is significant here
- =malloc_trim= can be invoked to release any memory /now/

So the memory is given back during a manual =malloc_trim= call, or in =free=,
when the parametrized logic says so.

To observe the effect of this logic, I wrote this =malloc_trim.sh= script:

#+begin_SRC bash
#!/bin/sh
set -e

PID=$1
test -n "$PID" || { echo "Need PID on the cmdline" > /dev/stderr; exit 1; }

before=`ps -h -p $PID -O rss  | awk '{print $2}'`
gdb --batch-silent --eval-command 'print malloc_trim(0)' -p $PID
after=`ps -h -p $PID -O rss  | awk '{print $2}'`

echo "before: $before"
echo "after: $after"
echo "freed: $(($before - $after))"
#+END_SRC

It takes in a PID on the commandline, and invokes =malloc_trim=. The results are
nothing short of miraculous (and very alarming). On my machine the largest
memory consumers are usually emacs, a web browser and X. An arbitrary invocation
of the script:

#+BEGIN_EXAMPLE
$ malloc_trim.sh `pidof emacs`
before: 1624156
after: 1101280
freed: 522876

$ malloc_trim.sh `pidof opera`
before: 491636
after: 327096
freed: 164540

$ malloc_trim.sh `pidof Xorg`
before: 101224
after: 53224
freed: 48000
#+END_EXAMPLE

Or put another way:

| process | Before trim (MB) | After trim (MB) | Freed (MB) |    % waste |
|---------+------------------+-----------------+------------+------------|
| emacs   |             1586 |            1075 |        511 |         32 |
| opera   |              480 |             319 |        161 |         34 |
| Xorg    |               99 |              52 |         47 |         47 |

Holy crap. It's clearly not just emacs. None of this is a smoking gun that
there's anything wrong, but it suggests strongly that the heuristics in =malloc=
either have a bug, or the parameters aren't set aggressively enough. I debugged
into it a bit, and the answer isn't obvious yet. I'll keep looking, however.

** DONE numpysane: a more reasonable numpy               :tools:python:numpy:
   CLOSED: [2016-05-17 Tue 00:16]

#+begin_o_blog_alert info Notice
Note that the documentation below is the original release, and is out-of-date.
See the [[https://github.com/dkogan/numpysane][repository]] for up-to-date docs.
#+end_o_blog_alert

As I mentioned a few times earlier, I'm trying to move from [[http://pdl.perl.org][PDL]] to [[http://www.numpy.org][numpy]] as my
computational tool of choice. I like PDL, but it was full of warts in several
areas, and much work would be needed to clean it up. Oddly, numpy is very warty
as well, but in completely different ways: their warts are complementary. The
major issues with numpy are strange and inconsistent core functions, and weak
and inconsistent [[http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html][broadcasting]] support. This is an area where PDL is quite good,
so to make this transition more palatable to me, I wrote =numpysane=, a python
module providing more reasonable core functionality. There will be many changes
down the line, but I just made the initial version 0.1 release. The code
repository lives [[http://github.com/dkogan/numpysane][here]], and the python module is available on pypi as well. The
full README from the initial release appears verbatim below.

*** NAME
  numpysane: more-reasonable core functionality for numpy

*** SYNOPSIS

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a   = np.arange(6).reshape(2,3)
  >>> b   = a + 100
  >>> row = a[0,:]

  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[100, 101, 102],
         [103, 104, 105]])

  >>> row
  array([1000, 1001, 1002])

  >>> nps.glue(a,b, axis=-1)
  array([[  0,   1,   2, 100, 101, 102],
         [  3,   4,   5, 103, 104, 105]])

  >>> nps.glue(a,b,row, axis=-2)
  array([[   0,    1,    2],
         [   3,    4,    5],
         [ 100,  101,  102],
         [ 103,  104,  105],
         [1000, 1001, 1002]])

  >>> nps.cat(a,b)
  array([[[  0,   1,   2],
          [  3,   4,   5]],

         [[100, 101, 102],
          [103, 104, 105]]])

  >>> @nps.broadcast_define( ('n',), ('n',) )
  ... def inner_product(a, b):
  ...     return a.dot(b)

  >>> inner_product(a,b)
  array([ 305, 1250])
  #+END_EXAMPLE

*** DESCRIPTION
  Numpy is widely used, relatively polished, and has a wide range of libraries
  available. At the same time, some of its very core functionality is strange,
  confusing and just plain wrong. This is in contrast with PDL
  (http://pdl.perl.org), which has a much more reasonable core, but a number of
  higher-level warts, and a relative dearth of library support. This module
  intends to improve the developer experience by providing alternate APIs to some
  core numpy functionality that is much more reasonable, especially for those who
  have used PDL in the past.

  Instead of writing a new module (this module), it would be really nice to simply
  patch numpy to give everybody the more reasonable behavior. I'd be very happy to
  do that, but the issues lie with some very core functionality, and any changes
  in behavior would likely break existing code. Any comments in how to achieve
  better behaviors in a less forky manner as welcome.

  Finally, if the system DOES make sense in some way that I'm simply not
  understanding, I'm happy to listen. I have no intention to disparage anyone or
  anything; I just want a more usable system for numerical computations.

  The issues addressed by this module fall into two broad categories:

  1. Incomplete broadcasting support
  2. Strange, special-case-ridden rules for basic array manipulation, especially
     dealing with dimensionality

**** Broadcasting
***** Problem
  Numpy has a limited support for broadcasting
  (http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html), a generic way
  to vectorize functions. When making a broadcasted call to a function, you pass
  in arguments with the inputs to vectorize available in new dimensions, and the
  broadcasting mechanism automatically calls the function multiple times as
  needed, and reports the output as an array collecting all the results.

  A basic example is an inner product: a function that takes in two
  identically-sized vectors (1-dimensional arrays) and returns a scalar
  (0-dimensional array). A broadcasted inner product function could take in two
  arrays of shape (2,3,4), compute the 6 inner products of length-4 each, and
  report the output in an array of shape (2,3). Numpy puts the most-significant
  dimension at the end, which is why this isn't 12 inner products of length-2
  each. This is a semi-arbitrary design choice, which could have been made
  differently: PDL puts the most-significant dimension at the front, for instance.

  The user doesn't choose whether to use broadcasting or not: some functions
  support it, and some do not. In PDL, broadcasting (called "threading" in that
  system) is a pervasive concept throughout. A PDL user has an expectation that
  every function can broadcast, and the documentation for every function is very
  explicit about the dimensionality of the inputs and outputs. Any data above the
  expected input dimensions is broadcast.

  By contrast, in numpy very few functions know how to broadcast. On top of that,
  the documentation is usually silent about the broadcasting status of a function
  in question. And on top of THAT, broadcasting rules state that an array of
  dimensions (n,m) is functionally identical to one of dimensions
  (1,1,1,....1,n,m). However, many numpy functions have special-case rules to
  create different behaviors for inputs with different numbers of dimensions, and
  this creates unexpected results. The effect of all this is a messy situation
  where the user is often not sure of the exact behavior of the functions they're
  calling, and trial and error is required to make the system do what one wants.

***** Solution
  This module contains functionality to make any arbitrary function broadcastable.
  This is invoked as a decorator, applied to the arbitrary user function. An
  example:

  #+BEGIN_EXAMPLE
  >>> import numpysane as nps

  >>> @nps.broadcast_define( ('n',), ('n',) )
  ... def inner_product(a, b):
  ...     return a.dot(b)
  #+END_EXAMPLE

  Here we have a simple inner product function to compute ONE inner product. We
  call 'broadcast_define' to add a broadcasting-aware wrapper that takes two 1D
  vectors of length 'n' each (same 'n' for the two inputs). This new
  'inner_product' function applies broadcasting, as needed:

  #+BEGIN_EXAMPLE
  >>> import numpy as np

  >>> a = np.arange(6).reshape(2,3)
  >>> b = a + 100

  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[100, 101, 102],
         [103, 104, 105]])

  >>> inner_product(a,b)
  array([ 305, 1250])
  #+END_EXAMPLE

  A detailed description of broadcasting rules is available in the numpy
  documentation: http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html

  In short:

  - The most significant dimension in a numpy array is the LAST one, so the
    prototype of an input argument must exactly match a given input's trailing
    shape. So a prototype shape of (a,b,c) accepts an argument shape of (......,
    a,b,c), with as many or as few leading dimensions as desired.
  - The extra leading dimensions must be compatible across all the inputs. This
    means that each leading dimension must either
    - equal to 1
    - be missing (thus assumed to equal 1)
    - equal to some positive integer >1, consistent across all arguments
  - The output is collected into an array that's sized as a superset of the
    above-prototype shape of each argument

  More involved example: A function with input prototype ( (3,), ('n',3), ('n',),
  ('m',) ) given inputs of shape

  #+BEGIN_SRC python
  (1,5,    3)
  (2,1,  8,3)
  (        8)
  (  5,    9)
  #+END_SRC

  will return an output array of shape (2,5, ...), where ... is the shape of each
  output slice. Note again that the prototype dictates the TRAILING shape of the
  inputs.

  Stock numpy has some rudimentary support for this with its vectorize() function,
  but it assumes only scalar inputs and outputs, which severaly limits its
  usefulness.

***** New planned functionality

  In addition to this basic broadcasting support, I'm planning the following:

  - Output memory should be used more efficiently. This means that the output
    array should be allocated once, and each slice output should be written
    directly into the correct place in the array. To make this possible, the
    output dimensions need to be a part of the prototype, and the output array
    should be passable to the function being wrapped.

  - A C-level broadcast_define(). This would be the analogue of PDL::PP
    (http://pdl.perl.org/PDLdocs/PP.html). This flavor of broadcast_define() would
    be invoked by the build system to wrap C functions. It would implement
    broadcasting awareness in C code it generates, which should work more
    effectively for performance-sensitive inner loops.

  - Automatic parallelization for broadcasted slices. Since each broadcasting loop
    is independent, this is a very natural place to add parallelism.

  - Dimensions should support a symbolic declaration. For instance, one could want
    a function to accept an input of shape (n) and another of shape (n*n). There's
    no way to declare this currently, but there should be.

**** Strangeness in core routines
***** Problem
  There are some core numpy functions whose behavior is strange, full of special
  cases and very confusing, at least to me. That makes it difficult to achieve
  some very basic things. In the following examples, I use a function "arr" that
  returns a numpy array with given dimensions:

  #+BEGIN_EXAMPLE
  >>> def arr(*shape):
  ...     product = reduce( lambda x,y: x*y, shape)
  ...     return np.arange(product).reshape(*shape)

  >>> arr(1,2,3)
  array([[[0, 1, 2],
          [3, 4, 5]]])

  >>> arr(1,2,3).shape
  (1, 2, 3)
  #+END_EXAMPLE

  The following sections are an incomplete list of the strange functionality I've
  encountered.

****** Concatenation
  A prime example of confusing functionality is the array concatenation routines.
  Numpy has a number of functions to do this, each being strange.

******* hstack()
  hstack() performs a "horizontal" concatenation. When numpy prints an array, this
  is the last dimension (remember, the most significant dimensions in numpy are at
  the end). So one would expect that this function concatenates arrays along this
  last dimension. In the special case of 1D and 2D arrays, one would be right:

  #+BEGIN_EXAMPLE
  >>> np.hstack( (arr(3), arr(3))).shape
  (6,)

  >>> np.hstack( (arr(2,3), arr(2,3))).shape
  (2, 6)
  #+END_EXAMPLE

  but in any other case, one would be wrong:

  #+BEGIN_EXAMPLE
  >>> np.hstack( (arr(1,2,3), arr(1,2,3))).shape
  (1, 4, 3)     <------ I expect (1, 2, 6)

  >>> np.hstack( (arr(1,2,3), arr(1,2,4))).shape
  [exception]   <------ I expect (1, 2, 7)

  >>> np.hstack( (arr(3), arr(1,3))).shape
  [exception]   <------ I expect (1, 6)

  >>> np.hstack( (arr(1,3), arr(3))).shape
  [exception]   <------ I expect (1, 6)
  #+END_EXAMPLE

  I think the above should all succeed, and should produce the shapes as
  indicated. Cases such as "np.hstack( (arr(3), arr(1,3)))" are maybe up for
  debate, but broadcasting rules allow adding as many extra length-1 dimensions as
  we want without changing the meaning of the object, so I claim this should work.
  Either way, if you print out the operands for any of the above, you too would
  expect a "horizontal" stack() to work as stated above.

  It turns out that normally hstack() concatenates along axis=1, unless the first
  argument only has one dimension, in which case axis=0 is used. This is 100%
  wrong in a system where the most significant dimension is the last one, unless
  you assume that everyone has only 2D arrays, where the last dimension and the
  second dimension are the same.

  The correct way to do this is to concatenate along axis=-1. It works for
  n-dimensionsal objects, and doesn't require the special case logic for
  1-dimensional objects that hstack() has.

******* vstack()
  Similarly, vstack() performs a "vertical" concatenation. When numpy prints an
  array, this is the second-to-last dimension (remember, the most significant
  dimensions in numpy are at the end). So one would expect that this function
  concatenates arrays along this second-to-last dimension. In the special
  case of 1D and 2D arrays, one would be right:

  #+BEGIN_EXAMPLE
  >>> np.vstack( (arr(2,3), arr(2,3))).shape
  (4, 3)

  >>> np.vstack( (arr(3), arr(3))).shape
  (2, 3)

  >>> np.vstack( (arr(1,3), arr(3))).shape
  (2, 3)

  >>> np.vstack( (arr(3), arr(1,3))).shape
  (2, 3)

  >>> np.vstack( (arr(2,3), arr(3))).shape
  (3, 3)
  #+END_EXAMPLE

  Note that this function appears to tolerate some amount of shape mismatches. It
  does it in a form one would expect, but given the state of the rest of this
  system, I found it surprising. For instance "np.hstack( (arr(1,3), arr(3)))"
  fails, so one would think that "np.vstack( (arr(1,3), arr(3)))" would fail too.

  And once again, adding more dimensions make it confused, for the same reason:

  #+BEGIN_EXAMPLE
  >>> np.vstack( (arr(1,2,3), arr(2,3))).shape
  [exception]   <------ I expect (1, 4, 3)

  >>> np.vstack( (arr(1,2,3), arr(1,2,3))).shape
  (2, 2, 3)     <------ I expect (1, 4, 3)
  #+END_EXAMPLE

  Similarly to hstack(), vstack() concatenates along axis=0, which is "vertical"
  only for 2D arrays, but not for any others. And similarly to hstack(), the 1D
  case has special-cased logic to work properly.

  The correct way to do this is to concatenate along axis=-2. It works for
  n-dimensionsal objects, and doesn't require the special case for 1-dimensional
  objects that vstack() has.

******* dstack()
  I'll skip the detailed description, since this is similar to hstack() and
  vstack(). The intent was to concatenate across axis=-3, but the implementation
  takes axis=2 instead. This is wrong, as before. And I find it strange that these
  3 functions even exist, since they are all special-cases: the concatenation axis
  should be an argument, and at most, the edge special case (hstack()) should
  exist. This brings us to the next function:

******* concatenate()
  This is a more general function, and unlike hstack(), vstack() and dstack(), it
  takes as input a list of arrays AND the concatenation dimension. It accepts
  negative concatenation dimensions to allow us to count from the end, so things
  should work better. And in many ways that failed previously, they do:

  #+BEGIN_EXAMPLE
  >>> np.concatenate( (arr(1,2,3), arr(1,2,3)), axis=-1).shape
  (1, 2, 6)

  >>> np.concatenate( (arr(1,2,3), arr(1,2,4)), axis=-1).shape
  (1, 2, 7)

  >>> np.concatenate( (arr(1,2,3), arr(1,2,3)), axis=-2).shape
  (1, 4, 3)
  #+END_EXAMPLE

  But many things still don't work as I would expect:

  #+BEGIN_EXAMPLE
  >>> np.concatenate( (arr(1,3), arr(3)), axis=-1).shape
  [exception]   <------ I expect (1, 6)

  >>> np.concatenate( (arr(3), arr(1,3)), axis=-1).shape
  [exception]   <------ I expect (1, 6)

  >>> np.concatenate( (arr(1,3), arr(3)), axis=-2).shape
  [exception]   <------ I expect (3, 3)

  >>> np.concatenate( (arr(3), arr(1,3)), axis=-2).shape
  [exception]   <------ I expect (2, 3)

  >>> np.concatenate( (arr(2,3), arr(2,3)), axis=-3).shape
  [exception]   <------ I expect (2, 2, 3)
  #+END_EXAMPLE

  This function works as expected only if

  - All inputs have the same number of dimensions
  - All inputs have a matching shape, except for the dimension along which we're
    concatenating
  - All inputs HAVE the dimension along which we're concatenating

  A legitimate use case that violates these conditions: I have an object that
  contains N 3D vectors, and I want to add another 3D vector to it. This is
  essentially the first failing example above.

******* stack()
  The name makes it sound exactly like concatenate(), and it takes the same
  arguments, but it is very different. stack() requires that all inputs have
  EXACTLY the same shape. It then concatenates all the inputs along a new
  dimension, and places that dimension in the location given by the 'axis' input.
  If this is the exact type of concatenation you want, this function works fine.
  But it's one of many things a user may want to do.

****** inner() and dot()
  Another arbitrary example of a strange API is np.dot() and np.inner(). In a
  real-valued n-dimensional Euclidean space, a "dot product" is just another name
  for an "inner product". Numpy disagrees.

  It looks like np.dot() is matrix multiplication, with some wonky behaviors when
  given higher-dimension objects, and with some special-case behaviors for
  1-dimensional and 0-dimensional objects:

  #+BEGIN_EXAMPLE
  >>> np.dot( arr(4,5,2,3), arr(3,5)).shape
  (4, 5, 2, 5) <--- expected result for a broadcasted matrix multiplication

  >>> np.dot( arr(3,5), arr(4,5,2,3)).shape
  [exception] <--- np.dot() is not commutative.
                   Expected for matrix multiplication, but not for a dot
                   product

  >>> np.dot( arr(4,5,2,3), arr(1,3,5)).shape
  (4, 5, 2, 1, 5) <--- don't know where this came from at all

  >>> np.dot( arr(4,5,2,3), arr(3)).shape
  (4, 5, 2) <--- 1D special case. This is a dot product.

  >>> np.dot( arr(4,5,2,3), 3).shape
  (4, 5, 2, 3) <--- 0D special case. This is a scaling.
  #+END_EXAMPLE

  It looks like np.inner() is some sort of quasi-broadcastable inner product, also
  with some funny dimensioning rules. In many cases it looks like np.dot(a,b) is
  the same as np.inner(a, transpose(b)) where transpose() swaps the last two
  dimensions:


  #+BEGIN_EXAMPLE
  >>> np.inner( arr(4,5,2,3), arr(5,3)).shape
  (4, 5, 2, 5) <--- All the length-3 inner products collected into a shape
                    with not-quite-broadcasting rules

  >>> np.inner( arr(5,3), arr(4,5,2,3)).shape
  (5, 4, 5, 2) <--- np.inner() is not commutative. Unexpected
                    for an inner product

  >>> np.inner( arr(4,5,2,3), arr(1,5,3)).shape
  (4, 5, 2, 1, 5) <--- No idea

  >>> np.inner( arr(4,5,2,3), arr(3)).shape
  (4, 5, 2) <--- 1D special case. This is a dot product.

  >>> np.inner( arr(4,5,2,3), 3).shape
  (4, 5, 2, 3) <--- 0D special case. This is a scaling.
  #+END_EXAMPLE

****** atleast_xd()
  Numpy has 3 special-case functions atleast_1d(), atleast_2d() and atleast_3d().
  For 4d and higher, you need to do something else. As expected by now, these do
  surprising things:

  #+BEGIN_EXAMPLE
  >>> np.atleast_3d( arr(3)).shape
  (1, 3, 1)
  #+END_EXAMPLE

  I don't know when this is what I would want, so we move on.


***** Solution
  This module introduces new functions that can be used for this core
  functionality instead of the builtin numpy functions. These new functions work
  in ways that (I think) are more intuitive and more reasonable. They do not refer
  to anything being "horizontal" or "vertical", nor do they talk about "rows" or
  "columns"; these concepts simply don't apply in a generic N-dimensional system.
  These functions are very explicit about the dimensionality of the
  inputs/outputs, and fit well into a broadcasting-aware system. Furthermore, the
  names and semantics of these new functions come directly from PDL, which is more
  consistent in this area.

  Since these functions assume that broadcasting is an important concept in the
  system, the given axis indices should be counted from the most significant
  dimension: the last dimension in numpy. This means that where an axis index is
  specified, negative indices are encouraged. glue() forbids axis>=0 outright.


  Example for further justification:

  An array containing N 3D vectors would have shape (N,3). Another array
  containing a single 3D vector would have shape (3). Counting the dimensions from
  the end, each vector is indexed in dimension -1. However, counting from the
  front, the vector is indexed in dimension 0 or 1, depending on which of the two
  arrays we're looking at. If we want to add the single vector to the array
  containing the N vectors, and we mistakenly try to concatenate along the first
  dimension, it would fail if N != 3. But if we're unlucky, and N=3, then we'd get
  a nonsensical output array of shape (3,4). Why would an array of N 3D vectors
  have shape (N,3) and not (3,N)? Because if we apply python iteration to it, we'd
  expect to get N iterates of arrays with shape (3,) each, and numpy iterates from
  the first dimension:

  #+BEGIN_EXAMPLE
  >>> a = np.arange(2*3).reshape(2,3)

  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> [x for x in a]
  [array([0, 1, 2]), array([3, 4, 5])]
  #+END_EXAMPLE

  New functions this module provides (documented fully in the next section):

****** glue
  Concatenates arrays along a given axis. Implicit length-1 dimensions are added
  at the start as needed. Dimensions other than the glueing axis must match
  exactly.

****** cat
  Concatenate a given list of arrays along a new least-significant (leading) axis.
  Again, implicit length-1 dimensions are added, and the resulting shapes must
  match, and no data duplication occurs.

****** clump
  Reshapes the array by grouping together the 'n' most significant dimensions,
  where 'n' is given. So for instance, if x.shape is (2,3,4) then
  nps.clump(x,2).shape is (2,12)

****** atleast_dims
  Adds length-1 dimensions at the front of an array so that all the given
  dimensions are in-bounds. Given axis<0 can expand the shape; given axis>=0 MUST
  already be in-bounds. This preserves the alignment of the most-significant axis
  index.

****** mv
  Moves a dimension from one position to another

****** xchg
  Exchanges the positions of two dimensions

****** transpose
  Reverses the order of the two most significant dimensions in an array. The whole
  array is seen as being an array of 2D matrices, each matrix living in the 2 most
  significant dimensions, which implies this definition.

****** dummy
  Adds a single length-1 dimension at the given position

****** reorder
  Completely reorders the dimensions in an array

****** dot
  Broadcast-aware non-conjugating dot product. Identical to inner

****** vdot
  Broadcast-aware conjugating dot product

****** inner
  Broadcast-aware inner product. Identical to dot

****** outer
  Broadcast-aware outer product.

****** matmult
  Broadcast-aware matrix multiplication

***** New planned functionality
  The function listed above are a start, but more will be added with time.

*** INTERFACE
**** broadcast_define()
  Vectorizes an arbitrary function, expecting input as in the given prototype.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> @nps.broadcast_define( ('n',), ('n',) )
  ... def inner_product(a, b):
  ...     return a.dot(b)

  >>> a = np.arange(6).reshape(2,3)
  >>> b = a + 100

  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[100, 101, 102],
         [103, 104, 105]])

  >>> inner_product(a,b)
  array([ 305, 1250])
  #+END_EXAMPLE


  The prototype defines the dimensionality of the inputs. In the inner product
  example above, the input is two 1D n-dimensional vectors. In particular, the
  'n' is the same for the two inputs. This function is intended to be used as
  a decorator, applied to a function defining the operation to be vectorized.
  Each element in the prototype list refers to each input, in order. In turn,
  each such element is a list that describes the shape of that input. Each of
  these shape descriptors can be any of

  - a positive integer, indicating an input dimension of exactly that length
  - a string, indicating an arbitrary, but internally consistent dimension

  The normal numpy broadcasting rules (as described elsewhere) apply. In
  summary:

  - Dimensions are aligned at the end of the shape list, and must match the
    prototype

  - Extra dimensions left over at the front must be consistent for all the
    input arguments, meaning:

    - All dimensions !=1 must be identical
    - Missing dimensions are implicitly set to 1
    - Dimensions that are =1 are set to the lengths implied by other arguments
    - The output has a shape where
      - The trailing dimensions are whatever the function being broadcasted
        outputs
      - The leading dimensions come from the extra dimensions in the inputs

  Let's look at a more involved example. Let's say we have a function that
  takes a set of points in R^2 and a single center point in R^2, and finds a
  best-fit least-squares line that passes through the given center point. Let
  it return a 3D vector containing the slope, y-intercept and the RMS residual
  of the fit. This broadcasting-enabled function can be defined like this:

  #+BEGIN_SRC python
  import numpy as np
  import numpysane as nps

  @nps.broadcast_define( ('n',2), (2,) )
  def fit(xy, c):
      # line-through-origin-model: y = m*x
      # E = sum( (m*x - y)**2 )
      # dE/dm = 2*sum( (m*x-y)*x ) = 0
      # ----> m = sum(x*y)/sum(x*x)
      x,y = (xy - c).transpose()
      m = np.sum(x*y) / np.sum(x*x)
      err = m*x - y
      err **= 2
      rms = np.sqrt(err.mean())
      # I return m,b because I need to translate the line back
      b = c[1] - m*c[0]

      return np.array((m,b,rms))
  #+END_SRC

  And I can use broadcasting to compute a number of these fits at once. Let's
  say I want to compute 4 different fits of 5 points each. I can do this:

  #+BEGIN_SRC python
  n = 5
  m = 4
  c = np.array((20,300))
  xy = np.arange(m*n*2, dtype=np.float64).reshape(m,n,2) + c
  xy += np.random.rand(*xy.shape)*5

  res = fit( xy, c )
  mb  = res[..., 0:2]
  rms = res[..., 2]
  print "RMS residuals: {}".format(rms)
  #+END_SRC

  Here I had 4 different sets of points, but a single center point c. If I
  wanted 4 different center points, I could pass c as an array of shape (4,2).
  I can use broadcasting to plot all the results (the points and the fitted
  lines):

  #+BEGIN_SRC python
  import gnuplotlib as gp

  gp.plot( *nps.mv(xy,-1,0), _with='linespoints',
           equation=['{}*x + {}'.format(mb_single[0],
                                        mb_single[1]) for mb_single in mb],
           unset='grid', square=1)
  #+END_SRC

  This function is analogous to thread_define() in PDL.

**** glue()
  Concatenates a given list of arrays along the given 'axis' keyword argument.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(6).reshape(2,3)
  >>> b = a + 100
  >>> row = a[0,:] + 1000

  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[100, 101, 102],
         [103, 104, 105]])

  >>> row
  array([1000, 1001, 1002])

  >>> nps.glue(a,b, axis=-1)
  array([[  0,   1,   2, 100, 101, 102],
         [  3,   4,   5, 103, 104, 105]])

  >>> nps.glue(a,b,row, axis=-2)
  array([[   0,    1,    2],
         [   3,    4,    5],
         [ 100,  101,  102],
         [ 103,  104,  105],
         [1000, 1001, 1002]])

  >>> nps.glue(a,b, axis=-3)
  array([[[  0,   1,   2],
          [  3,   4,   5]],

         [[100, 101, 102],
          [103, 104, 105]]])
  #+END_EXAMPLE

  If no 'axis' keyword argument is given, a new dimension is added at the
  front, and we concatenate along that new dimension. This case is equivalent
  to numpysane.cat()

  In order to count dimensions from the inner-most outwards, this function accepts
  only negative axis arguments. This is because numpy broadcasts from the last
  dimension, and the last dimension is the inner-most in the (usual) internal
  storage scheme. Allowing glue() to look at dimensions at the start would allow
  it to unalign the broadcasting dimensions, which is never what you want.

  To glue along the last dimension, pass axis=-1; to glue along the second-to-last
  dimension, pass axis=-2, and so on.

  Unlike in PDL, this function refuses to create duplicated data to make the
  shapes fit. In my experience, this isn't what you want, and can create bugs.
  For instance, PDL does this:

  #+BEGIN_SRC python
  pdl> p sequence(3,2)
  [
   [0 1 2]
   [3 4 5]
  ]

  pdl> p sequence(3)
  [0 1 2]

  pdl> p PDL::glue( 0, sequence(3,2), sequence(3) )
  [
   [0 1 2 0 1 2]   <--- Note the duplicated "0,1,2"
   [3 4 5 0 1 2]
  ]
  #+END_SRC

  while numpysane.glue() does this:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(6).reshape(2,3)
  >>> b = a[0:1,:]


  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[0, 1, 2]])

  >>> nps.glue(a,b,axis=-1)
  [exception]
  #+END_EXAMPLE

  Finally, this function adds as many length-1 dimensions at the front as
  required. Note that this does not create new data, just new degenerate
  dimensions. Example:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(6).reshape(2,3)
  >>> b = a + 100

  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[100, 101, 102],
         [103, 104, 105]])

  >>> res = nps.glue(a,b, axis=-5)
  >>> res
  array([[[[[  0,   1,   2],
            [  3,   4,   5]]]],



         [[[[100, 101, 102],
            [103, 104, 105]]]]])

  >>> res.shape
  (2, 1, 1, 2, 3)
  #+END_EXAMPLE

**** cat()
  Concatenates a given list of arrays along a new first (outer) dimension.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(6).reshape(2,3)
  >>> b = a + 100
  >>> c = a - 100

  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[100, 101, 102],
         [103, 104, 105]])

  >>> c
  array([[-100,  -99,  -98],
         [ -97,  -96,  -95]])

  >>> res = nps.cat(a,b,c)
  >>> res
  array([[[   0,    1,    2],
          [   3,    4,    5]],

         [[ 100,  101,  102],
          [ 103,  104,  105]],

         [[-100,  -99,  -98],
          [ -97,  -96,  -95]]])

  >>> res.shape
  (3, 2, 3)

  >>> [x for x in res]
  [array([[0, 1, 2],
          [3, 4, 5]]),
   array([[100, 101, 102],
          [103, 104, 105]]),
   array([[-100,  -99,  -98],
          [ -97,  -96,  -95]])]
  #+END_EXAMPLE

  This function concatenates the input arrays into an array shaped like the
  highest-dimensioned input, but with a new outer (at the start) dimension.
  The concatenation axis is this new dimension.

  As usual, the dimensions are aligned along the last one, so broadcasting
  will continue to work as expected. Note that this is the opposite operation
  from iterating a numpy array; see the example above.

**** clump()
  Groups the given n most significant dimensions together.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpysane as nps
  >>> nps.clump( arr(2,3,4), n=2).shape
  (2, 12)
  #+END_EXAMPLE

**** atleast_dims()
  Returns an array with extra length-1 dimensions to contain all given axes.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(6).reshape(2,3)
  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> nps.atleast_dims(a, -1).shape
  (2, 3)

  >>> nps.atleast_dims(a, -2).shape
  (2, 3)

  >>> nps.atleast_dims(a, -3).shape
  (1, 2, 3)

  >>> nps.atleast_dims(a, 0).shape
  (2, 3)

  >>> nps.atleast_dims(a, 1).shape
  (2, 3)

  >>> nps.atleast_dims(a, 2).shape
  [exception]

  >>> l = [-3,-2,-1,0,1]
  >>> nps.atleast_dims(a, l).shape
  (1, 2, 3)

  >>> l
  [-3, -2, -1, 1, 2]
  #+END_EXAMPLE

  If the given axes already exist in the given array, the given array itself
  is returned. Otherwise length-1 dimensions are added to the front until all
  the requested dimensions exist. The given axis>=0 dimensions MUST all be
  in-bounds from the start, otherwise the most-significant axis becomes
  unaligned; an exception is thrown if this is violated. The given axis<0
  dimensions that are out-of-bounds result in new dimensions added at the
  front.

  If new dimensions need to be added at the front, then any axis>=0 indices
  become offset. For instance:

  #+BEGIN_EXAMPLE
  >>> x.shape
  (2, 3, 4)

  >>> [x.shape[i] for i in (0,-1)]
  [2, 4]

  >>> x = nps.atleast_dims(x, 0, -1, -5)
  >>> x.shape
  (1, 1, 2, 3, 4)

  >>> [x.shape[i] for i in (0,-1)]
  [1, 4]
  #+END_EXAMPLE

  Before the call, axis=0 refers to the length-2 dimension and axis=-1 refers
  to the length=4 dimension. After the call, axis=-1 refers to the same
  dimension as before, but axis=0 now refers to a new length=1 dimension. If
  it is desired to compensate for this offset, then instead of passing the
  axes as separate arguments, pass in a single list of the axes indices. This
  list will be modified to offset the axis>=0 appropriately. Ideally, you only
  pass in axes<0, and this does not apply. Doing this in the above example:

  #+BEGIN_EXAMPLE
  >>> l
  [0, -1, -5]

  >>> x.shape
  (2, 3, 4)

  >>> [x.shape[i] for i in (l[0],l[1])]
  [2, 4]

  >>> x=nps.atleast_dims(x, l)
  >>> x.shape
  (1, 1, 2, 3, 4)

  >>> l
  [2, -1, -5]

  >>> [x.shape[i] for i in (l[0],l[1])]
  [2, 4]
  #+END_EXAMPLE

  We passed the axis indices in a list, and this list was modified to reflect
  the new indices: The original axis=0 becomes known as axis=2. Again, if you
  pass in only axis<0, then you don't need to care about this.

**** mv()
  Moves a given axis to a new position. Similar to numpy.moveaxis().

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(24).reshape(2,3,4)
  >>> a.shape
  (2, 3, 4)

  >>> nps.mv( a, -1, 0).shape
  (4, 2, 3)

  >>> nps.mv( a, -1, -5).shape
  (4, 1, 1, 2, 3)

  >>> nps.mv( a, 0, -5).shape
  (2, 1, 1, 3, 4)
  #+END_EXAMPLE

  New length-1 dimensions are added at the front, as required, and any axis>=0
  that are passed in refer to the array BEFORE these new dimensions are added.

**** xchg()
  Exchanges the positions of the two given axes. Similar to numpy.swapaxes()

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(24).reshape(2,3,4)
  >>> a.shape
  (2, 3, 4)

  >>> nps.xchg( a, -1, 0).shape
  (4, 3, 2)

  >>> nps.xchg( a, -1, -5).shape
  (4, 1, 2, 3, 1)

  >>> nps.xchg( a, 0, -5).shape
  (2, 1, 1, 3, 4)
  #+END_EXAMPLE

  New length-1 dimensions are added at the front, as required, and any axis>=0
  that are passed in refer to the array BEFORE these new dimensions are added.

**** transpose()
  Reverses the order of the last two dimensions.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(24).reshape(2,3,4)
  >>> a.shape
  (2, 3, 4)

  >>> nps.transpose(a).shape
  (2, 4, 3)

  >>> nps.transpose( np.arange(3) ).shape
  (3, 1)
  #+END_EXAMPLE

  A "matrix" is generally seen as a 2D array that we can transpose by looking
  at the 2 dimensions in the opposite order. Here we treat an n-dimensional
  array as an n-2 dimensional object containing 2D matrices. As usual, the
  last two dimensions contain the matrix.

  New length-1 dimensions are added at the front, as required, meaning that 1D
  input of shape (n,) is interpreted as a 2D input of shape (1,n), and the
  transpose is 2 of shape (n,1).


**** dummy()
  Adds a single length-1 dimension at the given position.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(24).reshape(2,3,4)
  >>> a.shape
  (2, 3, 4)

  >>> nps.dummy(a, 0).shape
  (1, 2, 3, 4)

  >>> nps.dummy(a, 1).shape
  (2, 1, 3, 4)

  >>> nps.dummy(a, -1).shape
  (2, 3, 4, 1)

  >>> nps.dummy(a, -2).shape
  (2, 3, 1, 4)

  >>> nps.dummy(a, -5).shape
  (1, 1, 2, 3, 4)
  #+END_EXAMPLE

  This is similar to numpy.expand_dims(), but handles out-of-bounds dimensions
  better. New length-1 dimensions are added at the front, as required, and any
  axis>=0 that are passed in refer to the array BEFORE these new dimensions
  are added.

**** reorder()
  Reorders the dimensions of an array.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(24).reshape(2,3,4)
  >>> a.shape
  (2, 3, 4)

  >>> nps.reorder( a, 0, -1, 1 ).shape
  (2, 4, 3)

  >>> nps.reorder( a, -2 , -1, 0 ).shape
  (3, 4, 2)

  >>> nps.reorder( a, -4 , -2, -5, -1, 0 ).shape
  (1, 3, 1, 4, 2)
  #+END_EXAMPLE

  This is very similar to numpy.transpose(), but handles out-of-bounds
  dimensions much better.

  New length-1 dimensions are added at the front, as required, and any axis>=0
  that are passed in refer to the array BEFORE these new dimensions are added.

**** dot()
  Non-conjugating dot product of two 1-dimensional n-long vectors.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(3)
  >>> b = a+5
  >>> a
  array([0, 1, 2])

  >>> b
  array([5, 6, 7])

  >>> nps.dot(a,b)
  array(20)
  #+END_EXAMPLE

  This is identical to numpysane.inner(). For a conjugating version of this
  function, use nps.vdot().

**** vdot()
  Conjugating dot product of two 1-dimensional n-long vectors.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.array(( 1 + 2j, 3 + 4j, 5 + 6j))
  >>> b = a+5
  >>> a
  array([ 1.+2.j,  3.+4.j,  5.+6.j])

  >>> b
  array([  6.+2.j,   8.+4.j,  10.+6.j])

  >>> nps.vdot(a,b)
  array((136-60j))

  >>> nps.dot(a,b)
  array((24+148j))
  #+END_EXAMPLE

  This is identical to numpysane.inner(). For a conjugating version of this
  function, use nps.vdot().

**** outer()
  Outer product of two 1-dimensional n-long vectors.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(3)
  >>> b = a+5
  >>> a
  array([0, 1, 2])

  >>> b
  array([5, 6, 7])

  >>> nps.outer(a,b)
  array([[ 0,  0,  0],
         [ 5,  6,  7],
         [10, 12, 14]])
  #+END_EXAMPLE

**** matmult()
  Multiplication of two matrices.

  Synopsis:

  #+BEGIN_EXAMPLE
  >>> import numpy as np
  >>> import numpysane as nps

  >>> a = np.arange(6).reshape(2,3)
  >>> b = np.arange(12).reshape(3,4)
  >>> a
  array([[0, 1, 2],
         [3, 4, 5]])

  >>> b
  array([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]])

  >>> nps.matmult(a,b)
  array([[20, 23, 26, 29],
         [56, 68, 80, 92]])
  #+END_EXAMPLE

*** COMPATIBILITY

  Python2 and python3 should are both supported. Please report a bug if either one
  doesn't work.

*** REPOSITORY

  https://github.com/dkogan/numpysane

*** AUTHOR

  Dima Kogan <dima@secretsauce.net>

*** LICENSE AND COPYRIGHT

  Copyright 2016 Dima Kogan.

  This program is free software; you can redistribute it and/or modify it under
  the terms of the GNU Lesser General Public License (version 3 or higher) as
  published by the Free Software Foundation

  See https://www.gnu.org/licenses/lgpl.html

** DONE Simple ASCII parsing: performance comparisons             :tools:dev:
   CLOSED: [2016-10-08 Sat 16:17]

*** Overview
I'm currently dealing with large data files composed of whitespace-separated
text. These are very well supported by various UNIX tools, which makes it an
attractive way to store data. Many processing tools are available, but I
discovered that the performance of these tools varies more widely than I had
assumed. None of this is news to many people, but I thought it'd be useful to
run a few experiments to quantify the differences.

I ran a number of trials looking at a particular data file I had lying around.
This file weighs in at about 40MB. It has roughly 120000 lines with about 100
whitespace-separated records each (most records are just a single character). I
want to test the most basic possible parsing program:

1. read in the data one line at a time
2. split each line into fields, chopping off the trailing newline
3. print out all but the first field

I'm comparing perl, python, gawk, mawk and cut, all installed from packages on
my Debian/sid box running on an amd64 arch. The package versions:

| Package name          |          version |
|-----------------------+------------------|
| perl                  |         5.22.2-1 |
| python2.7             |         2.7.11-7 |
| python3.6             |       3.6.0~b1-1 |
| gawk                  | 1:4.1.3+dfsg-0.1 |
| mawk                  |         1.3.3-17 |
| coreutils (for =cut=) |           8.25-2 |

*** Programs under test

Everything is plain ASCII: I have =LANG=C= and =LC_ALL=C=.

**** Perl
The perl code looks like this:

#+BEGIN_SRC perl
use feature 'say';
while(<>)
{
    chop;
    @F = split;
    shift @F;
    $_ = join(' ',@F);
    say;
}
#+END_SRC

I also tried omitting the =while(<>) {... say}= with =perl -p= and also omitting
the =chop= and =split= with =perl -a=. This produced much shorter code, but had
no measurable effect on the performance.

**** Python
The python code looks like this:

#+BEGIN_SRC python
import sys

for l in sys.stdin:
    fields     = l[:-1].split()
    fields[:1] = []
    scut       = ' '.join(fields)
    print(scut)
#+END_SRC

The =()= in the =print= were included for python3 and omitted for python2.

#+begin_o_blog_alert info update
Clément Pit-Claudel noted that python3 does more manipulation of text coming in
from a buffer, and suggested the following flavor of the test, in order to
bypass some of this overhead:

#+BEGIN_SRC python
import sys

for l in sys.stdin.buffer:
    fields     = l[:-1].split()
    fields[:1] = []
    scut       = b' '.join(fields)
    sys.stdout.buffer.write(scut + b'\n')
#+END_SRC
#+end_o_blog_alert

**** awk
The awk program simply looks like this:

#+BEGIN_SRC awk
{ $1=""; print; }
#+END_SRC

This applies to both gawk and mawk. I ran this one as a one-liner, not even
bothering to put it into a source file.

**** cut
Finally, the =cut= invocation was an ultimate one-liner:

#+BEGIN_EXAMPLE
$ cut -d ' ' -f 2-
#+END_EXAMPLE

*** Test invocation

All the tests were executed a few times, with the mean wall-clock time being
taken. For instance:

#+BEGIN_EXAMPLE
$ for i in `seq 10`; do < /tmp/data time python2.7 tst.py > /dev/null; done |& awk '{n=NF-1; s+=$n} END{print s/NR}'
#+END_EXAMPLE

For each application I would take a measurement for the full program, and then
I'd cut off commands from the end to get a sense of where the time was going.

*** Results
The raw results look like this (all timings in seconds):

|                     | Everything | without print | without join | without shift | without chop, split |
|---------------------+------------+---------------+--------------+---------------+---------------------|
| perl                |       3.04 |          3.01 |         2.40 |          2.38 |                0.11 |
| python2.7           |       1.19 |          1.08 |         0.76 |          0.71 |                0.05 |
| python3.6           |       1.64 |          1.30 |         0.97 |          0.89 |                0.13 |
| python3.6 (buffers) |       1.43 |          1.21 |         0.77 |          0.71 |                0.08 |
| gawk                |       1.00 |          0.09 |         0.08 |          0.08 |                     |
| mawk                |       0.65 |          0.63 |         0.00 |          0.00 |                     |
| cut                 |       0.55 |               |              |               |                     |

Taking differences, we get this:

|                     | overhead | chop, split | shift | join | print |
|---------------------+----------+-------------+-------+------+-------|
| perl                |     0.11 |        2.27 |  0.02 | 0.61 |  0.03 |
| python2.7           |     0.05 |        0.66 |  0.05 | 0.32 |  0.11 |
| python3.6           |     0.13 |        0.76 |  0.08 | 0.33 |  0.34 |
| python3.6 (buffers) |     0.08 |        0.63 |  0.06 | 0.44 |  0.22 |
| gawk                |        0 |           0 |     0 | 0.01 |  0.91 |
| mawk                |        0 |           0 |     0 | 0.63 |  0.02 |
| cut                 |     0.55 |             |       |      |       |

As expected, the plain =cut= utility from GNU coreutils is fastest, with =mawk=
being slower, but not terribly so. In a distant 3rd place is =gawk=. These all
are what I would expect. The following were surprising, however. Python2.7 is
not too much slower than =gawk=. Python3.6 is /much/ slower than Python2.7,
although some of this inefficiency can be bypassed by using special i/o objects.
And perl is /way/ slower than all the others. It's very possible that perl and
python3 are doing something overly-complicated that can be turned off with some
setting, but I don't at this time know what that is.

Looking at the components, it looks like perl has trouble dealing with lists:
the splitting and joining are dramatically slower for it than for other tools.

So that's it. I guess the only grand conclusion for me is to be wary of perl for
dealing with large data files. If anybody knows how to speed things up in any of
these cases, email me.

** DONE Using DWARF to find and save all state in a C library     :tools:dev:
   CLOSED: [2016-11-13 Sun 23:58]

#+begin_o_blog_alert info Update
Note that this technique finds /each individual stateful variable/ in a library.
Thinking about this more, a simpler technique would be to simply copy the whole
=.bss= and =.data= sections. This is much simpler, requires many fewer copies,
and does /not/ need DWARF. You end up copying more data since you copy the
padding bytes also, but this is likely still faster, since you only make 2 large
transfers. Another potential advantage of doing it this way, is that there might
be some data somewhere that would not show up in the DWARF: some language
runtime metadata or god-knows-what internal data in something. In my one
non-toy-problem test, the DWARF identified about 1600 variables for a grand
total of about 27MB. In any case, the DWARF logic still has its place, in case
we need finer-grained control than just dumping all the memory in a DSO.

See =get_writeable_memory_ranges()= in the [[https://github.com/dkogan/FindGlobals][sources]] for an example implementation
of getting the relevant memory ranges..

Also note, that I found a bug related to the size reporting of multi-dimensional
array: [[https://sourceware.org/bugzilla/show_bug.cgi?id=22546]]
#+end_o_blog_alert

Recently I needed to integrate a particular C library into a larger project I'm
working on. This larger project has an "undo" feature, which requires that any
action of the library must be revertable. This library does no I/O, so an undo
sequence is

1. save the persistent state of the library (all variables in memory)
2. perform some library action
3. revert to the previous state by restoring the state saved in 1.

The difficulty of this depends on the implementation details of each specific
library. The easiest to handle are libraries that store all of their state in a
context structure that is owned by the caller and that don't perform any dynamic
memory allocation:

#+BEGIN_SRC C
// inside library
struct library_context_t { ... };
void library_do_thing (struct library_context_t* ctx);

// inside client that invokes the library
void call_library(struct library_context_t* ctx)
{
    struct library_context_t ctx_saved = *ctx;

    library_do_thing(ctx);

    if(need to revert)
        *ctx = ctx_saved;
}
#+END_SRC

The most effortful are libraries that have a lot of internal state they manage
themselves and/or use dynamic allocation heavily.

My library didn't allocate anything dynamically, but it used internal state
/heavily/. Things like this were strewn all over:

#+BEGIN_SRC C
// library.c

static int a;
int        b;
extern struct { int w,x,y,z; } s;
void library_do_thing(void)
{
    static int c = 5;
    static struct { double v[10]; } d;

    a = ....;
    b = ....;
}
#+END_SRC

So we need to deal with lots and lots of internal state. I had access to the
source, so the thing could be cleaned up to consolidate the hundreds of stateful
variables into a more manageable (and accessible, unlike the =static= locals for
instance) number of structures. But this would require deep surgery into the
sources that would /not/ be accepted upstream.

Fundamentally, the task is to save a set of areas in memory, each area having an
address and a size. This is mostly known at compile time, and some
instrumentation can be added in a =save()= function in each source file in the
library:

#+BEGIN_SRC C
// library.c

static int a;
int        b;
extern struct { int w,x,y,z; } s;
void library_do_thing(void)
{
    static int c = 5;
    static struct { double v[10]; } d;
}

void save(int* a_saved, int* b_saved)
{
    *a_saved = a;
    *b_saved = b;

    // cannot access c and d
}
#+END_SRC

This lets me save the file-global state, but not the function-local state. And
I'd need to have one of these in /each/ source file I'm instrumenting. So this
doesn't fully work. I need to have something that can get the list of addresses
and sizes at runtime.

The addresses are available from the symbol table, which I can get from the ELF
file I'm running (or linking with). With =library.c= as above:

#+BEGIN_EXAMPLE
$ gcc -c -o library.o library.c

$ nm library.o | awk '$2 ~ /^[bcdgsBCDGS]$/'
0000000000000000 b a
0000000000000004 C b
0000000000000000 d c.1766
0000000000000020 b d.1769
#+END_EXAMPLE

Here I only look at /data/ and only data that is /writeable/ because the
read-only data doesn't need to be saved/restored. We can see the global
variables =a= and =b=, and the static function-local variables =c= and =d=. And
we can see their addresses. We cannot, however see the sizes: these are known
and used by the compiler, which discards them when finished. I need another
source of data, and I found the perfect one: DWARF debugging information. This
contains much meta-data about the compiled code, including the location and
sizes of all the variables. DWARF info isn't necessarily available at runtime,
but I control the build flags, so I can make sure I have this. Example:

#+BEGIN_EXAMPLE
$ gcc -c -g -o library.o library.c

$ readelf -wi library.o
...
 <1><3c>: Abbrev Number: 3 (DW_TAG_base_type)
    <3d>   DW_AT_byte_size   : 4
    <3e>   DW_AT_encoding    : 5        (signed)
    <3f>   DW_AT_name        : int
 <1><43>: Abbrev Number: 4 (DW_TAG_variable)
    <44>   DW_AT_name        : b
    <46>   DW_AT_decl_file   : 1
    <47>   DW_AT_decl_line   : 2
    <48>   DW_AT_type        : <0x3c>
    <4c>   DW_AT_external    : 1
    <4c>   DW_AT_location    : 9 byte block: 3 4 0 0 0 0 0 0 0  (DW_OP_addr: 4)
...
#+END_EXAMPLE

In this snippet we can see the variable =b= and we can see that it is a 4-byte
integer at some address within the file.

I wrote a program to automatically parse this information and to generate the
list of addresses and sizes that I care about. This is available at

https://github.com/dkogan/FindGlobals

The README is copied here:

*** Overview
This is a proof-of-concept program that is able to find all global state in a
program by parsing its debug information (which is assumed to be available).

The global state I'm looking for is all persistent data:

- globals variables (static or otherwise)
- static local variables in functions

I'm only interested in data that can change, so I look /only/ at the writeable
segment. This program is alpha-quality, but passes some basic tests.

*** Example
=tst1.c=:
#+BEGIN_SRC C
// ....
struct S1 v1[10];

volatile const char volatile_const_char;
const volatile char const_volatile_char;
const char          const_char = 1;

void print_tst1(void)
{
    showvar(v1);
    showvar(volatile_const_char);
    showvar(const_volatile_char);
    showvar_readonly(const_char);
}
#+END_SRC

=tst2.c=:
#+BEGIN_SRC C
// ....
int v2 = 5;

const char*       const_char_pointer       = NULL;
const char const* const_char_const_pointer = NULL;
char const*       char_const_pointer       = NULL;

const        char  const_string_array[]        = "456";
static const char  static_const_string_array[] = "abc";

struct S0 s0;
int       x0[0];
int       x1[30];



void print_tst2(void)
{
    static int    static_int_5[5];
    static double static_double;
    double dynamic_d;

    showvar(static_int_5);
    showvar(static_double);
    showvar(v2);
    showvar(const_char_pointer);
    showvar_readonly(const_char_const_pointer);
    showvar_readonly(char_const_pointer);
    showvar_readonly(const_string_array);
    showvar_readonly(static_const_string_array);
    showvar(s0);
    showvar(x0);
    showvar(x1);
}
#+END_SRC

=main.c=:
#+BEGIN_SRC C
// ...
#include "getglobals.h"

int main(int argc, char* argv[])
{
    printf("=================== Program sees: ===================\n");

    print_tst1();
    print_tst2();

    printf("=================== DWARF sees: ===================\n");

    get_addrs((void (*)())&print_tst1, "tst");

    return 0;
}
#+END_SRC

Results:

#+BEGIN_EXAMPLE
$ ./getglobals 2>/dev/null

=================== Program sees: ===================
v1 at 0x55756bc8e240, size 80
volatile_const_char at 0x55756bc8e220, size 1
const_volatile_char at 0x55756bc8e290, size 1
readonly const_char at 0x55756ba8cc65, size 1
static_int_5 at 0x55756bc8e040, size 20
static_double at 0x55756bc8e038, size 8
v2 at 0x55756bc8e010, size 4
const_char_pointer at 0x55756bc8e030, size 8
readonly const_char_pointer_const at 0x55756ba8cbd0, size 8
readonly char_pointer_const at 0x55756ba8cbc8, size 8
readonly const_string_array at 0x55756ba8cbc4, size 4
readonly static_const_string_array at 0x55756ba8cbc0, size 4
s0 at 0x55756bc8e120, size 100
x0 at 0x55756bc8e184, size 0
x1 at 0x55756bc8e1a0, size 120
=================== DWARF sees: ===================
v2 at 0x55756bc8e010, size 4
const_char_pointer at 0x55756bc8e030, size 8
readonly const_char_pointer_const at 0x55756ba8cbd0, size 8
readonly char_pointer_const at 0x55756ba8cbc8, size 8
readonly const_string_array at 0x55756ba8cbc4, size 4
readonly static_const_string_array at 0x55756ba8cbc0, size 4
s0 at 0x55756bc8e120, size 100
x1 at 0x55756bc8e1a0, size 120
static_int_5 at 0x55756bc8e040, size 20
static_double at 0x55756bc8e038, size 8
v1 at 0x55756bc8e240, size 80
volatile_const_char at 0x55756bc8e220, size 1
const_volatile_char at 0x55756bc8e290, size 1
readonly const_char at 0x55756ba8cc65, size 1


dima@shorty:$ ./getglobals 2>/dev/null | sort | uniq -c | sort

      1 =================== DWARF sees: ===================
      1 =================== Program sees: ===================
      1 x0 at 0x55be5ba3d184, size 0
      2 const_char_pointer at 0x55be5ba3d030, size 8
      2 const_volatile_char at 0x55be5ba3d290, size 1
      2 readonly char_pointer_const at 0x55be5b83bbc8, size 8
      2 readonly const_char at 0x55be5b83bc65, size 1
      2 readonly const_char_pointer_const at 0x55be5b83bbd0, size 8
      2 readonly const_string_array at 0x55be5b83bbc4, size 4
      2 readonly static_const_string_array at 0x55be5b83bbc0, size 4
      2 s0 at 0x55be5ba3d120, size 100
      2 static_double at 0x55be5ba3d038, size 8
      2 static_int_5 at 0x55be5ba3d040, size 20
      2 v1 at 0x55be5ba3d240, size 80
      2 v2 at 0x55be5ba3d010, size 4
      2 volatile_const_char at 0x55be5ba3d220, size 1
      2 x1 at 0x55be5ba3d1a0, size 120
#+END_EXAMPLE

In the example above we see the test program report the locations and addreses
of its data. Then the instrumentation reports the addresses and data that it
sees. These should be identical (in theory). We then sort and count duplicate
lines in the output. If the two sets of reports were identical, we should see
all lines appear 2 times. We see this with 2 trivial exceptions:

- The lines containing ======= are delimiters
- =x0= has size 0, so the instrumentation doesn't even bother to report it

*** Notes
- The example in this tree works with both static linking (=getglobals=) and
  dynamic linking (=getglobals_viaso=).

- This is alpha quality proof-of-concept software. It's an excellent starting
  point if you need this functionality, but do thoroughly test it for your use
  case.

- This was written with C code in mind. I can imagine that C++ can produce
  persistent state in more ways. Again, test it thoroughly

- The libraries I'm using to parse the DWARF are woefully underdocumented, and
  I'm probably not doing everything 100% correctly. At the risk of repeating
  myself: test it thoroughly.

- For ELF objects linked into the executable normally (whether statically or
  dynamically) this works. If we're doing something funny like loading
  libraries ourselves with =libdl=, then it /probably/ works too, but I'd test
  it before assuming.

This effort uncovered a bug in gcc:

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=78100

It turns out that gcc erroneously omits the sizing information if you have an
array object pre-declared as an =extern= of unknown size (as was done in my
specific library). So if you have this source:

#+BEGIN_SRC C
#include <stdio.h>

extern int s[];
int s[] = { 1,2,3 };

int main(void)
{
    printf("%zd\n", sizeof(s));
    return 0;
}
#+END_SRC

then the object produced by gcc <= 6.2 doesn't know that =s= has /3/ integers.

*** Copyright and License
Copyright 2016 Dima Kogan.

Released under an MIT-style license:

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
the Software, and to permit persons to whom the Software is furnished to do so.

** DONE Detecting missing DT_NEEDED tags                          :tools:dev:
   CLOSED: [2017-02-02 Thu 10:42]

I'm working on a project that contains many libraries (built as shared objects)
that use functionality from each other. In normal usage (in this project) these
are loaded dynamically by python. I have no idea how the python loader resolves
dependencies between these libraries, but if one tries to build an application
that uses the plain dynamic linker to link the libraries, things become
difficult. The reason: these libraries have no =DT_NEEDED= tags so the dynamic
linker has no dependency information.

*** Background

Suppose we have a library =libA.so= that uses functionality from =libB.so= to do
some of its work. Now suppose you have an application =exe= that calls functions
in =libA.so=, but /not/ =libB.so=. The person invoking the linker to link =exe=
knows to build with =-lA= because they know that =exe= uses =libA.so=. However
this person may not know to link with =-lB=: /their/ program that /they/ wrote
is =exe=, and they have no idea how the internals of =libA.so= work, nor should
they care. The solution is to put a =DT_NEEDED= tag into =libA.so= to indicate
that it needs =libB.so=. With this tag, the linker command to build =exe= only
needs to specify =-lA=, and the linker will read the =DT_NEEDED= tag, and link
in =libB.so= automatically.

This tag is created when =libA.so= is linked: /its/ build command needs to
include =-lB=. These tags can be queried with the =objdump= command. For
instance:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ objdump -p /usr/lib/x86_64-linux-gnu/libopencv_highgui.so.2.4 | grep NEEDED

  NEEDED               libopencv_imgproc.so.2.4
  NEEDED               libdl.so.2
  NEEDED               libpthread.so.0
  NEEDED               librt.so.1
  NEEDED               libtbb.so.2
  NEEDED               libatomic.so.1
  NEEDED               libz.so.1
  NEEDED               libjpeg.so.62
  NEEDED               libpng16.so.16
  NEEDED               libtiff.so.5
  NEEDED               libImath-2_2.so.12
  NEEDED               libIlmImf-2_2.so.22
  NEEDED               libIex-2_2.so.12
  NEEDED               libHalf.so.12
  NEEDED               libIlmThread-2_2.so.12
  NEEDED               libgtk-x11-2.0.so.0
  NEEDED               libgdk-x11-2.0.so.0
  NEEDED               libpangocairo-1.0.so.0
  NEEDED               libatk-1.0.so.0
  NEEDED               libcairo.so.2
  NEEDED               libgdk_pixbuf-2.0.so.0
  NEEDED               libgio-2.0.so.0
  NEEDED               libpangoft2-1.0.so.0
  NEEDED               libpango-1.0.so.0
  NEEDED               libgobject-2.0.so.0
  NEEDED               libglib-2.0.so.0
  NEEDED               libfontconfig.so.1
  NEEDED               libfreetype.so.6
  NEEDED               libgthread-2.0.so.0
  NEEDED               libdc1394.so.22
  NEEDED               libv4l1.so.0
  NEEDED               libavcodec.so.57
  NEEDED               libavformat.so.57
  NEEDED               libavutil.so.55
  NEEDED               libswscale.so.4
  NEEDED               libopencv_core.so.2.4
  NEEDED               libstdc++.so.6
  NEEDED               libm.so.6
  NEEDED               libgcc_s.so.1
  NEEDED               libc.so.6
#+END_EXAMPLE

So a user building an application that uses =libopencv_highgui.so= doesn't need
to know to link in all this stuff when building their executable.

*** Solution

So I have a bunch of libraries that have no =DT_NEEDED= tags, and I want to
change their build invocations to add those tags. But which tags do I need?
There are many libraries, and it would take a lot of work to look through each
one and make a list of libraries that it depends on. I wrote this script to do
that work for me:

=infer_dt_needed.pl=

#+BEGIN_SRC perl
#!/usr/bin/perl

# Invoke this tool thusly:
#
#     nm -o -D *.so | perl infer_dt_needed.pl


use strict;
use warnings;

use feature ':5.10';

my %undef_syms_in_libs;
my %defining_lib_for_sym;
my %library_dep;
my %libraries;

while(<>)
{
    my ($lib,$type,$sym) = /^(.*?):           # symbol name:
                            (?:[0-9a-fA-F]+)? # possibly an address
                            \s+
                            ([a-zA-Z])        # symbol type
                            \s+
                            (.*?)             # symbol name
                            $/x
      or next;

    $libraries{$lib} = 1;

    # read in each symbol, and make a record of each defined and undefined
    # function. I look for very specific symbol types here, and this set is
    # sufficient for my application. Look at the 'nm' manpage for the other
    # possible symbols that may be necessary for other applications
    if( length($type) )
    {
        if($type eq 'T')
        {
            $defining_lib_for_sym{$sym} = $lib;
        }
        elsif($type eq 'U')
        {
            $undef_syms_in_libs{$lib} //= [];
            push @{$undef_syms_in_libs{$lib}}, $sym;
        }
    }
}

# I go through each undefined symbol, find the library that provides it, and if
# found, record the dependency between those libraries
for my $lib (keys %undef_syms_in_libs)
{
    for my $sym ( @{$undef_syms_in_libs{$lib}} )
    {
        if ($defining_lib_for_sym{$sym})
        {
            $library_dep{$lib} //= {};
            $library_dep{$lib}{$defining_lib_for_sym{$sym}} = 1;
        }
    }
}

# print out the dependencies
say "######### All dependencies: #########";
for my $lib(keys %library_dep)
{
    say "$lib:" . join('', map { ' -l' . s/^lib(.*)\.so.*/\1/r} keys %{$library_dep{$lib}} );
}


# Find the existing DT_NEEDED flags
my %libraries_have_needed;
for my $lib (keys %libraries)
{
    my %thislib_have_needed;

    my $fd;
    open $fd, '-|', "objdump -p $lib | awk '\$1 ~ /NEEDED/ { print \$2 }'";
    while(<$fd>)
    {
        chomp;
        next unless length($_);
        $thislib_have_needed{$_} = 1;
    }

    $libraries_have_needed{$lib} = \%thislib_have_needed;
}

# Compare the existing DT_NEEDED flags with linkages we found by looking at the
# symbols. We SHOULD have DT_NEEDED flags for everything. If we don't, report it
say "######### All MISSING dependencies: #########";
for my $lib(keys %library_dep)
{
    # Library $lib depends on the libraries in keys %{$library_dep{$lib}}
    for my $libdep (keys %{$library_dep{$lib}})
    {
        if (not exists $libraries_have_needed{$lib}{$libdep})
        {
            say "Missing DT_NEEDED: $lib depends on $libdep";
        }
    }
}
#+END_SRC

To use this tool, read the symbol table from all my shared libraries, and feed
this list into the tool. The tool finds the connection between libraries that
use a particular symbol and libraries that provide it. In the trivial example
above, the invocation and output would look like this:

#+BEGIN_EXAMPLE
$ nm -o -D libA.so libB.so | perl infer_dt_needed.pl

######### All dependencies: #########
libA.so: -lB

######### All MISSING dependencies: #########
Missing DT_NEEDED: libA.so depends on libB.so
#+END_EXAMPLE

The =nm= tool reads the symbol table, and the =-o -D= options are more or less
required for this usage. The tool only looks at a very specific set of symbols
(nm types =T= and =U=), which is sufficient for my purposes. Read the =nm=
manpage and update the script if you need more.

With many libraries and many symbols this sort of automated bookeeping is
invaluable. The resulting list can be used to update the Makefiles to put the
appropriate tags in place.

** DONE Interfacing rr to gdb in GNU Emacs                        :tools:dev:
   CLOSED: [2017-02-24 Fri 00:08]

First of all, for those who haven't tried [[http://www.rr-project.org][=rr=]] yet, stop everything and go check
it out right now. It's revolutionary. I'll wait.

OK. Back? I generally talk to gdb from inside GNU Emacs. This has two modes:

1. GUD. This is the "old" mode, and you launch it via =M-x gud-gdb=. GUD expects
   gdb to be running with annotation level 1. This annotation level can be set
   in several ways:

   - =gdb --fullname=
   - =gdb -f=
   - =gdb --annotate 1=
   - Execute =set annotate 1= within =gdb=

   When you =M-x gud-gdb=, emacs suggests =gdb --fullname=, so by taking the
   suggestion the user doesn't have to remember any of these.

2. gdb-mi. This is the "new" mode, launched via =M-x gdb=. It also needs a
   special communications mode, but this one is entered with any of

   - =gdb -i=mi=
   - =gdb --ui=mi=
   - =gdb --interpreter=mi=

   As with GUD, =M-x gdb= suggests something: =gdb -i=mi= so once again the user
   doesn't need to remember any of this.

Now, when I =rr replay=, I'm also talking to gdb, and I need to pass the same
communication options. But =rr= doesn't support these, so the user needs to do
something to turn them on, which at the very least means learning and
remembering trivia about these options. Well, I fixed this, so now I can now go
back to being blissfully ignorant:

https://mail.mozilla.org/pipermail/rr-dev/2017-February/000452.html

https://github.com/mozilla/rr/commit/684045339af9905e4ae432d237ba2cfb98ad0df3

Now, when invoking =rr replay= from Emacs, you simply take the suggested command
(=gdb --fullname= or =gdb -i=mi=), replace =gdb= with =rr replay=, add the log
we're replaying, and things then work as expected.

Note that if you're stuck with an older version of =rr= then at least for GUD
there's a simple workaround:

#+BEGIN_EXAMPLE
echo set annotate 1 >> ~/.gdbinit
#+END_EXAMPLE

** DONE RPM distros and debug-symbol package dependencies      :distros:rant:
   CLOSED: [2017-03-31 Fri 11:48]

This is a rant.

So I've been using CentOS at work, and a long-standing suspicion that RPM
distros are built by children is being repeatedly confirmed.

Like other reasonable distributions, CentOS splits debug symbols into a packages
separate from the binary package itself, so that users don't need to always
download debug symbols they almost never use. Unlike other reasonable
distributions, CentOS creates one debug-symbol package per /source/, not per
/binary/ package. So if you have a source package called =foo= that builds
binary packages =foo=, =foo-devel= and =foo-tools= then RPM distros generate a
single =foo-debuginfo= package for all the symbols. This in itself isn't stupid,
but it raises a question: what should =foo-debuginfo= depend on? After all,
debug symbols are only useful together with the binary they were generated from.

If you have a 1-1 relationship between binary packages and their debug symbol
package, then the answer is obvious: =foo-tools-debuginfo= should depend on
=foo-tools= of the /exact same/ version. But if you have the joint debug
package, then should it depend on =foo=? =foo-devel=? =foo-tools=? Clearly the
answer is to depend on =foo= of the same version *or* =foo-devel= of the same
version *or* =foo-tools= of the same version. But since RPM was built by
children, it doesn't support this basic boolean specification.

So what do they do? They simply omit the dependency! So a hapless user installs
=foo= and =foo-debuginfo= and successfully debugs something. Then at some later
point they update =foo=, which /does not/ pull in a new =foo-debuginfo=. THEN
they try to debug something, and stuff doesn't work. So they then spend valuable
time debugging this bullshit and write an annoyed blog post.

In my case this was slightly more complex because the package in question was
python, and the debuginfo package contains not just the debug symbols, but also
gdb scripts to get python-level instrumentation. So figuring out where and how
gdb loads its scripts was the first step. And this is exactly why anal
dependencies are important: because the world is complicated and by omitting
something like this you're wasting somebody's time and defeating the whole point
of having a distro.

** DONE C++ is weird and full of pitfalls                        :tools:rant:
   CLOSED: [2017-04-18 Tue 16:36]

This is a rant. It's fairly widely known that this language is full of pitfalls
that waste one's time, and humans are better off using other languages. But I
keep getting stuck on projects where this isn't well known. So I will vent by
ranting.

Say we have this simple C++ program:

#+BEGIN_SRC C++
#include <stdio.h>

class B
{
public:
    virtual void f(void)
    {
        printf("Base: B::f\n");
    }
};

class C : public B
{
public:
    virtual void f(void)
    {
        printf("Child: C::f\n");
    }
};

int main(void)
{
    C c;
    B* pb = &c;
    pb->f();
    return 0;
}
#+END_SRC

This defines a derived class and calls a virtual function through a base
pointer. As expected, the virtual call finds the right function, and we print
=Child: C::f=. Now suppose we override the virtual function in a slightly
different way:

#+BEGIN_SRC C++
    virtual void f(int x = 5)
    {
        printf("Child: C::f\n");
    }
#+END_SRC

Here the child =f()= takes a default argument. Since this is a /default/
argument, one could expect that it looks similar enough to the base =f(void)= to
be overridden. Except it isn't: we print =Base: B::f=. The same thing happens if
the base has the default argument. Or if the function is defined as =const=.

This is a particularly pernicious failure, since your child =f()= is just
silently not called, which results in longer-than-they-should-be debugging
sessions. I get why this limitation exists, but I completely reject that this
state of affairs is even remotely acceptable. Name overloading should either be
banned by the language outright, or it should generate errors at even the
slightest hint of ambiguity. The failure described above is obviously ambiguous
and error prone, and this is clealy knowable at compile time.

C++ is full of such features that on the surface make writing code easier, but
that result in code that's impossible to read, and really easy to break.

Let's use better languages, OK?

** DONE rr in a chroot                                            :tools:dev:
   CLOSED: [2017-05-12 Fri 13:52]

I use [[http://www.rr-project.org][=rr=]] to magically debug applications in both temporal directions. To do
this you run the slave application inside =rr=, and then play back the log it
creates. I /also/ use a different distro on my box (Debian) than what we
standardized on at work (CentOS). To make this work, I have a CentOS chroot on
my host box, and most debugging happens inside this chroot. For various
uninteresting reasons (CentOS isn't very good, essentially) it's a pain in my
ass to get =rr= running on the CentOS side, while it's trivial on Debian.

So what I want is to use an =rr= running /outside/ the chroot (Debian) to
instrument an application running /inside/ the chroot (CentOS). This raises the
question: what is the chroot is for, anyway. The answer: to provide the client
OS's shared libraries (at least that's close-enough for this topic). So if
running "inside the chroot" simply means to link with some set of shared
libraries, I can do that just as well without any chroot, simply by instructing
the linker appropriately.

An example. Let's say I have a chroot session managed with [[https://wiki.debian.org/Schroot][=schroot=]] called
=centos_session=. The chroot lives in =/home/dima/centos/=. And I have a script
to run things inside this session called =centos=:

#+BEGIN_SRC sh
#!/bin/zsh
schroot -r -c centos_session -- $*
#+END_SRC

Now let's say I want to instrument an application called =app=. I ask the linker
about the full set of shared libraries my application needs:

#+BEGIN_EXAMPLE
$ centos ldd app

linux-vdso.so.1 =>  (0x00007ffdbaf9d000)
libjpeg.so.62 => /lib64/libjpeg.so.62 (0x00007f748b974000)
libz.so.1 => /lib64/libz.so.1 (0x00007f748b75e000)
libdogleg.so.2 => /lib64/libdogleg.so.2 (0x00007f748b343000)
liblbfgs.so.1 => /lib64/liblbfgs.so.1 (0x00007f748b13e000)
libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007f748a5cb000)
libm.so.6 => /lib64/libm.so.6 (0x00007f748a2c9000)
libgomp.so.1 => /lib64/libgomp.so.1 (0x00007f748a0b2000)
libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f7489e9b000)
libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f7489c7f000)
libc.so.6 => /lib64/libc.so.6 (0x00007f74898bd000)
libbsd.so.0 => /lib64/libbsd.so.0 (0x00007f74896ad000)
libdl.so.2 => /lib64/libdl.so.2 (0x00007f74894a9000)
librt.so.1 => /lib64/librt.so.1 (0x00007f74892a0000)
libpng15.so.15 => /lib64/libpng15.so.15 (0x00007f7489075000)
libtiff.so.5 => /lib64/libtiff.so.5 (0x00007f7488e01000)
libjasper.so.1 => /lib64/libjasper.so.1 (0x00007f7488ba7000)
/lib64/ld-linux-x86-64.so.2 (0x000055d54ef45000)
libresolv.so.2 => /lib64/libresolv.so.2 (0x00007f747c6a6000)
... (many more)
#+END_EXAMPLE

This is a long list, and all the libraries are /relative to the chroot/. I can
now try to execute this application /without the chroot/, simply by asking the
linker to load all the inside-the-chroot libraries that I know the application
needs. I ignore the things that touch the kernel and the dynamic linker itself
(=linux-vdso.so.1= and =/lib64/ld-linux-x86-64.so.2=), since I'm going to be
using the host to link:

#+BEGIN_EXAMPLE
$ LIBS=`centos ldd app |
        perl -anE 'if($F[2] =~ m{^/usr|^/lib})
                   { push @l,"/home/dima/centos7$F[2]"; }
                   END
                   { say join(":",@l); }'`


$ echo $LIBS

/home/dima/centos/lib64/libjpeg.so.62:/home/dima/centos/lib64/libz.so.1:.....


$ LD_PRELOAD=$LIBS ./app

./app: relocation error: /home/dima/centos/lib64/libc.so.6: symbol _dl_starting_up, version GLIBC_PRIVATE not defined in file ld-linux-x86-64.so.2 with link time reference
#+END_EXAMPLE

OK, well /that/ didn't work. It looks like some core linker component (from
Debian) didn't like some part of my libc (from CentOS). I don't care enough to
investigate this deeply. My Debian install is significantly newer than the
CentOS one, and I'd expect core libraries like libc to be compatible. So I
extend my little script to use more of the core libraries from the host:

#+BEGIN_EXAMPLE
$ LIBS=`centos ldd app |
        perl -anE 'if($F[2] =~ m{^/usr|^/lib} &&
                      $F[2] !~ m{/libc.so.6|ld-linux|libdl.so|libgcc|libm.so|libpthread.so|libresolv.so|libstdc\+\+|libgfortran})
                   { push @l,"/home/dima/centos7$F[2]"; }
                   END { say join(":",@l); }'`


$ LD_PRELOAD=$LIBS ./app

[ the thing runs! ]


$ LD_PRELOAD=$LIBS rr record ./app

[ the thing runs! And I get an rr log! ]

#+END_EXAMPLE

Huzzah! You can see which libraries I pulled from Debian above. I didn't choose
this list in a particularly principled way, just picked the core language,
linker stuff that seemed important, and things ended up working. More attention
to detail may be needed when looking at other distros or running other
applications. Runtime linking would need more attention too, but the concept is
clearly sound: you don't /need/ a chroot at all here.

As an aside, if I was using something heavier-weight than a chroot (a VM for
instance), then I possibly wouldn't have direct access to the file system, and I
wouldn't be able to do this.

** DONE Another gcc bug: undetected use of unitialized memory     :tools:dev:
   CLOSED: [2017-05-20 Sat 18:30]

Last week I had to use an hour to debug an intermittent issue in my program that
ended up being a simple, accidental use of uninitialized memory ([[http://www.rr-project.org][=rr=]] made the
debugging session much faster; go try it).

Many compilers have basic static analysis features to detect such bugs, and
throw a warning. =gcc= is such a compiler, and these warnings (turned on with
=-Wuninitialized=, which is a part of =-Wall=) are incredibly useful. And if
this worked properly, I wouldn't have had to spend that hour debugging. The
minimized program looks like this:

#+BEGIN_SRC C
void f(void);

double g(void)
{
    f();

    struct S
    {
        double x[2];
        double y;
    } s;

    for(int i=0; i<2; i++)
        s.x[i] = 2.0;

    return s.y;
}
#+END_SRC

I have an object of type =struct S=. I initialize some parts of it (=s.x=), but
not others (=s.y=). Then I return the uninitialized =s.y=. I'm unambiguously
returning an uninitialized value, but when I build this with gcc 7.1.0, no
warning is produced:

#+BEGIN_EXAMPLE
$ gcc-7 -Wall -Wextra -o /dev/null -c tst.c

[ no warning ]
#+END_EXAMPLE

One could argue that this isn't a bug, but rather a missing feature: maybe gcc
simply isn't sophisticated enough to analyze this properly. This isn't true,
however: I can get the warning if I either remove the =f()= call or unroll the
=for()= loop. With the latter:

#+BEGIN_SRC C
void f(void);

double g(void)
{
    f();

    struct S
    {
        double x[2];
        double y;
    } s;


    s.x[0] = 2.0;
    s.x[1] = 2.0;

    return s.y;
}
#+END_SRC

produces

#+BEGIN_EXAMPLE
$ gcc-7 -Wall -Wextra -o /dev/null -c tst.c

tst.c: In function 'g':
tst.c:16:13: warning: 's.y' is used uninitialized in this function [-Wuninitialized]
     return s.y;
            ~^~
#+END_EXAMPLE

Interestingly, a simpler test case was broken similarly in gcc 6, but /works/ in
gcc 7:

#+BEGIN_SRC C
void f(void);

double g(void)
{
    f();

    double a[6];
    return a[0];
}
#+END_SRC

As before, this produces the warning correctly if the =f()= call is removed.
Finally, it looks like =clang=, for whatever reason, fails to produce the
warning in all of these cases. I tried out the pre-release =clang= 5.0, and all
of these are too complicated for it to handle here.

The gcc bug report:

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80824

** DONE Interfacing numpy and C: an example                 :tools:dev:numpy:
   CLOSED: [2017-07-23 Sun 17:09]

I need to implement a numerical algorithm. I'd like to write all the
performance-critical parts in C and the rest in python. This should produce
something that works, and is maintainable, debuggable, etc, etc, while remaning
fast.

Connecting numpy to C is not even remotely new or interesting. People have been
doing this from the start, and there's plenty of documentation. What there isn't
is a simple skeleton program that provides a working python module written in C,
that exports some dummy function to take in numpy arguments, do something with
them, and return something. This should build, and just work. I'd want to use
this example as a starting point for any real work. Unfortunately, I could not
find this, so I had to spend hours figuring out how to do the basics here. I
should not have had to spend this time, so I now share this example program I
came up with for others to use. Please take this and hack it up. I'm releasing
it into the public domain, and am giving up all copyright.

This all works with Python 2.7.13 (Python 3 probably needs a few tweaks; patches
welcome), and numpy 1.12.1 on Debian/stretch. Clearly I'm not an expert here, so
if something isn't quite right, please tell me.

The python module is called =tst= and exports a single function =tst.foo()=.
Everything should be mostly self-explanatory.

=tst.c=:
#+BEGIN_SRC C
#define NPY_NO_DEPRECATED_API NPY_API_VERSION

#include <Python.h>
#include <numpy/arrayobject.h>
#include <signal.h>

const char foo_docstring[] =
    "This function returns the (i,j) element of the matrix\n"
    "\n"
    "i,j are given in kwargs; each is assumed to be 0 if omitted.\n"
    "Matrix must have ndims == 2 and (i,j) must be in-bounds (exception\n"
    "thrown otherwise). The returned element is converted to a double-\n"
    "precision float, so the input matrix does not need to have dtype=float\n";
static PyObject* foo(PyObject* NPY_UNUSED(self),
                     PyObject* args,
                     PyObject* kwargs)
{
    char* keywords[] = {"arr",

                        // optional kwargs
                        "i", "j",
                        NULL};

    PyObject* result = NULL;

    // Python is silly. There's some nuance about signal handling where it sets
    // a SIGINT (ctrl-c) handler to just set a flag, and the python layer then
    // reads this flag and does the thing. Here I'm running C code, so SIGINT
    // would set a flag, but not quit, so I can't interrupt the C code as it's
    // running, which is very surprising to the user. Thus I reset the SIGINT
    // handler to the default, and put it back to the python-specific version
    // when I'm done. This ignores a potential custom python sigint handler.
    // Left as an excercise for the reader
    struct sigaction sigaction_old;
    if( 0 != sigaction(SIGINT,
                       &(struct sigaction){ .sa_handler = SIG_DFL },
                       &sigaction_old) )
    {
        PyErr_SetString(PyExc_RuntimeError, "sigaction() failed");
        goto done;
    }

    PyArrayObject* arr;
    unsigned int i = 0, j = 0; // defaults, if arguments not given

    if(!PyArg_ParseTupleAndKeywords( args, kwargs,
                                     "O&|II",
                                     keywords,
                                     PyArray_Converter, &arr,
                                     &i, &j))
        return NULL;

    {
        int        ndim     = PyArray_NDIM(arr);
        npy_intp*  dims     = PyArray_DIMS(arr);
        int        typenum  = PyArray_TYPE(arr);
        if( ndim != 2 )
        {
            PyErr_SetString(PyExc_RuntimeError, "I assume that ndims == 2");
            goto done;
        }
        if( i >= dims[0] )
        {
            PyErr_Format(PyExc_RuntimeError, "I assume that i < shape[0], but i == %d", i);
            goto done;
        }
        if( j >= dims[1] )
        {
            PyErr_Format(PyExc_RuntimeError, "I assume that j < shape[1], but j == %d", j);
            goto done;
        }

        if( typenum != NPY_DOUBLE )
        {
            PyArrayObject* arr2 =
                (PyArrayObject*)PyArray_FromArray(arr, PyArray_DescrFromType(NPY_DOUBLE),
                                                  NPY_ARRAY_ALIGNED);
            Py_DECREF(arr);
            arr = arr2;
        }
    }


    // Useful metadata about this matrix
    __attribute__((unused)) char*      data0    = PyArray_DATA    (arr);
    __attribute__((unused)) char*      data1    = PyArray_BYTES   (arr);
    __attribute__((unused)) npy_intp  *strides  = PyArray_STRIDES (arr);
    __attribute__((unused)) int        ndim     = PyArray_NDIM    (arr);
    __attribute__((unused)) npy_intp*  dims     = PyArray_DIMS    (arr);
    __attribute__((unused)) npy_intp   itemsize = PyArray_ITEMSIZE(arr);
    __attribute__((unused)) int        typenum  = PyArray_TYPE    (arr);


    // Two ways to grab the data out of the matrix:
    //
    // 1. Call a function. Clear what this does, but if we need to access a lot
    // of data in a loop, this has overhead: this function is pre-compiled, so
    // the overhead will not be inlined
    double d0 = *(double*)PyArray_GetPtr( arr, (npy_intp[]){i, j} );

    // 2. inline the function ourselves. It's pretty simple
    double d1 = *(double*)&data0[ i*strides[0] + j*strides[1] ];

    // The two methods should be identical. If not, this example is wrong, and I
    // barf
    if( d0 != d1 )
    {
        PyErr_Format(PyExc_RuntimeError, "PyArray_GetPtr() inlining didn't work: %f != %f", d0, d1);
        goto done;
    }


    result = Py_BuildValue( "d", d0 );

 done:
    Py_DECREF(arr);

    if( 0 != sigaction(SIGINT,
                       &sigaction_old, NULL ))
        PyErr_SetString(PyExc_RuntimeError, "sigaction-restore failed");
    return result;
}

PyMODINIT_FUNC inittst(void)
{
    static PyMethodDef methods[] =
        { {"foo", (PyCFunction)foo, METH_VARARGS | METH_KEYWORDS, foo_docstring},
         {}
        };


    PyImport_AddModule("tst");
    Py_InitModule3("tst", methods,
                   "Demo module to show binding numpy to C");

    // Required to avoid mysterious segfaults
    import_array();
}
#+END_SRC

To build this, need this =setup.py=

#+BEGIN_SRC python
#!/usr/bin/python

from setuptools import setup
from distutils.core import Extension

setup(name         = 'tst',
      version      = '0.1',
      author       = 'Dima Kogan',
      author_email = 'dima@secretsauce.net',
      ext_modules  = [Extension('tst',
                                sources = ['tst.c'])])
#+END_SRC

A build:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ python setup.py build                 

running build
running build_ext
building 'tst' extension
creating build/temp.linux-x86_64-2.7
x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-HVkOs2/python2.7-2.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c tst.c -o build/temp.linux-x86_64-2.7/tst.o
x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-HVkOs2/python2.7-2.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-HVkOs2/python2.7-2.7.13=. -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/tst.o -o build/lib.linux-x86_64-2.7/tst.so
#+END_EXAMPLE

This builds the binary module into =/tmp/build/lib.linux-x86_64-2.7=. While
testing, we can then run it by telling python to find the module there (after
testing, =setup.py= can install to a more standard location). A =tst.py=:

#+BEGIN_SRC python
#!/usr/bin/python2

import numpy as np
import numpysane as nps
import sys

sys.path[:0] = ('/tmp/build/lib.linux-x86_64-2.7',)
import tst

x0 = np.arange(12).reshape(3,4)
x1 = nps.mv(x0, 0,1) # same matrix, with the axis order reversed

print tst.foo(x0, i=1, j=0)
print tst.foo(x1, i=0, j=1)
#+END_SRC

We create a matrix, and then an identical one with a flipped axis ordering. We
then print element =(1,0)= of the first, and element =(0,1)= of the second,
expecting identical results:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ python tst.py

4.0
4.0
#+END_EXAMPLE

Great! This works if we're creating a python module that we then want to call
from a python program. What if our main application is written in C also? We
then have a C application running some computations that invoke a python
interpreter, and run some python code, which in turn invokes some C code. This
is simpler in some ways, since we don't /need/ to build our module into a shared
object. Add this =main()= to the end of our =tst.c= above:

#+BEGIN_SRC C
int main(int argc, char **argv)
{
    Py_SetProgramName(argv[0]);
    Py_Initialize();

    inittst();

    PySys_SetArgvEx(argc, argv, 0);

    PyRun_SimpleString("import numpy as np\n");
    PyRun_SimpleString("import numpysane as nps\n");
    PyRun_SimpleString("import tst\n");

    PyRun_SimpleString("x0 = np.arange(12).reshape(3,4)\n");
    PyRun_SimpleString("x1 = nps.mv(x0, 0,1)\n");
    PyRun_SimpleString("print tst.foo(x0, i=1, j=0)\n");
    PyRun_SimpleString("print tst.foo(x1, i=0, j=1)\n");

    Py_Exit(0);
    return 0;
}
#+END_SRC

Then build the =tst.c= into an application, linking in python as a library. Note
that this bypasses =setup.py=:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ gcc -o tst -Wall -Wextra `pkg-config --cflags --libs python` tst.c
#+END_EXAMPLE

Now run the application we just built directly. We are /not/ invoking the python
interpreter ourselves:

#+BEGIN_EXAMPLE
dima@shorty:/tmp$ ./tst                                                             

4.0
4.0
#+END_EXAMPLE

Oh good. /Now/ I can actually try to get work done.

** DONE clockfunction: a non-sampling "profiler"                  :tools:dev:
   CLOSED: [2017-08-11 Fri 16:57]

Profiling tools generally answer the question *where's all my time going?*, and
they answer this question by periodically sampling a running executable to see
what it's doing at that point in time. When many samples are collected, they can
be analyzed and tallied. Execution hotspots then show up as a large sample
count: the application spent more time executing the hotspots. As expected, a
higher sampling rate yields more precise results at the cost of a higher
instrumentation overhead.

Recently I needed to answer a slightly diffent question: *how quickly does THIS
SPECIFIC function run?*. A sampling profiler is both too much and too little
here. Too much because I don't need to sample when the function of interest is
not running, and too little because a very high sampling resolution may be
necessary to report the timings with sufficient precision.

I looked around, and didn't see any available tools that solved this problem.
The [[https://perf.wiki.kernel.org/][=perf=]] tool from the linux kernel does 99% of what is needed to construct
such a tool, so I wrote a simple tool that uses =perf= to give me the facilities
I need: [[https://github.com/dkogan/clockfunction][=clockfunction=]].

=perf= is able to probe executables at /entry/ points to a function and at
/exit/ points of a function. Probing the entry is trivial, since the addresses
can be looked up in the symbol table or in the debug symbols. Probing the exit
is /not/ trivial, since multiple =ret= statements could be present. One could
either do some minor static analysis to find the =ret= statements, or one could
look at the return address upon entry, and then dynamically place a probe there.
And if there're any non-local exits, these would both break. I'm not 100% sure,
but I suspect that =perf= does neither, but instead uses some special hardware
to do this. In any case, I don't care: =perf= allows me to probe function
returns somehow, and that's all I care about.

After placing the probes, I run the executable being evaluated while =perf= is
recording all probe crossings. When the executable exits, the probe log can be
analyzed to extract the timing information.

The =clockfunction= tool automates this. Multiple functions can be sampled, with
each one specified as a =func@lib= string (ltrace-style). =func= is the name of
the function we care about. This could be a shell pattern to pick out multiple
functions. =lib= is the ELF library or executable that contains this function;
must be an absolute path. An example:

#+BEGIN_EXAMPLE
$ ./clockfunction.py '*rand*'@/usr/bin/perl perl_run@/usr/bin/perl perl -e 'for $i (0..100000) { $s = rand(); }'

# function mean min max stdev Ncalls
## All timings in seconds
Perl_drand48_init_r 7.55896326154e-06 7.55896326154e-06 7.55896326154e-06 0.0               1
Perl_drand48_r      1.95271501819e-06 1.76404137164e-06 3.67719912902e-05 4.0105865074e-07  100001
Perl_pp_rand        5.23026800056e-06 4.78199217469e-06 0.000326015986502 1.71576428687e-06 100001
perl_run            0.662568764063    0.662568764063    0.662568764063    0.0               1
#+END_EXAMPLE

The table was re-spaced for readability. We see that the main perl application
took 0.66 seconds. And =Perl_pp_rand= was called 100001 times, taking 5.23us
each time, on average, for a total of 0.523 seconds. A lower-level
=Perl_drand48_r= function took about 1/3 of the time of =Perl_pp_rand=. If one
cared about this detail of perl, this would be very interesting to know. And we
found it out without any compile-time instrumentation of our binary and without
even bothering to find out what the =*rand*= functions area called.

Recursive or parallel invocations are supported so far as the mean and Ncalls
will be reported correctly. The min, max and stdev of the timings will not be
available, however.

This tool is a quick hack. This tool calls =sudo= all over the place, which is
ugly. It's also probably not very robust to errors, and probably has other
issues. I'll probably clean things up as I go, but it works well already, and
it's done-enough for my present purposes.

License: released into the public domain; I'm giving up all copyright.

** DONE osmgnuplot update: correct plots over a large geographical area :tools:GIS:
   CLOSED: [2017-09-05 Tue 13:33]

[[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Rendering OpenStreetMap tiles in gnuplot")){/lisp}][Previously]] I described [[https://github.com/dkogan/osmgnuplot][=osmgnuplot=]], a tool to plot registered OpenStreetMap
tiles in gnuplot, which makes it easy to plot georeferenced data. The tool
worked just fine with a caveat: it would produce bogus plots when looking at a
large geographical area. This is because the relationship between latitude and
the row coordinate of the OSM image was assumed to be linear, which is not the
case in reality (longitude is always linear, however).

Well, I just hit this issue in one of my projects, and have fixed the tool to
lift this limitation. The latest version of the tool uses linked gnuplot axes to
connect the pixel coordinates to latitude/longitude. So the image is plotted
against the gnuplot x1,y1 axes, which are mapped to the latitude,longitude on
the x2,y2 axes. The conversions are all handled by gnuplot, which is quite nice.

One interesting thing is that the expression I was using for the
latitude-to-pixel conversion contained this subexpression:

# o-blog v1 can only export equations as mathjs, but I want a JS-less page. I
# just rendered the below (C-c C-x C-l) and copied the image manually
# $f(x)      = \frac{\sin(x)+1}{\cos(x)}$
# $f^{-1}(y) = \sin^{-1}\left( \frac{y^2 - 1}{y^2 + 1} \right)$
[[file:files/osmgnuplot2/forward.png]]

The code update requires me to compute the inverse conversion
(pixel-to-latitude), and deriving this, I need to reverse the above
subexpression. I derived this inverse:

[[file:files/osmgnuplot2/reverse.png]]

The proof is left as an exercise to the reader. If I had bothered to look it up,
however, I would have found all the expressions (forward and backward) [[http://wiki.openstreetmap.org/wiki/Slippy_map_tilenames][here]].

The script works, and some examples will appear in an [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Pole of road inaccessibility of the contiguous US")){/lisp}][upcoming post]].

There were some questions that came up in writing this, and the discussion is in
this [[https://sourceforge.net/p/gnuplot/mailman/gnuplot-beta/thread/877exe9qut.fsf%40scrawny/][mailing list thread]].

** DONE Pole of road inaccessibility of the contiguous US   :hiking:data:GIS:
   CLOSED: [2017-09-25 Mon 10:17]

So I want out hiking with a friend, and he was telling me about a 9-day
backpacking trip he took to reach some back-country cabin, which is said to be
further away from any driveable road than any other spot in the continental US.

I computed something very similar [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Poles of Inaccessibility in the San Gabriel Mountains")){/lisp}][earlier]] so I decided to extend the previous
work to the whole continental US to check if that area really /is/ the road Pole
of Inaccessibility. Note that for the San Gabriels I was looking at 3 different
poles:

- relative to all roads and trails
- relative to all roads
- relative to just /paved/ roads

Here I'm looking at the middle one. This is an arbitrary choice, and I'm only
doing that to match my friend's trip. As before, the OpenStreetMap database is
the source of data, so the results are only as good as that dataset. I've found
the data to be fairly good, and in any case, that's the only available source of
data.

I ran the computations, and updated the previous repo:
[[https://github.com/dkogan/inaccessibility]]. So where is this point? My friend was
sure that it was either in the SE corner of Yellowstone National Park (where he
went) or in the Frank Church wilderness in Idaho, or the Bob Marshall wilderness
in Montana. Want to know which one of these areas contains the pole? Read on.

The only real difference from the previous effort is that this is a much larger
area, which has two ramifications:

1. Simply applying the previous algorithm to the whole are would require
   processing a massive data set with an algorithm that's not well suited for
   it. This would be painful and slow

2. The existing implementation assumes a locally-flat Earth, which is very wrong
   if looking at the whole continental US.

Both of these concerns can be addressed by running some sort of coarse search
over the entire area, and then focusing in on the candidates areas using the
previous local technique. How can we find the candidate areas? I don't know of a
better way than an exhaustive grid search. The OSM data lives in a database with
fast spatial queries, so this shouldn't be too terrible. If somebody knows a
nicer way to query the database to avoid the grid search, please tell me.

I'm looking for a spatial box at least 40-50 miles per side that contains no
roads, so I semi-arbitrarily set my grid search to look for all lat/lon cells
0.4 degrees per side (I know these aren't really square in reality, but it's
good enough for this first pass). The contiguous US spans approximately 25-50
degrees of N latitude and 66-125 degrees of W latitude, so this search would
entail on the order of 10000 OSM database queries. I asked on =#osm= and it was
suggested strongly that making this many database queries is an unreasonable use
of a public resource and that I'd be banned long before I could finish anyway.
So I downloaded the North America subset of OSM and set up my own database
instance that I could query as many times as I wanted.

This proved to be very straightforward. The data can be obtained from multiple
places. I got it from [[https://download.geofabrik.de/][here]]. Instructions to get the database set up live [[http://wiki.openstreetmap.org/wiki/Overpass_API/Installation][here]],
and they are simple, and just work. You download data, download software, build
software, feed the data to the software, and that's it. The North America
extract weighs in at 13GB and the computer pulled down all these bits while I
twiddled my thumbs. The import took hours, and I ran it overnight. In the
morning I had a functional database that supported Overpass queries. I tweaked
the earlier script to look for roadless cells and to just report the number of
matching ways:

#+BEGIN_SRC sh
#!/bin/zsh

# script to get OSM ways from the global database in a lat/lon rectangle.
# Required usage is
#   $0 lat lon radius


for i (`seq 3`)
    {
        [ -z "${@[$i]}" ] && { echo "need 3 arguments on the commandline: lat,lon,radius"; exit 1  }
    }


lat0=$(($1 - $3))
lat1=$(($1 + $3))
lon0=$(($2 - $3))
lon1=$(($2 + $3))

OVERPASS="/home/dima/projects/osm_overpass/osm-3s_v0.7.53/"

$OVERPASS/bin/osm3s_query --db-dir=$OVERPASS/db <<EOF
[out:json];

way["highway"] ["highway" != "footway" ] ["highway" != "path" ]($lat0,$lon0,$lat1,$lon1);

(._;>;);

out count;

EOF

#+END_SRC

I ran this in a loop, driven by =sample.pl=:

#+BEGIN_SRC perl
#!/usr/bin/perl
use strict;
use warnings;

use feature ':5.10';
use IPC::Run qw(run);


my $lat0 = 25.0;
my $lat1 = 50.0;
my $lon0 = -125.0;
my $lon1 = -66.0;
my $r    = 0.2;


my $N_lat = int (($lat1 - $lat0) / (2. * $r) + 0.5);
my $N_lon = int (($lon1 - $lon0) / (2. * $r) + 0.5);

say "# lat lon Nways";

for my $ilat (0..$N_lat-1)
{
    my $lat = $lat0 + $ilat * 2. * $r;
    for my $ilon (0..$N_lon-1)
    {
        my $lon = $lon0 + $ilon * 2. * $r;

        my $in = '';
        my $out;
        my $err;
        run [ './query_center_countonly.sh', $lat, $lon, $r ], \$in, \$out, \$err;

        my ($Nways) = $out =~ /"ways": ([0-9]+)/;
        say "$lat $lon $Nways";
    }
}

#+END_SRC

This took about 30 minutes, and in the end I had a [[file:files/inaccessibility_lower48/samples.dat][nice list]] of cell-centers and
their corresponding road-way counts. It looks like this (made with [[file:files/inaccessibility_lower48/big_samplegrid_all.gp][this script]]):

[[file:files/inaccessibility_lower48/big_samplegrid_all.png]]

Neat! Here the way counts are represented by bigger points and brighter colors.
This plot just exists to look pretty and to tell me that the sampling is
functional: road density is correlated with population density. OK. The goal of
this was to find empty cells, so let's do that (made with [[file:files/inaccessibility_lower48/big_samplegrid_onlyempty.gp][this script]]):

[[file:files/inaccessibility_lower48/big_samplegrid_onlyempty.png]]

Neat!!! The oceans have no roads. The Great Lakes also have no roads. There's a
big roadless chunk on the Mexican side of the Rio Grande, but it looks like
that's due more to missing data than to the roadless-ness of that area. So I
focus on the US. Looks like there're exactly two empty cells: one in Wyoming
(this is the Yellowstone area) and one in Idaho (Frank Church wilderness).

For each of the two candidates I download the actual way data for each of the
candidate areas to get a sanity check and to get a bound for the area where I
will compute the voronoi diagram. For the Yellowstone area:

#+BEGIN_EXAMPLE
$ ./query_rect.sh 43.7 -110.6 44.6 -109.5

$ < query*.json(om[1]) | 
    awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' |
    sed 's/,//g' |
    feedgnuplot --domain --xlabel lon --ylabel lat --square
#+END_EXAMPLE

[[file:files/inaccessibility_lower48/roads_around_yellowstone_point.png]]

As expected, this looks like an empty area surrounded by roads. Let's process it
further, using the previous tools:

#+BEGIN_EXAMPLE
$ ../massage_input.pl query*.json(om[1])

$ ../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/)
-1936 -481
-4993 34239
-17383 -31726
32848 -2699
distance: 34854.600557

# ../massage_input.pl query*.json(om[1]) -1936 -481 -4993 34239 -17383 -31726 32848 -2699
44.1469937670531,-110.074264145003
44.4600470404567,-110.112910721116
43.8666857606725,-110.266830150028
44.1270516438681,-109.638457792143
#+END_EXAMPLE

Cool! The furthest-out point is 34.85km from the nearest road. We have the
coords of this nearest point and of the 3 bounding points that lie on the
nearest roads (centers of incident voronoi edges). These look like this:

https://caltopo.com/m/T90N

Doing the same thing for the empty cell in the Frank Church wilderness:


#+BEGIN_EXAMPLE
$ ./query_rect.sh 45.1 -115.4 45.7 -114.6

$ < query*.json(om[1]) | 
    awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' |
    sed 's/,//g' |
    feedgnuplot --domain --xlabel lon --ylabel lat --square
#+END_EXAMPLE

[[file:files/inaccessibility_lower48/roads_around_frankchurch_point.png]]

#+BEGIN_EXAMPLE

../massage_input.pl query*.json(om[1])

../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/)
-1799 2013
5545 25966
-23440 14637
-5850 -22711
distance: 25053.636148

../massage_input.pl query*.json(om[1]) -1799 2013 5545 25966 -23440 14637 -5850 -22711
45.4188043146378,-115.023049362629
45.6347011469176,-114.928683730544
45.5324855876438,-115.300922886895
45.196820550155,-115.074658128517
#+END_EXAMPLE

OK. This area isn't as empty, and you can only get 25.05km away from the nearest
road:

https://caltopo.com/m/H0TM

So we found a point 34.85km from the nearest road. This is the furthest-out
point in areas where we found an empty grid cell. I picked my grid sampling
semi-arbitrarily, so now I need to make sure that no circles with a radius
larger than 34.85km could lie anywhere else. What is the largest-radius circle
that I could possibly fit into areas without empty cells? The worst case is on
the Southern edge of my search area (largest distance between adjacent
even-latitude divisions). In the worst case I'd have a 2x2 block of cells where
the only road points lie in the four opposite corners of this block; these 4
cells would be technically non-empty, but you could fit a large circle there:

#+ATTR_HTML: :width 90%
[[file:files/inaccessibility_lower48/worstcase.svg]]

My cells are 0.4 degrees per side, so the largest circle that fits among
non-empty cells has radius

#+begin_comment
$0.4 \sqrt{2} = 0.57 \mathrm{ degrees} = 0.4 \sqrt{2} \frac{\pi}{180} R_\mathrm{earth} = 62901.35 \mathrm{ meters}$
#+end_comment
[[file:files/inaccessibility_lower48/0.4sqrt2.png]]

So in the worst case, we can fit a radius 62.9km circle within my non-empty
cells. This is a bit less than double my radius 34.85km circle that I already
found. Thus I need to re-run my grid-sampling with a finer grid, and check all
the empty areas that turns up. Fine.

The worst case was slighly less than a factor of 2 off. I keep my numbers round,
and resample with cell widths of 0.2 degrees instead of 0.4 degrees. This took
over 2 hours. Anybody know a better way? The resulting map of empty cells looks
like this ([[file:files/inaccessibility_lower48/samples_0.1.dat][data]], [[file:files/inaccessibility_lower48/big_samplegrid_onlyempty_0.1.gp][script]]):

[[file:files/inaccessibility_lower48/big_samplegrid_onlyempty_0.1.png]]

We now have a higher-res view, and there're more locations in the continental US
that could contain the point we seek. I checked each one, and that gives a
conclusive result. *The winner is still the point in Yellowstone National Park
computed above: https://caltopo.com/m/T90N with a roadless radius of 34.85km*:

[[file:files/inaccessibility_lower48/yellowstone.png]]

There are several noteworthy runners up. In second place, coming in with a
roadless radius of 29.0km is the Bob Marshall Wilderness:
https://caltopo.com/m/FKP3

In third is the point with a roadless radius of 25.05km that we found above in
the Frank Church Wilderness: https://caltopo.com/m/H0TM.

So the original predictions were spot on. I'd like to do this again but staying
away from trails also, but that would require even smaller cell sizes, and I
really don't want to go through this brute-force grid-search process again.
Ideas?



#+begin_comment

looking through all the candidates

< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
36.9988443764437,-118.700754455334
37.1594173222578,-118.564463078861
36.9725072483412,-118.941194027935
36.8106973684886,-118.756814451555
distance: 21560.561652
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-118.5; cy=36.5; w=2; h=2; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
36.6182404576021,-118.480425422385
36.7717719329123,-118.34101598319
36.6056228296001,-118.715990033605
36.4529876848734,-118.595987019603
distance: 21073.577148
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-119.5; cy=38; w=2; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
37.6789466773625,-119.323071956094
37.6882057495995,-119.095773060414
37.846765617309,-119.403696967493
37.7276548319986,-119.542115397644
distance: 20027.013149
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ 
dima@scrawny:~/projects/inaccessibility/lower48$ 
dima@scrawny:~/projects/inaccessibility/lower48$ 
dima@scrawny:~/projects/inaccessibility/lower48$ cx=-123.6; cy=47.8; w=3; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
47.7703142614567,-123.595009006202
47.9681090031692,-123.582646214583
47.5763312929941,-123.647977575046
47.5733405735074,-123.580271537299
distance: 21984.533538
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-121.4; cy=48.8; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
48.8392377461774,-121.351015000326
49.0010795508578,-121.35446190063
48.7503947782245,-121.555587670172
48.6867816228213,-121.271077711711
distance: 17954.845231
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-121.0; cy=48.2; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
48.0994789651052,-121.068057588594
48.1552383096572,-121.265420699722
47.9956828969584,-121.21436183271
48.0802210735043,-120.855980753773
distance: 15898.095144
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-118.8; cy=41.2; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
41.238649342276,-118.772385447942
41.3479101384552,-118.793782633214
41.290663189891,-118.901832470416
41.1749981701495,-118.652742406169
distance: 12263.689591
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-113.5; cy=40.5; w=2; h=2; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
40.4046219285722,-113.605227499269
40.5866755920297,-113.485670995751
40.5163584367163,-113.82914722936
40.2035465048971,-113.568776058231
distance: 22633.397077
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-112.4; cy=41.0; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
41.0663338090071,-112.619500317054
41.2228708428885,-112.61373321938
40.9361195118447,-112.733407936277
40.9327683970627,-112.511706800174
distance: 17375.614966
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-110.3; cy=40.8; w=2; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
40.7175208333879,-110.616174063163
40.876187671278,-110.679240033613
40.5524662958417,-110.621495993973
40.5714457875096,-110.51382887281
distance: 18405.819366
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-108.4; cy=33.2; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
33.2450801461737,-108.463712355248
33.4117617187981,-108.45648614979
33.0800090630568,-108.489340507945
33.2298211975994,-108.265437343379
distance: 18519.281560
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-123.6; cy=49.8; w=3; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
50.1208161502195,-123.889684593762
50.2204678153231,-123.558848113193
50.2206038955042,-123.558946380024
49.9560974978007,-124.148259367468
distance: 26019.783074
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-117.0; cy=49.4; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
49.4284824401492,-116.946861799913
49.4559170833793,-116.765055324768
49.4460233606974,-117.131461133465
49.3228385180905,-116.855677826775
distance: 13488.161149
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-114.2; cy=49.2; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
48.8787840370136,-114.105579832535
49.0200442408965,-114.045040106118
48.7884231440657,-114.281649611831
48.7876115092405,-114.280665545801
distance: 16387.056815
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-114.0; cy=48.8; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
48.8694244281339,-113.967351013553
49.0200990671499,-114.045034267359
48.8293925229867,-114.20061388391
48.7551386860996,-113.800446769536
distance: 17642.589399
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-113.2; cy=47.7; w=3; h=4; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
47.5937940679444,-113.25172813214
47.8280928033937,-113.423717129583
47.5045536899974,-113.614707671762
47.5036846301514,-112.889038614692
distance: 29032.474210
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-114.4; cy=46.4; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
46.1733912204036,-114.728198493812
46.3206372552874,-114.652622874615
46.3206821557286,-114.652805385457
46.1464133734151,-114.505101917434
distance: 17437.052437
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-114.8; cy=46.2; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
46.1702298957518,-115.117848205197
46.3178537658114,-115.143039125283
46.1205022971264,-114.915784301709
46.133623864555,-114.910036946138
distance: 16507.871483
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-114.5; cy=46.0; w=2; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
46.3174314643972,-114.916786259501
46.3225060063103,-114.652968196666
46.3228226154325,-114.652982093884
46.1344199557521,-114.910024507801
distance: 20270.819625
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-115.0; cy=45.2; w=3; h=3; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
45.4192116036171,-115.023049194231
45.6358720287227,-114.9286851921
45.5332945797169,-115.300918600635
45.1964547712571,-115.074696889755
distance: 25053.397010
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-115.0; cy=44.0; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
43.9680901515954,-115.071997992162
43.9751463951544,-114.881480465558
43.9920128662981,-115.259930673638
43.8484054431942,-115.164667726815
distance: 15266.006173
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-110.4; cy=45.4; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
45.0740005653174,-110.362102521955
45.2049587696652,-110.240362076032
45.0998300154654,-110.581457951103
45.0995079698016,-110.581533356803
distance: 17459.993384
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-110.0; cy=44.8; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
44.6832327275974,-110.093346141747
44.8686480538879,-110.169058429391
44.5053697331901,-110.196141688958
44.5129488175268,-109.967122436631
distance: 21465.427203
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-110.0; cy=44.1; w=5; h=2; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
44.1470290162187,-110.074271308914
44.4603596504345,-110.112918673011
43.8665796071024,-110.266838034628
44.126881488827,-109.638459391943
distance: 34854.515373
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-109.5; cy=43.0; w=4; h=3; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
43.1846315051443,-109.667823906805
43.2779525696537,-109.444747605173
43.3121193867728,-109.857389227919
43.0089201832395,-109.757011425299
distance: 20826.446515
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-94.6; cy=49.2; w=3; h=3; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
49.3465618277399,-94.4736233277712
49.4224493087663,-94.1304744556204
49.3141735885236,-94.831618463913
49.1381148181973,-94.3059602692793
distance: 26192.657600
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-83.8; cy=43.8; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
43.8968428801422,-83.4757645197919
44.0506419736775,-83.5795183849721
43.7260662295355,-83.4715255699055
43.7261919781907,-83.4662484881302
distance: 18990.652217
dima@scrawny:~/projects/inaccessibility/lower48$ dima@scrawny:~/projects/inaccessibility/lower48$ cx=-80.8; cy=27.0; w=1; h=1; dx=$(( (w+2.) * 0.1 * 1.1)); dy=$(( (h+2.) * 0.1 * 1.1)); ./query_rect.sh $((cy-dy)) $((cx-dx)) $((cy+dy)) $((cx+dx)) && \
< query*.json(om[1]) | awk '/"lat"/ {lat=$2} /"lon"/ {lon=$2} /}/ {print lon,lat}' | sed 's/,//g' | feedgnuplot --domain --xlabel lon --ylabel lat --exit && \
../massage_input.pl query*.json(om[1]) && \
../voronoi < $PWD/query*.json(om[1]:s/json/dat/:s/query/points/) | \
sed -n '2,1000p' |\
xargs -n100 ../massage_input.pl query*.json(om[1]);

cmdand> cmdand cmdand> cmdand cmdand cmdand> cmdand cmdand cmdand pipe> cmdand cmdand cmdand pipe pipe> encoding remark: Please enter your query and terminate it with CTRL+D.
26.9455301812507,-80.8071022577094
26.9855196174801,-80.6213515937586
27.0500199485332,-80.958090961487
26.813112414914,-80.6873139612105
distance: 18935.831522
dima@scrawny:~/projects/inaccessibility/lower48$
dima@scrawny:~/projects/inaccessibility/lower48$ 



316 34854.515373
212 29032.474210
342 26192.657600
160 26019.783074
264 25053.397010

108 22633.397077
56 21984.533538
14 21560.561652
303 21465.427203
27 21073.577148
329 20826.446515
251 20270.819625
40 20027.013149
355 18990.652217
368 18935.831522
147 18519.281560
134 18405.819366
69 17954.845231
199 17642.589399
290 17459.993384
225 17437.052437
121 17375.614966
238 16507.871483
186 16387.056815
82 15898.095144
277 15266.006173
173 13488.161149
95 12263.689591

#+end_comment

** DONE Python extension modules without setuptools or distutils  :tools:dev:
   CLOSED: [2017-11-14 Tue 15:22]

#+begin_o_blog_alert info Update
I recently added python3 support (its build stuff is thankfully almost
identical), pushed the demo to a [[https://github.com/dkogan/python_extensions_without_setuptools][repo]], and integrated this stuff into [[https://github.com/dkogan/mrbuild][mrbuild]]
#+end_o_blog_alert


I'm a big fan of standardized build systems, and I get really annoyed when
projects come along and try to reinvent this particular wheel in their
"ecosystem". Because no "ecosystem" is truly isolated. This is unfortunatly
ubiquitous, and I spend a lot of my time being annoyed. Usually I can simply
ignore the chaos, but sometimes I need to step into it. Today was such a day.

At work I work with a number of software projects, mostly written in C. Each one
lives in a separate repository and builds its own shared library. All of these
use a standardized build system (using GNU Make), so each project-specific
Makefile is short, simple and uniform. For one of these projects I wanted a
Python interface in addition to the one provided by the C header. The "normal"
way to build a Python extension module is to use =setuptools= and/or
=distutils=. You're supposed to create a =setup.py= file that declares the
module via some boilerplaty runes, then you invoke this =setup.py= in some way
to build the module, and then you need to install the module somewhere to
actually use it. At least I think you're supposed to do those things; these
tools are massively complicated, and I won't pretend to fully understand them.
Which is exactly the point: I already have a build system and I know how to
build C code, and I don't /want/ to learn a new build system for every language
I touch. This problem has already been solved.

Thus what I /really/ want is to build the extension module within my build
system, instead of using some other infrastructure just for this component. This
would keep the build uniform, and the various things the build system does would
remain working. There's nothing special about the python sources, and I don't
want to pretend like there is. Requirements:

- =make= should work. I.e. I should be able to invoke =make= and everything
  should build
- Normal =make= things should work. I.e. I should be able to invoke =make -j=,
  and have parallel makes work. I should be able to invoke =make -n=, and get a
  full list of build commands that would run. And so on.
- Normal build system options should work. For instance if my build system has a
  specific method for turning compiler optimizations on/off, this method should
  apply as usual to the Python extension module
- Python scripts using the extension module should work either with the module
  built in-place /or/ installed. This should work without touching =sys.path= or
  =LD_LIBRARY_PATH= or any such nonsense

I think these all are /extremely/ reasonable, and I'm trying to clear what
should be a very low bar.

The most direct way to build the python extension module as a part of the larger
build system is to make the build system invoke =setup.py=. But then

- =make -n= would be broken since Make was never told what =setup.py= does
- Dependencies of whatever =setup.py= does would need to manually be
  communicated to Make, otherwise we could easily rebuild too often or not often
  enough
- =make -j= wouldn't work: the parallelization settings wouldn't make it to the
  =setup.py=, and it would be crippled by the incomplete dependency graph
- Python scripts using the extension module would need a =sys.path= to find this
  extension module
- Build customizations wouldn't work either: =setup.py= does /something/ but it
  lives in a vacuum, and any Makefile settings would not be respected

Today I integrated the build of my extension module into my build system,
/without/ using any =setup.py=. This solves /all/ the issues. Since
fundamentally the only thing the =setup.py= does is to compile and link some C
code with some specific flags, I just need a way to query those flags, and tell
my build to use them.

This is most easily described with [[https://github.com/dkogan/python_extensions_without_setuptools][an example]]. Let's say I have some C code I
want to wrap in my main directory:

=c_library.h=:
#+BEGIN_SRC C
void f(void);
#+END_SRC

and =c_library.c=:
#+BEGIN_SRC C
#include <stdio.h>
#include "c_library.h"

void f(void)
{
    printf("in f() written in C\n");
}
#+END_SRC

I also have a basic python wrapper of this library called =c_library_pywrap.c=:

#+BEGIN_SRC C
#include <Python.h>
#include <stdio.h>

#include "c_library.h"

static PyObject* f_py(PyObject* self __attribute__((unused)),
                      PyObject* args __attribute__((unused)))
{
    printf("in f() Python wrapper. About to call C library\n");
    f();
    Py_RETURN_NONE;
}


static PyMethodDef methods[] =
    { {"f", (PyCFunction)f_py, METH_NOARGS, "Python bindings to f() in c_library\n"},
      {}
    };

#if PY_MAJOR_VERSION == 2

PyMODINIT_FUNC initc_library(void)
{

    PyImport_AddModule("c_library");
    Py_InitModule3("c_library", methods,
                   "Python bindings to c_library");
}

#else

static struct PyModuleDef module =
    {
     PyModuleDef_HEAD_INIT,
     "c_library",   /* name of module */
     "Python bindings to c_library",
     -1,
     methods
    };

PyMODINIT_FUNC PyInit_c_library(void)
{
    return PyModule_Create(&module);
}

#endif
#+END_SRC

This defines a python extension module called =c_library=, and exports a
function =f= that calls the written-in-C =f()=. This =c_library_pywrap.c= is
what would normally be built with the =setup.py=. I want my
importable-from-python modules to end up in a subdirectory called =project/=. So
=project/__init__.py= exists and for testing I have a separate written-in-python
module =project/pymodule.py=:

#+BEGIN_SRC python
import c_library

def g():
    print "in my written-in-python module g(). Calling c_library.f()"
    c_library.f()
#+END_SRC

This module calls our C wrapper. Finally, I also have a test script in the main
directory called =test.py=:
#+BEGIN_SRC python
import project.pymodule
import project.c_library

project.c_library.f()
project.pymodule .g()
#+END_SRC

So all python modules (written in either C or python) should be importable with
a =import project.whatever=. Inside =project/= a simple =import whatever=
suffices.

Note that I didn't touch =sys.path=. Since the project subdirectory is called
=project/= both post-install and in-tree, the importer will find the module in
either case without any hand-holding. To make that work I build the
=project.c_library= DSO in-tree into =project/=. Now the main part: the
=Makefile=:
#+BEGIN_SRC makefile
# This is a demo Makefile. The stuff on top pulls out the build flags from
# Python and tell Make to use them. The stuff on the bottom is generic build
# rules, that would come from a common build system.

ifeq (,$(PYTHONVER))
  $(error Please set the PYTHONVER env var to "2" or "3" to select your python release)
endif


# The python libraries (compiled ones and ones written in python all live in
# project/).

# I build the python extension module without any setuptools or anything.
# Instead I ask python about the build flags it likes, and build the DSO
# normally using those flags.
#
# There's some sillyness in Make I need to work around. First, I produce a
# python script to query the various build flags, but replacing all whitespace
# with __whitespace__. The string I get when running this script will then have
# a number of whitespace-separated tokens, each setting ONE variable
define PYVARS_SCRIPT
from __future__ import print_function
import sysconfig
import re
conf = sysconfig.get_config_vars()
for v in ("CC","CFLAGS","CCSHARED","INCLUDEPY","BLDSHARED","LDFLAGS","EXT_SUFFIX"):
    if v in conf:
        print(re.sub("[\t ]+", "__whitespace__", "PY_{}:={}".format(v, conf[v])))
endef
PYVARS := $(shell python$(PYTHONVER) -c '$(PYVARS_SCRIPT)')

# I then $(eval) these tokens one at a time, restoring the whitespace
$(foreach v,$(PYVARS),$(eval $(subst __whitespace__, ,$v)))
# this is not set in python2
PY_EXT_SUFFIX := $(or $(PY_EXT_SUFFIX),.so)

# The compilation flags are all the stuff python told us about. Some of its
# flags live inside its CC variable, so I pull those out. I also pull out the
# optimization flag, since I want THIS build system to control it
FLAGS_FROM_PYCC := $(wordlist 2,$(words $(PY_CC)),$(PY_CC))
c_library_pywrap.o: CFLAGS += $(filter-out -O%,$(FLAGS_FROM_PYCC) $(PY_CFLAGS) $(PY_CCSHARED) -I$(PY_INCLUDEPY))

# I add an RPATH to the python extension DSO so that it runs in-tree. The build
# system should pull it out at install time
PY_LIBRARY_SO := project/c_library$(PY_EXT_SUFFIX)
$(PY_LIBRARY_SO): c_library_pywrap.o libc_library.so
	$(PY_BLDSHARED) $(PY_LDFLAGS) $< -lc_library -o $@ -L$(abspath .) -Wl,-rpath=$(abspath .)

all: $(PY_LIBRARY_SO)
EXTRA_CLEAN += project/*.so



##########################################################################
##########################################################################
##########################################################################
# vanilla build-system stuff. Your own build system goes here!

LIB_OBJECTS  := c_library.o
ABI_VERSION  := 0
TAIL_VERSION := 0



# if no explicit optimization flags are given, optimize
define massageopts
$1 $(if $(filter -O%,$1),,-O3)
endef

%.o:%.c
	$(CC) $(call massageopts, $(CFLAGS) $(CPPFLAGS)) -c -o $@ $<

LIB_NAME           := libc_library
LIB_TARGET_SO_BARE := $(LIB_NAME).so
LIB_TARGET_SO_ABI  := $(LIB_TARGET_SO_BARE).$(ABI_VERSION)
LIB_TARGET_SO_FULL := $(LIB_TARGET_SO_ABI).$(TAIL_VERSION)
LIB_TARGET_SO_ALL  := $(LIB_TARGET_SO_BARE) $(LIB_TARGET_SO_ABI) $(LIB_TARGET_SO_FULL)

BIN_TARGETS := $(basename $(BIN_SOURCES))

CFLAGS += -std=gnu99

# all objects built for inclusion in shared libraries get -fPIC. We don't build
# static libraries, so this is 100% correct
$(LIB_OBJECTS): CFLAGS += -fPIC
$(LIB_TARGET_SO_FULL): LDFLAGS += -shared -Wl,--default-symver -fPIC -Wl,-soname,$(notdir $(LIB_TARGET_SO_BARE)).$(ABI_VERSION)

$(LIB_TARGET_SO_BARE) $(LIB_TARGET_SO_ABI): $(LIB_TARGET_SO_FULL)
	ln -fs $(notdir $(LIB_TARGET_SO_FULL)) $@

# Here instead of specifying $^, I do just the %.o parts and then the
# others. This is required to make the linker happy to see the dependent
# objects first and the dependency objects last. Same as for BIN_TARGETS
$(LIB_TARGET_SO_FULL): $(LIB_OBJECTS)
	$(CC) $(LDFLAGS) $(filter %.o, $^) $(filter-out %.o, $^) $(LDLIBS) -o $@

all: $(LIB_TARGET_SO_ALL)
.PHONY: all
.DEFAULT_GOAL := all


clean:
	rm -rf *.a *.o *.so *.so.* *.d $(EXTRA_CLEAN)
#+END_SRC

There're two sections here: the part that actually defines how the extension
module should be built, and then a part with some generic rules that would
normally come from your own build system. Those are here just as an example. The
details should be clear from the comments. I should note that I got the
necessary build flags by poking =setup.py= with =sysdig=. =sysdig= is awesome;
go check it out.

And that's it. I can build:

#+BEGIN_EXAMPLE
$ V=2 make

cc  -std=gnu99 -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-VlMpWk/python2.7-2.7.14=. -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7  -O3 -c -o c_library_pywrap.o c_library_pywrap.c
cc  -std=gnu99 -fPIC  -O3 -c -o c_library.o c_library.c
cc -shared -Wl,--default-symver -fPIC -Wl,-soname,libc_library.so.0 c_library.o   -o libc_library.so.0.0
ln -fs libc_library.so.0.0 libc_library.so
x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-VlMpWk/python2.7-2.7.14=. -fstack-protector-strong -Wformat -Werror=format-security  -Wl,-z,relro c_library_pywrap.o -lc_library -o project/c_library.so -L/home/dima/blog/files/python_extensions_without_setuptools -Wl,-rpath=/home/dima/blog/files/python_extensions_without_setuptools
ln -fs libc_library.so.0.0 libc_library.so.0
#+END_EXAMPLE

And I can run the test:

#+BEGIN_EXAMPLE
$ python test.py 

in f() Python wrapper. About to call C library
in f() written in C
in my written-in-python module g(). Calling c_library.f()
in f() Python wrapper. About to call C library
in f() written in C
#+END_EXAMPLE

Furthermore, Make works. The sample =Makefile= has a rule where it optimizes
with =-O3= unless there's some other optimization flag already given, in which
case =-O3= is /not/ added. Look:

#+BEGIN_EXAMPLE
$ rm c_library_pywrap.o

$ make -n c_library_pywrap.o
cc  -std=gnu99 -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-VlMpWk/python2.7-2.7.14=. -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7  -O3 -c -o c_library_pywrap.o c_library_pywrap.c

$ CFLAGS=-O0 make -n c_library_pywrap.o
cc  -O0 -std=gnu99 -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-VlMpWk/python2.7-2.7.14=. -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7   -c -o c_library_pywrap.o c_library_pywrap.c
#+END_EXAMPLE

Which is really nice. And =make -n= works. And I can ask a particular target to
be built, which wouldn't be possible with =setup.py=.

The python extension module is a DSO that calls a function from my C library
DSO. When running in-tree an =RPATH= is required in order for the former to find
the latter:

#+BEGIN_EXAMPLE
$ objdump -p project/c_library.so | grep PATH
  RUNPATH              /home/dima/blog/files/python_extensions_without_setuptools
#+END_EXAMPLE

At install time, this should be stripped out (with the =chrpath= tool for
instance). Build systems generally do this anyway.

And I'm done. I really wish this wasn't a hack. It'd be nice if the Python
project (and all the others) provided these flags officially, via =pkg-config=
or something. Someday.

License: released into the public domain; I'm giving up all copyright.

** DONE Some new free-software bugs                                     :dev:
   CLOSED: [2017-12-18 Mon 20:16]

I found and fixed bugs in two free-software projects recently: zsh and elfutils.
Nothing particularly interesting, but I thought I'd make a note here.

*** zsh

Report, patch: http://www.zsh.org/mla/workers/2017/msg01704.html. This one is
interesting for two reasons:

1. The broken-ness is in a very non-obscure place: qualifying a particular type
   of glob doesn't work. For instance =dir*/file(Om)= would not actually do the
   sorting, as requested by the =(Om)=.

2. This bug was there /at least/ since 2001! This is my record, I think

*** elfutils

Report, patch: [[https://sourceware.org/bugzilla/show_bug.cgi?id=22546]]. This one
is also very non-obscure: =libdw= lies about the size of a variable if the
variable is a multi-dimensional array.

It is interesting how breakage is /everywhere/ (look: two bugs in common usage
in common tools) and /nowhere/ (tons of people uses both of these tools everyday
without hitting these) at the same time.

** DONE Vnlog!                                             :tools:data:vnlog:
   CLOSED: [2018-02-18 Sun 11:58]

In the last few jobs I've worked at I ended up writing a tool to store data in a
nice format, and to be able to manipulate it easily. I'd rewrite this from
scratch each time partly because I was never satisfied with the previous
version. Each iteration was an improvement on the previous one, and /this/
version is the good one. I wrote it at NASA/JPL, went through the release
process (this thing was called =asciilog= then), added a few more features, and
I'm now releasing it. The toolkit lives [[https://github.com/dkogan/vnlog/][here]] and here's the initial README:

*** Summary

Vnlog (pronounced "vanillog") is a trivially-simple log format:

- A whitespace-separated table of ASCII human-readable text
- Lines beginning with =#= are comments
- The first line that begins with a single =#= (not =##= or =#!=) is a /legend/,
  naming each column

Example:

#+BEGIN_EXAMPLE
#!/usr/bin/whatever
# a b c
1 2 3
## another comment
4 5 6
#+END_EXAMPLE

Such data works very nicely with normal UNIX tools (=awk=, =sort=, =join=), can
be easily read by fancier tools (=numpy=, =matlab= (yuck), =excel= (/yuck/), etc),
and can be plotted with =feedgnuplot=. This tookit provides some tools to
manipulate =vnlog= data and a few libraries to read/write it. The core
philosophy is to keep everything as simple and light as possible, and to provide
methods to enable existing (and familiar!) tools and workflows to be utilized in
nicer ways.

*** Synopsis

In one terminal, sample the CPU temperature over time, and write the data to a
file as it comes in, at 1Hz:

#+BEGIN_EXAMPLE
$ ( echo '# time temp1 temp2 temp3';
    while true; do echo -n "`date +%s` "; < /proc/acpi/ibm/thermal awk '{print $2,$3,$4; fflush()}';
    sleep 1; done )
    > /tmp/temperature.vnl
#+END_EXAMPLE

In another terminal, I sample the consumption of CPU resources, and log /that/
to a file:

#+BEGIN_EXAMPLE
$ (echo "# user system nice idle waiting hardware_interrupt software_interrupt stolen";
   top -b -d1 | awk '/%Cpu/ {print $2,$4,$6,$8,$10,$12,$14,$16; fflush()}')
   > /tmp/cpu.vnl
#+END_EXAMPLE

These logs are now accumulating, and I can do stuff with them. The legend and
the last few measurements:

#+BEGIN_EXAMPLE
$ vnl-tail /tmp/temperature.vnl
# time temp1 temp2 temp3
1517986631 44 38 34
1517986632 44 38 34
1517986633 44 38 34
1517986634 44 38 35
1517986635 44 38 35
1517986636 44 38 35
1517986637 44 38 35
1517986638 44 38 35
1517986639 44 38 35
1517986640 44 38 34
#+END_EXAMPLE

I grab just the first temperature sensor, and align the columns

#+BEGIN_EXAMPLE
$ < /tmp/temperature.vnl vnl-tail |
    vnl-filter -p time,temp=temp1 |
    vnl-align
#  time    temp
1517986746 45
1517986747 45
1517986748 46
1517986749 46
1517986750 46
1517986751 46
1517986752 46
1517986753 45
1517986754 45
1517986755 45
#+END_EXAMPLE

I do the same, but read the log data in realtime, and feed it to a plotting tool
to get a live reporting of the cpu temperature. This plot updates as data comes
in. I then spin a CPU core (=while true; do true; done=), and see the
temperature climb. Here I'm making an ASCII plot that's pasteable into the docs.

#+BEGIN_EXAMPLE
$ < /tmp/temperature.vnl vnl-tail -f           |
    vnl-filter --unbuffered -p time,temp=temp1 |
     feedgnuplot --stream --domain
       --lines --timefmt '%s' --set 'format x "%M:%S"' --ymin 40
       --unset grid --terminal 'dumb 80,40'

  70 +----------------------------------------------------------------------+
     |      +      +      +      +       +      +      +      +      +      |
     |                                                                      |
     |                                                                      |
     |                                                                      |
     |                      **                                              |
  65 |-+                   ***                                            +-|
     |                    ** *                                              |
     |                    *  *                                              |
     |                    *  *                                              |
     |                   *   *                                              |
     |                  **   **                                             |
  60 |-+                *     *                                           +-|
     |                 *      *                                             |
     |                 *      *                                             |
     |                 *      *                                             |
     |                **      *                                             |
     |                *       *                                             |
  55 |-+              *       *                                           +-|
     |                *       *                                             |
     |                *       **                                            |
     |                *        *                                            |
     |               **        *                                            |
     |               *         **                                           |
  50 |-+             *          **                                        +-|
     |               *           **                                         |
     |               *            ***                                       |
     |               *              *                                       |
     |               *              ****                                    |
     |               *                 *****                                |
  45 |-+             *                     ***********                    +-|
     |    ************                               ********************** |
     |          * **                                                        |
     |                                                                      |
     |                                                                      |
     |      +      +      +      +       +      +      +      +      +      |
  40 +----------------------------------------------------------------------+
   21:00  22:00  23:00  24:00  25:00   26:00  27:00  28:00  29:00  30:00  31:00
#+END_EXAMPLE

Cool. I can then join the logs, pull out simultaneous CPU consumption and
temperature numbers, and plot the path in the temperature-cpu space:

#+BEGIN_EXAMPLE
$ vnl-join -j time /tmp/temperature.vnl /tmp/cpu.vnl |
  vnl-filter -p temp1,user                           |
  feedgnuplot --domain --lines
    --unset grid --terminal 'dumb 80,40'

  45 +----------------------------------------------------------------------+
     |           +           +           +          +           +           |
     |                                       *                              |
     |                                       *                              |
  40 |-+                                    **                            +-|
     |                                      **                              |
     |                                     * *                              |
     |                                     * *      *    *    *             |
  35 |-+               ****      *********** **** * **** ***  ******      +-|
     |        *********   ********       *   *  *****  *** * ** *  *        |
     |        *    *                            * * *  * * ** * *  *        |
     |        *    *                                   *   *  *    *        |
  30 |-+      *                                                    *      +-|
     |        *                                                    *        |
     |        *                                                    *        |
     |        *                                                    *        |
  25 |-+      *                                                    *      +-|
     |        *                                                    *        |
     |        *                                                    *        |
     |        *                                                    *        |
  20 |-+      *                                                    *      +-|
     |        *                                                    *        |
     |        *                                                    *        |
     |      * *                                                    *        |
  15 |-+    * *  *                                                 *      +-|
     |      * *  *                                                 *        |
     |      ***  *                                                 *        |
     |      ***  *                                                 *        |
  10 |-+    ***  *                                                 *      +-|
     |      ***  *                                                 *        |
     |      ***  *                                                 *        |
     |      ***  *                                                 *        |
   5 |-+    ***  *                                                 *      +-|
     |      ***  *                                                 *        |
     |      * *  * *                                               *        |
     |      **** * ** *****  *********** +       *******       *****        |
   0 +----------------------------------------------------------------------+
     40          45          50          55         60          65          70
#+END_EXAMPLE

*** Description

As stated before, vnlog tools are designed to be very simple and light. There
exist other tools that are similar. For instance:

- https://csvkit.readthedocs.io/
- https://github.com/johnkerl/miller
- https://github.com/eBay/tsv-utils-dlang

These all provide facilities to run various analyses, and are neither simple nor
light. Vnlog by contrast doesn't analyze anything, but makes it easy to write
simple bits of awk or perl to process stuff to your heart's content. The main
envisioned use case is one-liners, and the tools are geared for that purpose.
The above mentioned tools are much more powerful than vnlog, so they could be a
better fit for some use cases.

In the spirit of doing as little as possible, the provided tools are wrappers
around tools you already have and are familiar with. The provided tools are:

- =vnl-filter= is a tool to select a subset of the rows/columns in a vnlog
  and/or to manipulate the contents. This is effectively an =awk= wrapper where
  the fields can be referenced by name instead of index. 20-second tutorial:

#+BEGIN_EXAMPLE
vnl-filter -p col1,col2,colx=col3+col4 'col5 > 10' --has col6
#+END_EXAMPLE
  will read the input, and produce a vnlog with 3 columns: =col1= and =col2=
  from the input and a column =colx= that's the sum of =col3= and =col4= in the
  input. Only those rows for which the =col5 > 10= is true will be output.
  Finally, only those rows that have a non-null value for =col6= will be
  selected. A null entry is signified by a single =-= character.

#+BEGIN_EXAMPLE
vnl-filter --eval '{s += x} END {print s}'
#+END_EXAMPLE
  will evaluate the given awk program on the input, but the column names work as
  you would hope they do: if the input has a column named =x=, this would
  produce the sum of all values in this column.

- =vnl-sort=, =vnl-join=, =vnl-tail= are wrappers around the corresponding GNU
  Coreutils tools. These work exactly as you would expect also: the columns can
  be referenced by name, and the legend comment is handled properly. These are
  wrappers, so all the commandline options those tools have "just work" (except
  options that don't make sense in the context of vnlog). As an example,
  =vnl-tail -f= will follow a log: data will be read by =vnl-tail= as it is
  written into the log (just like =tail -f=, but handling the legend properly).
  And you already know how to use these tools without even reading the manpages!
  Note: these were written for and have been tested with the Linux kernel and
  GNU Coreutils =sort=, =join= and =tail=. Other kernels and tools probably
  don't (yet) work. Send me patches.

- =vnl-align= aligns vnlog columns for easy interpretation by humans. The
  meaning is unaffected

- =Vnlog::Parser= is a simple perl library to read a vnlog

- =libvnlog= is a C library to simplify writing a vnlog. Clearly all you
  /really/ need is =printf()=, but this is useful if we have lots of columns,
  many containing null values in any given row, and/or if we have parallel
  threads writing to a log

- =vnl-make-matrix= converts a one-point-per-line vnlog to a matrix of data.
  I.e.

#+BEGIN_EXAMPLE
$ cat dat.vnl
# i j x
0 0 1
0 1 2
0 2 3
1 0 4
1 1 5
1 2 6
2 0 7
2 1 8
2 2 9
3 0 10
3 1 11
3 2 12

$ < dat.vnl vnl-filter -p i,x | vnl-make-matrix --outdir /tmp
Writing to '/tmp/x.matrix'

$ cat /tmp/x.matrix
1 2 3
4 5 6
7 8 9
10 11 12
#+END_EXAMPLE

All the tools have manpages that contain more detail. And tools will probably be
added with time.

*** C interface

For most uses, these logfiles are simple enough to be generated with plain
prints. But then each print statement has to know which numeric column we're
populating, which becomes effortful with many columns. In my usage it's common
to have a large parallelized C program that's writing logs with hundreds of
columns where any one record would contain only a subset of the columns. In such
a case, it's helpful to have a library that can output the log files. This is
available. Basic usage looks like this:

In a shell:

#+BEGIN_EXAMPLE
$ vnl-gen-header 'int w' 'uint8_t x' 'char* y' 'double z' 'void* binary' > vnlog_fields_generated.h
#+END_EXAMPLE

In a C program test.c:

#+BEGIN_SRC C
#include "vnlog_fields_generated.h"

int main()
{
    vnlog_emit_legend();

    vnlog_set_field_value__w(-10);
    vnlog_set_field_value__x(40);
    vnlog_set_field_value__y("asdf");
    vnlog_emit_record();

    vnlog_set_field_value__z(0.3);
    vnlog_set_field_value__x(50);
    vnlog_set_field_value__w(-20);
    vnlog_set_field_value__binary("\x01\x02\x03", 3);
    vnlog_emit_record();

    vnlog_set_field_value__w(-30);
    vnlog_set_field_value__x(10);
    vnlog_set_field_value__y("whoa");
    vnlog_set_field_value__z(0.5);
    vnlog_emit_record();

    return 0;
}
#+END_SRC

Then we build and run, and we get

#+BEGIN_EXAMPLE
$ cc -o test test.c -lvnlog

$ ./test

# w x y z binary
-10 40 asdf - -
-20 50 - 0.2999999999999999889 AQID
-30 10 whoa 0.5 -
#+END_EXAMPLE

The binary field in base64-encoded. This is a rarely-used feature, but sometimes
you really need to log binary data for later processing, and this makes it
possible.

So you

1. Generate the header to define your columns

2. Call =vnlog_emit_legend()=

3. Call =vnlog_set_field_value__...()= for each field you want to set in that
   row.

4. Call =vnlog_emit_record()= to write the row and to reset all fields for the
   next row. Any fields unset with a =vnlog_set_field_value__...()= call are
   written as null: =-=

This is enough for 99% of the use cases. Things get a bit more complex if we
have have threading or if we have multiple vnlog ouput streams in the same
program. For both of these we use vnlog /contexts/.

To support reentrant writing into the same vnlog by multiple threads, each
log-writer should create a context, and use it when talking to vnlog. The
context functions will make sure that the fields in each context are independent
and that the output records won't clobber each other:

#+BEGIN_SRC C
void child_writer( // the parent context also writes to this vnlog. Pass NULL to
                   // use the global one
                   struct vnlog_context_t* ctx_parent )
{
    struct vnlog_context_t ctx;
    vnlog_init_child_ctx(&ctx, ctx_parent);

    while(records)
    {
        vnlog_set_field_value_ctx__xxx(&ctx, ...);
        vnlog_set_field_value_ctx__yyy(&ctx, ...);
        vnlog_set_field_value_ctx__zzz(&ctx, ...);
        vnlog_emit_record_ctx(&ctx);
    }
}
#+END_SRC

If we want to have multiple independent vnlog writers to /different/ streams
(with different columns andlegends), we do this instead:

=file1.c=:
#+BEGIN_SRC C
#include "vnlog_fields_generated1.h"

void f(void)
{
    // Write some data out to the default context and default output (STDOUT)
    vnlog_emit_legend();
    ...
    vnlog_set_field_value__xxx(...);
    vnlog_set_field_value__yyy(...);
    ...
    vnlog_emit_record();
}
#+END_SRC

=file2.c=:
#+BEGIN_SRC C
#include "vnlog_fields_generated2.h"

void g(void)
{
    // Make a new session context, send output to a different file, write
    // out legend, and send out the data
    struct vnlog_context_t ctx;
    vnlog_init_session_ctx(&ctx);
    FILE* fp = fopen(...);
    vnlog_set_output_FILE(&ctx, fp);
    vnlog_emit_legend_ctx(&ctx);
    ...
    vnlog_set_field_value__a(...);
    vnlog_set_field_value__b(...);
    ...
    vnlog_emit_record();
}
#+END_SRC

Note that it's the user's responsibility to make sure the new sessions go to a
different =FILE= by invoking =vnlog_set_output_FILE()=. Furthermore, note that
the included =vnlog_fields_....h= file defines the fields we're writing to; and
if we have multiple different vnlog field definitions in the same program (as in
this example), then the different writers /must/ live in different source files.
The compiler will barf if you try to =#include= two different
=vnlog_fields_....h= files in the same source.

More APIs are


=vnlog_printf(...)= and =vnlog_printf_ctx(ctx, ...)= write to a pipe like
=printf()= does. This exists for comments.

=vnlog_clear_fields_ctx(ctx, do_free_binary)=:
Clears out the data in a context and makes it ready to be used for the next
record. It is rare for the user to have to call this manually. The most common
case is handled automatically (clearing out a context after emitting a record).
One area where this is useful is when making a copy of a context:

#+BEGIN_SRC C
struct vnlog_context_t ctx1;
// .... do stuff with ctx1 ... add data to it ...

struct vnlog_context_t ctx2 = ctx1;
// ctx1 and ctx2 now both have the same data, and the same pointers to
// binary data. I need to get rid of the pointer references in ctx1

vnlog_clear_fields_ctx(&ctx1, false);
#+END_SRC

=vnlog_free_ctx(ctx)=:

Frees memory for an vnlog context. Do this before throwing the context away.
Currently this is only needed for context that have binary fields, but this
should be called in for all contexts, just in case

*** numpy interface

The built-in =numpy.loadtxt= =numpy.savetxt= functions work well to read and
write these files. For example to write to standard output a vnlog with fields
=a=, =b= and =c=:

#+BEGIN_SRC python
numpy.savetxt(sys.stdout, array, fmt="%g", header="a b c")
#+END_SRC

Note that numpy automatically adds the =#= to the header. To read a vnlog from a
file on disk, do something like

#+BEGIN_SRC python
array = numpy.loadtxt('data.vnl')
#+END_SRC

These functions know that =#= lines are comments, but don't interpret anything
as field headers. That's easy to do, so I'm not providing any helper libraries.
I might do that at some point, but in the meantime, patches are welcome.

*** Caveats and bugs

The tools that wrap GNU coreutils (=vnl-sort=, =vnl-join=, =vnl-tail=) are
written specifically to work with the Linux kernel and the GNU coreutils. None
of these have been tested with BSD tools or with non-Linux kernels, and I'm sure
things don't just work. It's probably not too effortful to get that running, but
somebody needs to at least bug me for that. Or better yet, send me nice
patches :)

These tools are meant to be simple, so some things are hard requirements. A big
one is that columns are whitespace-separated. There is /no/ mechanism for
escaping or quoting whitespace into a single field. I think supporting something
like that is more trouble than it's worth.


*** Repository

https://github.com/dkogan/vnlog/

*** Author

Dima Kogan (=dima@secretsauce.net=)

*** License and copyright

This library is free software; you can redistribute it and/or modify it under
the terms of the GNU Lesser General Public License as published by the Free
Software Foundation; either version 2.1 of the License, or (at your option) any
later version.

Copyright 2016-2017 California Institute of Technology

Copyright 2017-2018 Dima Kogan (=dima@secretsauce.net=)

=b64_cencode.c= comes from =cencode.c= in the =libb64= project. It is written by
Chris Venter (=chris.venter@gmail.com=) who placed it in the public domain. The
full text of the license is in that file.

** DONE Vnlog integration with feedgnuplot                 :tools:data:vnlog:
   CLOSED: [2018-02-24 Sat 17:24]

This is mostly a continuation of the last post, but it's /so nice/!

As [[http://www.github.com/dkogan/feedgnuplot][=feedgnuplot=]] reads data, it interprets it into separate datasets with IDs
that can be used to refer to these datasets. For instance you can pass
=feedgnuplot --autolegend= to create a legend for each dataset, labelling each
with its ID. Or you can set specific directives for one dataset but not another:
=feedgnuplot --style position 'with lines' --y2 temperature= would plot the
=position= data with lines, and the =temperature= data on the second y axis.

Let's say we were plotting a data stream

#+BEGIN_EXAMPLE
1 1
2 4
3 9
4 16
5 25
#+END_EXAMPLE


Without =--domain= this data would be interpreted like this:

- without =--dataid=. This stream would be interpreted as two data sets: IDs =0=
  and =1=. There're 5 points in each one

- /with/ =--dataid=. This stream would be interpreted as 5 different datasets
  with IDs =1=, =2=, =3=, =4= and =5=. Each of these datasets would contain
  point point each.

This is a silly example for =--dataid=, obviously. You'd instead have a dataset
like

#+BEGIN_EXAMPLE
temperature 34 position 4
temperature 35 position 5
temperature 36 position 6
temperature 37 position 7
#+END_EXAMPLE

and this would mean two datasets: =temperature= and =position=. This is nicely
flexible because it can be as sparse as you want: each row doesn't need to have
one temperature and one position, although in many datasets you would have
exactly this. Real datasets are often more complex:

#+BEGIN_EXAMPLE
1 temperature1 34 temperature2 35 position 4
2 temperature1 35 temperature2 36
3 temperature1 36 temperature2 33
4 temperature1 37 temperature2 32 position 7
#+END_EXAMPLE

Here the first column could be a domain of sort sort, time for instance. And we
have two different temperature sensors. And we don't always get a position
report for whatever reason. This works fine, but is verbose, and usually the
data is never stored in this way; I'd use =awk= to convert the data from its
original form into this form for plotting. Now that [[http://www.github.com/dkogan/vnlog][=vnlog=]] is a thing,
=feedgnuplot= has direct support for it, and this works like a 3rd way to get
dataset IDs: vnlog headers. I'd represent the above like this:

#+BEGIN_EXAMPLE
# time temperature1 temperature2 position
1 34 35 4
2 35 36 -
3 36 33 -
4 37 32 7
#+END_EXAMPLE

This would be the /working/ representation; I'd log directly to this format, and
work with this data even before plotting it. But I can now plot this directly:

#+BEGIN_EXAMPLE
$ < data.vnl 
  feedgnuplot --domain --vnlog --autolegend --with lines 
              --style position 'with points pt 7' --y2 position
#+END_EXAMPLE

I think the command above makes it clear what was intended. It looks like this:

#+ATTR_HTML: :width 90%
[[file:files/feedgnuplot-vnlog/vnl1.svg]]

The input data is now much more concise, I don't need a different format for
plotting, and the amount of typing has been greatly reduced. /And/ I can do the
normal vnlog things. What if I want to plot only temperatures:

#+BEGIN_EXAMPLE
$ < data.vnl 
  vnl-filter -p time,temp |
  feedgnuplot --domain --vnlog --autolegend --with lines 
#+END_EXAMPLE

Nice!

** DONE More Vnlog demos                             :tools:data:vnlog:vnlog:
   CLOSED: [2018-05-21 Mon 21:43]

More demos of [[https://github.com/dkogan/vnlog][=vnlog=]] and [[https://github.com/dkogan/feedgnuplot][=feedgnuplot=]] usage! This is pretty pointless, but
should be a decent demo of the tools at least. This is a demo, not
documentation; so for usage details consult the normal docs.

Each Wednesday night I join a group [[http://www.thepassageride.com][bike ride]]. This is an organized affair, and
each week an email precedes the ride, very roughly describing the route. The two
organizers alternate leading the ride each week, and consequently the emails
alternate also. I was getting the feeling that some of the announcements show up
in my mailbxo more punctually than others, and after a recent
20-minutes-before-the ride email, I decided this just had to be quantified.

The emails all go to a google-group email. The google-groups people are a
wheel-reinventing bunch, so talking to the archive can't be done with normal
tools (NNTP? mbox files? No?). A brief search revealed somebody's home-grown
tool to programmatically grab the archive:

https://github.com/icy/google-group-crawler.git

The docs look funny, but are actually correct: you really /do/ run the script to
download stuff and generate another script; and then run /that/ script to
download the rest of the stuff.

Anyway, I used that tool to grab all the emails that are available. Then I wrote
a quick/dirty script to parse out the data I care about and dump everything into
a vnlog:

#+BEGIN_SRC perl
#!/usr/bin/perl
use strict;
use warnings;

use feature ':5.10';

my %daysofweek = ('Mon' => 0,
                  'Tue' => 1,
                  'Wed' => 2,
                  'Thu' => 3,
                  'Fri' => 4,
                  'Sat' => 5,
                  'Sun' => 6);
my %months = ('Jan' => 1,
              'Feb' => 2,
              'Mar' => 3,
              'Apr' => 4,
              'May' => 5,
              'Jun' => 6,
              'Jul' => 7,
              'Aug' => 8,
              'Sep' => 9,
              'Oct' => 10,
              'Nov' => 11,
              'Dec' => 12);


say '# path ridenum who whenwedh date wordcount subject';

for my $path (<mbox/m.*>)
{
    my ($ridenum,$who,$date,$whenwedh,$subject);

    my $wordcount = 0;
    my $inbody    = undef;

    open FD, '<', $path;
    while(<FD>)
    {
        if( !$inbody && /^From: *(.*?)\s*$/ )
        {
            $who = $1;
            if(   $who =~ /sean/i)   { $who = 'sean'; }
            elsif($who =~ /nathan/i) { $who = 'nathan'; }
            else                     { $who = 'other'; }
        }
        if( !$inbody &&
            /^Subject: \s*
             (?:=\?UTF-8\?Q\?)?
             (.*?) \s* $/x )
        {
            $subject = $1;
            ($ridenum) = $subject =~ /^(?: \# | (?:=\?ISO-8859-1\?Q\?=23) )
                                      ([0-9]+)/x;
            $subject =~ s/[\s#]//g;
        }
        if( !$inbody && /^Date: *(.*?)\s*$/ )
        {
            $date = $1;

            my ($zone) = $date =~ / (\(.+\) | -0700 | -0800) /x;
            if( !defined $zone)
            {
                die "No timezone in: '$date'";
            }
            if( $zone !~ /PST|PDT|-0700|-0800/)
            {
                die "Unexpected timezone: '$zone'";
            }

            my ($Dayofweek,$D,$M,$Y,$h,$m,$s) = $date =~ /^(...),? +(\d+) +([a-zA-Z]+) +(20\d\d) +(\d\d):(\d\d):(\d\d)/;
            if( !(defined $Dayofweek && defined $h && defined $m && defined $s) )
            {
                die "Unparseable date '$date'";
            }
            my $dayofweek = $daysofweek{$Dayofweek} // die "Unparseable day-of-week '$Dayofweek'";

            my $t     = $dayofweek*24 + $h + ($m + $s/60)/60;
            my $twed0 = 2*24; # start of wed
            $M = $months{$M} // die "Unknown month '$M'. Line: '$_'";
            $date = sprintf('%04d%02d%02d', $Y,$M,$D);

            $whenwedh = $t - $twed0;
        }

        if( !$inbody && /^[\r\n]*$/ )
        {
            $inbody = 1;
        }
        if( $inbody )
        {
            if( /------=_Part/ || /Content-Type:/)
            {
                last if $wordcount > 0;
                $inbody = undef;
                next;
            }
            my @words = /(\w+)/g;
            $wordcount += @words;
        }
    }
    close FD;

    $who      //= '-';
    $subject  //= '-';
    $ridenum  //= '-';
    $date     //= '-';
    $whenwedh //= '-';

    say "$path $ridenum $who $whenwedh $date $wordcount $subject";
}
#+END_SRC

The script isn't important, and the resulting data is [[file:files/vnlog-passage/rides.vnl][here]]. Now that I have a
log on disk, I can do stuff with it. The first few lines of the log look like
this:

#+BEGIN_EXAMPLE
dima@scrawny:~/projects/passagemining/google-group-crawler/the-passage-announcements$ < rides.vnl head

# path ridenum who whenwedh date wordcount subject
mbox/m.-EF1u5bbw5A.SywitKQ3y1sJ 265 sean 1.40722222222222 20140903 190 265-Coasting
mbox/m.-JdiiTIvyYs.Jgy_rCiwAGAJ 151 sean 18.6441666666667 20120606 199 151-FinalsWeek
mbox/m.-l6z9-1WC78.SgP3ytLsDAAJ 312 nathan 19.5394444444444 20150812 189 312-SpaceFilling
mbox/m.-vfVuoUxJ0w.FwpRRWC7EgAJ 367 nathan 18.1766666666667 20160831 164 367-Dislocation
mbox/m.-YHTEvmbIyU.HHWjbs_xpesJ 110 sean 10.9108333333333 20110810 407 110-SouslesParcs,laPoubelle
mbox/m.0__GMaUD_O8.Pjupq0AwBAAJ 404 sean 13.5255555555556 20170524 560 404-Bumped
mbox/m.0CT9ybx3uIU.sdZGwo8rSQUJ 53 sean -23.1402777777778 20100629 223 53WeInventedtheRemix
mbox/m.0FtQxCkxVHA.AjhGJ7mgAwAJ 413 nathan 20.4155555555556 20170726 178 413-GradientAssent
mbox/m.0haCNC_N2fY.bJ-93LQSFQAJ 337 nathan 57.3708333333333 20160205 479 337-TheCronutRide
#+END_EXAMPLE

I can align the columns to make it more human-readable:

#+BEGIN_EXAMPLE
dima@scrawny:~/projects/passagemining/google-group-crawler/the-passage-announcements$ < rides.vnl head | vnl-align

#             path              ridenum   who       whenwedh        date   wordcount           subject          
mbox/m.-EF1u5bbw5A.SywitKQ3y1sJ 265     sean     1.40722222222222 20140903 190       265-Coasting               
mbox/m.-JdiiTIvyYs.Jgy_rCiwAGAJ 151     sean    18.6441666666667  20120606 199       151-FinalsWeek             
mbox/m.-l6z9-1WC78.SgP3ytLsDAAJ 312     nathan  19.5394444444444  20150812 189       312-SpaceFilling           
mbox/m.-vfVuoUxJ0w.FwpRRWC7EgAJ 367     nathan  18.1766666666667  20160831 164       367-Dislocation            
mbox/m.-YHTEvmbIyU.HHWjbs_xpesJ 110     sean    10.9108333333333  20110810 407       110-SouslesParcs,laPoubelle
mbox/m.0__GMaUD_O8.Pjupq0AwBAAJ 404     sean    13.5255555555556  20170524 560       404-Bumped                 
mbox/m.0CT9ybx3uIU.sdZGwo8rSQUJ  53     sean   -23.1402777777778  20100629 223       53WeInventedtheRemix       
mbox/m.0FtQxCkxVHA.AjhGJ7mgAwAJ 413     nathan  20.4155555555556  20170726 178       413-GradientAssent         
mbox/m.0haCNC_N2fY.bJ-93LQSFQAJ 337     nathan  57.3708333333333  20160205 479       337-TheCronutRide          
dima@scrawny:~/projects/passagemining/google-group-crawler/the-passage-announcements$ 
#+END_EXAMPLE

If memory serves, we're at around ride 450 right now. Is that right?

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-sort -nr -k ridenum | head -n2 | vnl-filter -p ridenum

# ridenum
452
#+END_EXAMPLE

Cool. This command was longer than it needed to be in order to produce nicer
output. If I was exploring the dataset, I'd save keystrokes and do this instead:

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-sort -nrk ridenum | head

# path ridenum who whenwedh date wordcount subject
mbox/m.7TnUbcShAz8.67KgwBGhAAAJ 452 nathan 20.7694444444444 20180502 175 452-CastingtoType
mbox/m.ej7Oz6sDzgc.bEnN04VEAQAJ 451 sean 0.780833333333334 20180425 258 451-Recovery
mbox/m.LWfydBtpd_s.35SgEJEqAgAJ 450 nathan 67.9608333333333 20180420 659 450-AnotherGreenWorld
mbox/m.3mv-Cm0EzkM.oAm3MkNYCAAJ 449 sean 17.5875 20180411 290 449-DoYouHaveRockNRoll?
mbox/m.AEV4ukSjO5U.IPlUabfEBgAJ 448 nathan 20.6138888888889 20180404 175 448-TheThirdString
mbox/m.bYTM6kgxtJs.5iHcVQKPBAAJ 447 sean 15.8355555555556 20180328 196 447-PassParticiple
mbox/m.tHMqRWp9o_Y.FQ8hFvnqCQAJ 446 nathan 20.5213888888889 20180321 139 446-Chiaroscuro
mbox/m.jr0SBsDBzgk.UHrbCv4VBQAJ 445 sean 15.3280555555556 20180314 111 445-85%
mbox/m.K2Yg_FRXuAo.SyViTwXXAQAJ 444 nathan 19.6180555555556 20180307 171 444-BackintheLoop
#+END_EXAMPLE

OK, how far back does the archive go? I do the same thing as before, but sort in
the opposite order to find the ealiest rides

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-sort -n -k ridenum | head -n2 | vnl-filter -p ridenum

# ridenum
#+END_EXAMPLE

Nothing. That's odd. Let me look at whole records, and at more than just the
first two lines

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-sort -n -k ridenum | head | vnl-align

#             path              ridenum   who       whenwedh       date   wordcount                       subject                      
mbox/m.2gywN9pxMI4.40UBrDjnAwAJ -       nathan  17.6572222222222 20171206  95       Noridetonight;daytimeridethisSaturday!             
mbox/m.49fZsvZac_U.a0CazPinCAAJ -       sean   -34.495           20170320 463       Extraridethisweekend+Passage400save-the-date       
mbox/m.5gJd21W24vo.ICDEHrnQJvcJ -       nathan  12.1063888888889 20130619 172       NoPassageRideTonight;GalleryOpeningTomorrowNight   
mbox/m.7qEbhBWSN1U.Cx6cxYTECgAJ -       nathan  17.7891666666667 20180418 134       Noridetonight;Passage450onSaturday!                
mbox/m.DVssP4Th__4.jXzzu9clZLQJ -       sean    20.9138888888889 20101222 209       TheWrathofTlaloc                                   
mbox/m.E6etBSqEQIc.C35-SkBllHoJ -       sean    50.7575          20131220 292       Noridenextweek;seeyounextyear                      
mbox/m.GyJ16HiK8Ds.z6yNC4W5SeUJ -       sean   -11.5666666666667 20120529 228       NoRideThisWeek!...AIDS/Lifecycle...ThirdAnniversary
mbox/m.H3QGBvjeTfM.CS-xRn1WDQAJ -       sean    17.0180555555555 20171227 257       Noridetonight;nextride1/6                          
mbox/m.K2P6D_BGfYU.ve6a_8l6AAAJ -       sean    37.8166666666667 20170223 150       RemainingPassageRouteMapShirtsAvailableforPurchase 
#+END_EXAMPLE

Aha. A bunch of emails aren't announncing a ride, but are announcing that
there's /no/ ride that week. Let's ignore those

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-filter -p +ridenum | vnl-sort -n -k ridenum | head -n2

# ridenum
52
#+END_EXAMPLE

Bam. So we have emails going back to ride 52. Good enough. All right. I'm aiming
to create a time histogram for Sean's emails and another for Nathan's emails.
What about emails that came from neither one? In theory there shouldn't be any
of those, but there could be a parsing error, or who knows what.

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-filter 'who == "other"'

# path ridenum who whenwedh date wordcount subject
mbox/m.A-I0_i9-YOs.QRX1P99_uiUJ 65 other 65.1413888888889 20100917 330 65-LosAngelesRidesItself+specialscreening
mbox/m.pHpzsjH7H68.O7CP_v6bcEoJ 67 other 16.5663888888889 20101006 50 67Sortition,NotSaturation
#+END_EXAMPLE

OK. Exactly 2 emails out of hundreds. That's not bad, and I'll just ignore
those. Out of curiosity, what happened? Is this a parsing error?

#+BEGIN_EXAMPLE
$ grep From: $(< rides.vnl vnl-filter 'who == "other"' --eval '{print path}')

mbox/m.A-I0_i9-YOs.QRX1P99_uiUJ:From: The Passage Announcements <the-passage-...@googlegroups.com>
mbox/m.pHpzsjH7H68.O7CP_v6bcEoJ:From: The Passage Announcements <the-passage-...@googlegroups.com>
#+END_EXAMPLE

So on rides 65 and 67 "The Passage Announcements" emailed themselves. Oops.
Since the ride leaders alternate, I can infer who actually sent these by looking
at the few rides around this one:

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-filter 'ridenum > 60 && ridenum < 70' -p ridenum,who | vnl-sort -n -k ridenum

# ridenum who
61 sean
62 nathan
63 sean
64 nathan
65 other
66 nathan
67 other
68 nathan
69 sean
#+END_EXAMPLE

That's pretty conclusive: clearly these emails came from Sean. I'm still going
to ignore them, though.

The ride is on Wed evening, and the emails generally come in the day or two
before then. Does my data set contain any data outside this reasonable range?
Hopefully very little, just like the "other" author emails.

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-filter --has ridenum -p whenwedh | feedgnuplot --histo 0 --binwidth 1 --xlabel 'Hour (on Wed)' --ylabel 'Email frequency'
#+END_EXAMPLE

#+ATTR_HTML: :width 90%
[[file:files/vnlog-passage/frequency-all.svg]]

The ride starts at 21:00 on Wed, and we see a nice spike immediately before. The
smaller cluster prior to that is the emails that go out the night before.
There's a tiny number of stragglers going out the previous day (that I'm simply
going to ignore). And there're a number of emails going out /after/ Wed. These
likely announce an occasional weekend ride that I will also ignore. But let's do
check. How many are there?

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-filter --has ridenum 'whenwedh > 22' | wc -l

16
#+END_EXAMPLE

Looking at these manually, most are indeed weekend rides, with a small number of
actual extra-early announcements for Wed. I can parse the email text more
fancily to pull those out, but that's really not worth my time.

OK. I'm now ready for the main thing.

#+BEGIN_EXAMPLE
$ < rides.vnl | 
    vnl-filter --has ridenum 'who != "other"' -p who,whenwedh |
    feedgnuplot --dataid --autolegend
                --histo sean,nathan --binwidth 0.5
                --style sean   'with boxes fill transparent solid 0.3 border lt -1'
                --style nathan 'with boxes fill transparent pattern 1 border lt -1'
                --xmin -12 --xmax 24
                --xlabel "Time (hour)" --ylabel 'Email frequency'
                --set 'xtics ("12\n(Tue)" -12,"16\n(Tue)" -8,"20\n(Tue)" -4,"0\n(Wed)" 0,"4\n(Wed)" 4,"8\n(Wed)" 8,"12\n(Wed)" 12,"16\n(Wed)" 16,"21\n(Wed)" 21,"0\n(Thu)" 24)'
                --set 'arrow from 21, graph 0 to 21, graph 1 nohead lw 3 lc "red"'
                --title "Passage email timing distribution"
#+END_EXAMPLE

#+ATTR_HTML: :width 90%
[[file:files/vnlog-passage/frequency-zoomed.svg]]

This looks verbose, but most of the plotting command is there to make things
look nice. When analyzing stuff, I'd omit most of that. Anyway, I can now see
what I suspected: Nathan is a procrastinator! His emails almost always come in
on Wed, usually an hour or two before the deadline. Sean's emails are bimodal:
one set comes in on Wed afternoon, and another in the extreme early morning on
Wed. Presumably he sleeps in-between.

We have more data, so we can make more pointless plots. For instance, what does
the verbosity of the emails look like? Is one sender more verbose than another?

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-sort -n -k ridenum |
  vnl-filter 'who != "other"' -p +ridenum,who,wordcount |
  feedgnuplot --lines --domain --dataid --autolegend
              --xlabel 'Ride number' --ylabel 'Words per email'
#+END_EXAMPLE

#+ATTR_HTML: :width 90%
[[file:files/vnlog-passage/verbosity_unfiltered.svg]]

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-filter 'who != "other"' --has ridenum -p who,wordcount |
  feedgnuplot --dataid --autolegend
              --histo sean,nathan --binwidth 20
              --style sean   'with boxes fill transparent solid 0.3 border lt -1'
              --style nathan 'with boxes fill transparent pattern 1 border lt -1'
              --xlabel "Words per email" --ylabel 'frequency'
              --title "Passage verbosity distribution"
#+END_EXAMPLE

#+ATTR_HTML: :width 90%
[[file:files/vnlog-passage/verbosity_histogram.svg]]

The time series doesn't obviously say anything, but from the histogram, it looks
like Sean is a bit more verbose, maybe? What's the average?

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-filter --eval 'ridenum != "-" { if(who == "sean")   { Ns++; Ws+=wordcount; }
                                                  if(who == "nathan") { Nn++; Wn+=wordcount; } }
                                 END { print "Mean verbosity sean,nathan: "Ws/Ns, Wn/Nn }'

Mean verbosity sean,nathan: 304.955 250.425
#+END_EXAMPLE

Indeed. Is the verbosity time-dependent? Is anybody getting more or less verbose
over the years? The time-series plot above is pretty noisy, so it's not clear.
Let's filter it to reduce the noise. We're getting into an area that's too
complicated for these tools, and moving to something more substantial at this
point would be warranted. But I'll do one more thing with /these/ tools, and
then stop. I can implement a half-assed filter by time-shifting the verbosity
series, re-joining the shifted series, and computing the mean. I do this
separately for the two email authors, and then re-combine the series. I could
join these two, but simply catting the two data sets together is sufficient here.

#+BEGIN_EXAMPLE
$ < rides.vnl vnl-sort -n -k ridenum |
    vnl-filter 'who == "nathan"' --has ridenum |
    vnl-filter -p ridenum,idx=NR,wordcount > nathanrp0

$ < rides.vnl vnl-sort -n -k ridenum |
    vnl-filter 'who == "nathan"' --has ridenum |
    vnl-filter -p ridenum,idx=NR-1,wordcount > nathanrp-1

$ < rides.vnl vnl-sort -n -k ridenum |
    vnl-filter 'who == "nathan"' --has ridenum |
    vnl-filter -p ridenum,idx=NR+1,wordcount > nathanrp+1

$ ... same for Sean ...

$ cat <(vnl-join --vnl-suffix2 after --vnl-sort n -j idx
                 <(vnl-join --vnl-suffix2 before --vnl-sort n -j idx
                            nathanrp{0,-1})
                 nathanrp+1 |
        vnl-filter -p ridenum,who='"nathan"','wordcountfiltered=(wordcount+wordcountbefore+wordcountafter)/3')

      <(vnl-join --vnl-suffix2 after --vnl-sort n -j idx
                 <(vnl-join --vnl-suffix2 before --vnl-sort n -j idx
                            seanrp{0,-1})
                 seanrp+1 |
        vnl-filter -p ridenum,who='"sean"','wordcountfiltered=(wordcount+wordcountbefore+wordcountafter)/3') |
  feedgnuplot --lines --domain --dataid --autolegend
              --xlabel 'Ride number' --ylabel 'Words per email'

#+END_EXAMPLE

#+ATTR_HTML: :width 90%
[[file:files/vnlog-passage/verbosity_filtered.svg]]

Whew. Clearly this was doable, but that's a one-liner that has clearly gotten
out of hand, and pushing it further would be unwise. Looking at the data there
isn't any obvious time dependence. But what you /can/ clearly see is the extra
verbiage around the round-number rides 100, 200, 300, 350, 400, etc. These were
often a special weekend ride, with the email containing lots of extra
instructions and such.

This was all clearly a waste of time, but as a demo of vnlog workflows, this was
ok.

** DONE UNIX curiosities                                              :tools:
   CLOSED: [2018-08-03 Fri 22:04]

Recently I've been doing more UNIXy things in various tools I'm writing, and I
hit two interesting issues. Neither of these are "bugs", but behaviors that I
wasn't expecting.

*** Thread-safe printf

I have a C application that reads some images from disk, does some processing,
and writes output about these images to STDOUT. Pseudocode:

#+BEGIN_SRC c
for(imagefilename in images)
{
    results = process(imagefilename);
    printf(results);
}
#+END_SRC

The processing is independent for each image, so naturally I want to distribute
this processing between various CPUs to speed things up. I usually use =fork()=,
so I wrote this:

#+BEGIN_SRC c
for(child in children)
{
    pipe = create_pipe();
    worker(pipe);
}

// main parent process
for(imagefilename in images)
{
    write(pipe[i_image % N_children], imagefilename)
}

worker()
{
    while(1)
    {
        imagefilename = read(pipe);
        results = process(imagefilename);
        printf(results);
    }
}
#+END_SRC

This is the normal thing: I make pipes for IPC, and send the child workers image
filenames through these pipes. Each worker /could/ write its results back to the
main process via another set of pipes, but that's a pain, so here each worker
writes to the shared STDOUT directly. This works OK, but as one would expect,
the writes to STDOUT clash, so the results for the various images end up
interspersed. That's bad. I didn't feel like setting up my own locks, but
fortunately GNU libc provides facilities for that: [[https://www.gnu.org/software/libc/manual/html_node/Streams-and-Threads.html][=flockfile()=]]. I put those
in, and ... it didn't work! Why? Because whatever =flockfile()= does internally
ends up restricted to a single subprocess because of =fork()='s copy-on-write
behavior. I.e. the extra safety provided by =fork()= (compared to threads)
actually ends up breaking the locks.

I haven't tried using other locking mechanisms (like pthread mutexes for
instance), but I can imagine they'll have similar problems. And I want to keep
things simple, so sending the output back to the parent for output is out of the
question: this creates more work for both me the programmer, and for the
computer running the program.

The solution: use threads instead of forks. This has a nice side effect of
making the pipes redundant. Final pseudocode:

#+BEGIN_SRC c
for(children)
{
    pthread_create(worker, child_index);
}
for(children)
{
    pthread_join(child);
}

worker(child_index)
{
    for(i_image = child_index; i_image < N_images; i_image += N_children)
    {
        results = process(images[i_image]);
        flockfile(stdout);
        printf(results);
        funlockfile(stdout);
    }
}
#+END_SRC

Much simpler, and actually works as desired. I guess sometimes threads are
better.

*** Passing a partly-read file to a child process

For various [[http://www.github.com/dkogan/vnlog][=vnlog=]] tools I needed to implement this sequence:

1. process opens a file with =O_CLOEXEC= turned /off/
2. process reads a /part/ of this file (up-to the end of the legend in the case
   of =vnlog=)
3. process calls =exec= to invoke another program to process the rest of the
   already-opened file

The second program may require a file /name/ on the commandline instead of an
already-opened file descriptor because this second program may be calling
=open()= by itself. If I pass it the filename, this new program will re-open the
file, and then start reading the file from the beginning, /not/ from the
location where the original program left off. It is important for my application
that this does not happen, so passing the filename to the second program does
/not/ work.

So I really need to pass the already-open file descriptor somehow. I'm using
Linux (other OSs maybe behave differently here), so I can in theory do this by
passing =/dev/fd/N= instead of the filename. But it turns out this does not work
either. On Linux (again, maybe this is Linux-specific somehow) for normal files
=/dev/fd/N= is a symlink to the original file. So this ends up doing exactly the
same thing that passing the filename does.

But there's a workaround! If we're reading a /pipe/ instead of a /file/, then
there's nothing to symlink to, and =/dev/fd/N= ends up passing the original pipe
down to the second process, and things then work correctly. And I can fake this
by changing the =open("filename")= above to something like =popen("cat
filename")=. Yuck! Is this really the best we can do? What does this look like
on one of the BSDs, say?

** DONE A nice oneliner                              :tools:data:vnlog:vnlog:
   CLOSED: [2018-09-26 Wed 17:51]

Pop quiz! Let's say I have a datafile describing some items (images and feature
points in this example):

#+BEGIN_EXAMPLE
# filename x y
000.jpg 79.932824 35.609049
000.jpg 95.174662 70.876506
001.jpg 19.655072 52.475315
002.jpg 19.515351 33.077847
002.jpg 3.010392 80.198282
003.jpg 84.183099 57.901647
003.jpg 93.237358 75.984036
004.jpg 99.102619 7.260851
005.jpg 24.738357 80.490116
005.jpg 53.424477 27.815635
....
....
149.jpg 92.258132 99.284486
#+END_EXAMPLE

How do I get a random subset of N images, using only the shell and standard
commandline tools?

Bam!

#+BEGIN_EXAMPLE
$ N=5;
  (  echo '# filename';
     seq 0 149                       |
       shuf                          |
       head -n $N                    |
       xargs -n1 printf "%03d.jpg\n" |
       sort)  |
  vnl-join -j filename input.vnl -

# filename x y
017.jpg 41.752204 96.753914
017.jpg 86.232504 3.936258
027.jpg 41.839110 89.148368
027.jpg 82.772742 27.880592
067.jpg 57.790706 46.153623
067.jpg 87.804939 15.853087
076.jpg 41.447477 42.844849
076.jpg 93.399829 64.552090
142.jpg 18.045497 35.381083
142.jpg 83.037867 17.252172
#+END_EXAMPLE

** DONE Self-plotting output from feedgnuplot and python-gnuplotlib :tools:data:feedgnuplot:gnuplotlib:vnlog:
   CLOSED: [2018-09-29 Sat 11:40]

I just made a small update to [[http://github.com/dkogan/feedgnuplot][=feedgnuplot=]] (version 1.51) and to
[[http://github.com/dkogan/gnuplotlib][=python-gnuplotlib=]] (version 0.23). Demo:

#+BEGIN_EXAMPLE
$ seq 5 | feedgnuplot --hardcopy showplot.gp

$ ./showplot.gp
[plot pops up]

$ cat showplot.gp
#!/usr/bin/gnuplot
set grid
set boxwidth 1
histbin(x) = 1 * floor(0.5 + x/1)
plot '-'   notitle  
1 1
2 2
3 3
4 4
5 5
e
pause mouse close
#+END_EXAMPLE

I.e. there's now support for a fake =gp= terminal that's not a gnuplot terminal
at all, but rather a way to produce a self-executable gnuplot script. 99% of
this was already implemented in =--dump=, but this way to access that
functionality is much nicer. In fact, the machine running =feedgnuplot= doesn't
even need to have =gnuplot= installed at all. I needed this because I was making
complicated plots on a remote box, and X-forwarding was being way too slow. Now
the remote box creates the self-plotting gnuplot scripts, I scp those, evaluate
them locally, and then work with interactive visualizations.

The python frontend =gnuplotlib= has received an analogous update.

** DONE Generating manpages from python and argparse  :tools:python:manpages:
   CLOSED: [2018-10-07 Sun 14:42]

I find the python ecosystem deeply frustrating. On some level, they fixed some
issues in previous languages. But at the same time, they chose to completely
flout long-standing conventions, and have rewritten the world in ways that are
different for no good reason. And when you rewrite the world, you always end up
rewriting only the parts you care about, so your new implementation lacks pieces
that other people find very important.

Today's annoyance: manpages. I have some tools written in python that I'm going
to distribute, and since this isn't intended to be user-hostile, I want to ship
manpages. The documentation already exists in the form of docstrings and
=argparse= option decriptions, so I don't want to /write/ the manpages, but they
should be generated for me. I looked around and I can't find anything anywhere.
There're some hoaky hacks people have come up with to do some of this, but I
don't see any sort of "standard" way to do this at /all/. Even when I reduce the
requirements to almost nothing, I still can't find /anything/. Please tell me if
I missed something.

Anyway, I came up with yet another hoaky hack. It works for me. Sample project:

https://github.com/dkogan/python-argparse-generate-manpages-example

This has a python tool called [[https://github.com/dkogan/python-argparse-generate-manpages-example/blob/master/frobnicate][=frobnicate=]], and a [[https://github.com/dkogan/python-argparse-generate-manpages-example/blob/master/make-pod.pl][script]] to generate its manpage
as a =.pod=. The [[https://github.com/dkogan/python-argparse-generate-manpages-example/blob/master/Makefile][=Makefile=]] has rules to make this =.pod= and to convert it into
a manpage. It works by running =frobnicate --help=, and parsing that output.

The =--help= message is set up to be maximally useful by [[https://github.com/dkogan/python-argparse-generate-manpages-example/blob/master/frobnicate#L21][including the main
docstring into the message]]. This is good not just for the manpages, but to make
an informative =--help=. And this has a nice consequence in that the manpage
generator /only/ needs to look at the =--help= output.

It is assumed that the [[https://github.com/dkogan/python-argparse-generate-manpages-example/blob/master/frobnicate#L3][main docstring]] (and thus the =--help= message) is
formatted like a manpage would be, beginning with a synopsis. This isn't usually
done in python, but it should be; they just like being contrarian for no good
reason.

With those assumptions I can parse the =--help= output, and produce a reasonable
manpage. Converted to html, it looks like [[file:files/python-manpages/python-manpage-example.html][this]]. Not the most exciting thing in
the world, but that's the point.

This could all be cleaned up, and made less brittle. That would be great. In the
meantime, it solves my use case. I'm releasing this into the public domain.
Please use it and hack it up. If you fix it and give the changes back, I
wouldn't complain. And if there're better ways to have done this, somebody
please tell me.

#+begin_o_blog_alert info Update

A few people reached out to me with suggestions of tools they have found and/or
used for this purpose. A survey:

*** argparse-manpage

This lives [[https://pypi.org/project/argparse-manpage/][here]] and [[https://github.com/praiskup/argparse-manpage][here]]. I actually found this thing in my search, but it
didn't work at all, and I didn't want to go into the rabbithole of debugging it.
Well, I debugged it now and know what it needed. Issues

- First off, there're 3 binaries that could be executed:
  - =argparse-manpage=
  - =wrap=
  - =bin/argparse-manpage=
  It appears that only the first one is meant to be functional. The rest throw
  errors that require the user to debug this thing
- To make it more exciting, =bin/argparse-manpage= doesn't work out of the box
  on Debian-based boxes: it begins with
  #+BEGIN_EXAMPLE
  #!/bin/python
  #+END_EXAMPLE
  Which isn't something that exists there. Changing this to
  #+BEGIN_EXAMPLE
  #!/usr/bin/python
  #+END_EXAMPLE
  makes it actually runnable. Once you know that this is the thing that needs
  running, of course.
- All right, now that it can run, we need to figure out more of this. The
  "right" invocation for the example project is this:
  #+BEGIN_EXAMPLE
  $ argparse-manpage/bin/argparse-manpage --pyfile python-argparse-generate-manpages-example/frobnicate  --function parse_args > frobnicator.1
  #+END_EXAMPLE
  Unsurprisingly it doesn't work
- argparse-manpage wants a function that returns an =ArgumentParser= object.
  This went against what the example program did: return the already-parsed
  options.
- Once this is done, it still doesn't work. Apparently it attempts to actually
  run the program, and the program barfs that it wasn't given enough arguments.
  So for manpage-creation purposes I disable the actual option parsing, and make
  the program do nothing
- And it still doesn't work. Apparently the function that parses the arguments
  doesn't see the =argparse= import when argparse-manpage works with it (even
  though it works fine when you just run the program). Moving that import into
  the function makes it work finally. A patch to the test program combining all
  of the workarounds together:

#+BEGIN_SRC diff
diff --git a/frobnicate b/frobnicate
index 89da764..475ac63 100755
--- a/frobnicate
+++ b/frobnicate
@@ -16,6 +16,7 @@ import argparse
 
 
 def parse_args():
+    import argparse
     parser = \
         argparse.ArgumentParser(description = __doc__,
                                 formatter_class=argparse.RawDescriptionHelpFormatter)
@@ -29,8 +30,8 @@ def parse_args():
                         nargs='+',
                         help='''Inputs to process''')
 
-    return parser.parse_args()
+    return parser
 
 
 
-args = parse_args()
+#args = parse_args().parse_args()
#+END_SRC

Now that we actually get output, let's look at it. Converted to html, looks like
[[file:files/python-manpages/argparse-manpage.html][this]]. Some chunks are missing because I didn't pass them on the commandline
(author, email, etc). But it also generated information about the arguments
/only/. I.e. the description in the main docstring is missing even though
=argparse= was given it, and it's reported with =--help=. And the synopsis
section contains the short options instead of an example, like it should.

*** help2man

This is in Debian in the =help2man= package. It works out of the box at least.
Invocation:

#+BEGIN_EXAMPLE
help2man python-argparse-generate-manpages-example/frobnicate --no-discard-stderr > frobnicator.1
#+END_EXAMPLE

In html looks like [[file:files/python-manpages/help2man.html][this]]. Better. At least the description made it in. The usage
message is mistakenly in the =NAME= section, and the =SYNOPSIS= section is
missing with the synopsis ending up in the =DESCRIPTION= section. The tool was
not given enough information to do this correctly. It could potentially do
something POD-like where the user is responsible for actually writing out all
the sections, and =help2man= would just pick out and reformat the option-parsing
stuff. This would be very heuristic, and impossible to do "right".

*** Others

There's apparently some way to do this with sphinx. I generally avoid the
python-specific reimaginings of the world, so I haven't touched those. Besides,
the "python" part is a detail in this project. The sphinx thing is [[https://sphinx-argparse.readthedocs.io/en/stable/][here]]. And
apparently you can invoke it like this:

#+BEGIN_EXAMPLE
python3 -m sphinx -b man ...
#+END_EXAMPLE

*** Thoughts

Not sure what a better solution to all this would be. Maybe some sort of
=docstring2man= tool that works like =pod2man= combined with =docopt= instead of
=argparse=.
#+end_o_blog_alert

** DONE Manpages and READMEs                                 :tools:manpages:
   CLOSED: [2018-10-23 Tue 21:36]

I guess this is a follow-on from the [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Generating manpages from python and argparse")){/lisp}][last post]]. I generally want complete
documentation, and I don't want to document the same thing twice. Thus I want to
include all my manpages in the README. For projects hosted someplace like [[http://github.com][github]]
the README serves as the front page of a project, so the README is the
documentation that will be seen and read the most. I just set this up for yet
another project, and I may as well talk about it here.

Demo case: [[http://github.com/dkogan/mrgingham][mrgingham]]. This is a semi-released component that's the first in a
set of calibration/SFM tools. The details aren't important for this post, but
anybody interested in such things should talk to me.

This project has two user-facing tools that are distributed:

- [[http://github.com/dkogan/mrgingham/blob/master/mrgingham-from-image.cc][=mrgingham=]]: the main tool that ingests images, and spits out chessboard
  corners it found, as a [[http://github.com/dkogan/vnlog][vnlog]]. This is written in C++, and needs to be built.
  The documentation lives in a separate [[https://raw.githubusercontent.com/dkogan/mrgingham/master/mrgingham.pod][POD]] file. This POD isn't attached to the
  =getopt()= option parser; yet.

- [[https://github.com/dkogan/mrgingham/blob/master/mrgingham-observe-pixel-uncertainty][=mrgingham-observe-pixel-uncertainty=]]: a tool to empirically quantify the
  uncertainty of the output. /This/ is written in python, and I make its manpage
  as described in the [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Generating manpages from python and argparse")){/lisp}][last post]]: first I make a POD, then convert the POD to a
  manpage.

The README exists as a [[https://github.com/dkogan/mrgingham/blob/master/README.template.org][template]], with [[https://github.com/dkogan/mrgingham/blob/master/README.template.org#manpages][designated areas]] for the manpages. And all
the action is in the [[https://github.com/dkogan/mrgingham/blob/master/Makefile][=Makefile=]]. Relevant section:

#+BEGIN_SRC makefile

DIST_BIN := mrgingham mrgingham-observe-pixel-uncertainty

# I construct the README.org from the template. The only thing I do is to insert
# the manpages. Note that this is more complicated than it looks:
#
# 1. The documentation lives in a POD
# 2. This documentation is stripped out here with pod2text, and included in the
#    README. This README is an org-mode file, and the README.template.org
#    container included the manpage text inside a #+BEGIN_EXAMPLE/#+END_EXAMPLE.
#    So the manpages are treated as a verbatim, unformatted text blob
# 3. Further down, the same POD is converted to a manpage via pod2man
define MAKE_README =
BEGIN                                                                   \
{                                                                       \
  for $$a (@ARGV)                                                       \
  {                                                                     \
    $$base = $$a =~ s/\.pod$$//r;                                       \
    $$c{$$base} = `pod2text $$a | mawk "/REPOSITORY/{exit} {print}"`;   \
  }                                                                     \
}                                                                       \
                                                                        \
while(<STDIN>)                                                          \
{                                                                       \
  print s/xxx-manpage-(.*?)-xxx/$$c{$$1}/gr;                            \
}
endef

README.org: README.template.org $(DIST_BIN:%=%.pod)
        < $(filter README%,$^) perl -e '$(MAKE_README)' $(filter-out README%,$^) > $@
all: README.org

%.1: %.pod
        pod2man --center="mrgingham: chessboard corner finder" --name=MRGINGHAM --release="mrgingham $(VERSION)" --section=1 $^ $@
mrgingham-observe-pixel-uncertainty.pod: %.pod: %
        ./make-pod.pl $< > $@
        cat footer.pod >> $@
EXTRA_CLEAN += *.1 mrgingham-observe-pixel-uncertainty.pod README.org

#+END_SRC

And that's it. I get a [[https://github.com/dkogan/mrgingham/blob/master/README.org][=README.org=]] that contains the manpages, and never goes
out of date. For github to pick it up I need to commit it to the repo, even
though it's a generated file. Probably can set up some sort of hook to take care
of this too, but it's close-enough.

** DONE Keeping C OpenCV applications building                   :dev:opencv:
   CLOSED: [2018-12-31 Mon 11:44]

The OpenCV people decided to break their C API. Except (at least as of OpenCV
3.x) they didn't really remove or deprecate the API, they just stopped testing
it. So when things start to break, you just get errors for no reason. Case in
point: I have an application that now throws linker error when built without
optimizations:

#+BEGIN_EXAMPLE
$ gcc -O0 application.c -lopencv_core ...

/usr/bin/ld: /tmp/cc8R0Ote.o: in function `cvPointFrom32f':
tst.c:(.text+0x557): undefined reference to `cvRound'
/usr/bin/ld: tst.c:(.text+0x56d): undefined reference to `cvRound'
/usr/bin/ld: /tmp/cc8R0Ote.o: in function `cvReadInt':
tst.c:(.text+0xdb1): undefined reference to `cvRound'
collect2: error: ld returned 1 exit status
#+END_EXAMPLE

Great. I'm not actually calling =cvRound()= anywhere, and looking into the
sources, this breakage was easily avoidable if they actually cared.

A workaround is to define my own =cvRound()=. Something like this works:

#+BEGIN_SRC C
#include <math.h>
static inline int cvRound(float value)
{
    return (int)roundf(value);
}

#include <opencv2/calib3d/calib3d_c.h>
#+END_SRC

This compiles and links with or without optimizations. Defining this before the
OpenCV =#include= is significant, and you need to link with =-lm= to get the
implementation of =roundf()=.

This is with OpenCV 3.2, as shipped by Debian. I can imagine later releases
either un-breaking this, or actually removing the APIs entirely. That would be
the impetus for me to finally dump this library. Which would be a good thing
overall, to be honest.

** DONE Where are all these crashes coming from? 	   :hiking:GIS:vnlog:
   CLOSED: [2019-01-06 Sun 23:56]

Check this out!

[[http://ntsb.secretsauce.net/map/][file:files/wrecks/wrecks.png]]

(click for an interactive map thing).

So let's say you're walking around in the mountains, and BAM! You run into a
crashed airplane (pretend this happens all the time). You wonder "what is this?"
"When did it go down?" "How?" Usually it doesn't happen this way for me (rather
[[https://www.tapatalk.com/groups/sangabrielmnts/before-removing-hubcap-move-center-screws-t7931.html][like this]]), but in any case this is all useful.

It turns out that most (all?) such incidents are investigated by the NTSB. And
they then write a report. And then somebody puts all the reports into a database
and makes them available on the internet. How helpful! For my purposes, I mostly
care about the location each impact site. Some reports actually have a lat/lon
pair, while others have a range/bearing from some "observing" airport. Many
reports don't have either, but there isn't a lot I can do about that.

In any case, I did some typing, and produced the map linked above:

http://ntsb.secretsauce.net/map/

This shows all the incidents after 1982 that have a location listed in an NTSB
report. The locations are represented by a red circle (if a report lists a
latitude/longitude) and/or a range/bearing area (if a report lists a weather
observation and its relative location to the crash site). Clicking an object
produces links to the different versions of the NTSB reports. The data isn't
very accurate it looks like (sometimes not even close to right), but the text of
the reports often has more specific information. I have all the data in the US,
and you can use the interactive map in the above link to look at any of it.

The code, docs, and full final dataset appear [[https://github.com/dkogan/ntsb-wrecks][here]].

And I learned of exciting things looking at this. For instance in 2016 a Boeing
777 cleared Mt. Wilson by 1000 ft, not counting the towers. And [[http://ntsb.secretsauce.net/map/#12/33.0776/-114.3581][this]] happened.

Huge thanks to Erik Price and Chris McKenzie for help in getting the interactive
map working.

** DONE Are planes crashing any less than they used to?		 :data:vnlog:
   CLOSED: [2019-09-07 Sat 16:25]

Recently, I've been spending more of my hiking time looking for old
plane crashes in the mountains. And I've been looking for data that
helps me do that, for instance the [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Where are all these crashes coming from?")){/lisp}][last post]]. A question that came up
in conversation is: "are crashes getting more rare?" And since I now
have several datasets at my disposal, I can very easily come up with a
crude answer.

The [[file:{lisp}(ob:link-to-post (ob:get-post-by-title "Where are all these crashes coming from?")){/lisp}][last post]] describes how to map the available NTSB reports
describing aviation incidents. I was only using the post-1982 reports
in that project, but here let's /also/ look at the older
reports. Today I can download both from their site:

#+BEGIN_EXAMPLE
$ wget https://app.ntsb.gov/avdata/Access/avall.zip
$ unzip avall.zip    # <------- Post 1982

$ wget https://app.ntsb.gov/avdata/PRE1982.zip
$ unzip PRE1982.zip  # <------- Pre 1982
#+END_EXAMPLE

I import the relevant parts of each of these into sqlite:

#+BEGIN_EXAMPLE
$ ( mdb-schema avall.mdb sqlite -T events;
    echo "BEGIN;";
    mdb-export -I sqlite avall.mdb events;
    echo "COMMIT;";
  ) | sqlite3 post1982.sqlite

$ ( mdb-schema PRE1982.MDB sqlite -T tblFirstHalf;
    echo "BEGIN;";
    mdb-export -I sqlite PRE1982.MDB tblFirstHalf;
    echo "COMMIT;";
  ) | sqlite3 pre1982.sqlite
#+END_EXAMPLE

And then I pull out the incident dates, and make a histogram:

#+BEGIN_EXAMPLE
$ cat <(sqlite3 pre1982.sqlite 'select DATE_OCCURRENCE from tblFirstHalf') \
      <(sqlite3 post1982.sqlite 'select ev_date from events') |
  perl -pe 's{^../../(..) .*}{$1 + (($1<40)? 2000: 1900)}e'   |
  feedgnuplot --histo 0 --binwidth 1 --xmin 1960 --xlabel Year \
              --title 'NTSB-reported incident counts by year'
#+END_EXAMPLE

#+ATTR_HTML: :width 90%
[[file:files/wrecks/ntsb-histogram-by-year.svg]]

I guess by that metric everything is getting safer. This clearly just
counts NTSB incidents, and I don't do any filtering by the severity of
the incident (not all reports describe crashes), but close-enough. The
NTSB only deals with civilian incidents in the USA, and only after the
early 1960s, it looks like. Any info about the military?

At one point I went through "Historic Aircraft Wrecks of Los Angeles
County" by G. Pat Macha, and listed all the described incidents in
that book. This histogram of that dataset looks like this:

#+ATTR_HTML: :width 90%
[[file:files/wrecks/macha-la-histogram-by-year.svg]]

Aaand there're a few internet resources that list out significant
incidents in Southern California. For instance:

- http://www.av.qnet.com/~carcomm/a.htm
- http://www.av.qnet.com/~carcomm/b.htm
- http://www.av.qnet.com/~carcomm/c.htm

I visualize /that/ dataset:
#+BEGIN_EXAMPLE
$ < [abc].htm perl -nE '/^ \s* 19(\d\d) | \d\d \s*(?:\s|-|\/)\s* \d\d \s*(?:\s|-|\/)\s* (\d\d)[^\d]/x || next; $y = 1900+($1 or $2); say $y unless $y==1910' |
  feedgnuplot --histo 0 --binwidth 5
#+END_EXAMPLE

#+ATTR_HTML: :width 90%
[[file:files/wrecks/carcomm-by-year.svg]]

So what did we learn? I guess overall crashes are becoming more
rare. And there was a glut of military incidents in the 1940s and
1950s in Southern California (not surprising given all the military
bases and aircraft construction facilities here at that time). And
by one metric there were lots of incidents in the late 1970s/early
1980s, but they were much more interesting to this "carcomm" person,
than they were to Pat Macha.

** DONE python-gnuplotlib knows about multiplots :data:tools:python:gnuplot:gnuplotlib:
   CLOSED: [2019-11-28 Thu 19:30]
*** Synopsis

I just released gnuplotlib 0.32. It can do this:

#+BEGIN_SRC python
import gnuplotlib as gp
import numpy      as np
import numpysane  as nps

th = np.linspace(0, np.pi*2, 30)

gp.plot( th, nps.cat( np.cos(th), np.sin(th)),
         _xrange = [0,2.*np.pi],
         _yrange = [-1,1])
#+END_SRC

#+ATTR_HTML: :width 90%
[[file:files/gnuplotlib/sinusoids-single-plot.svg]]


#+BEGIN_SRC python
import gnuplotlib as gp
import numpy      as np
import numpysane  as nps

th = np.linspace(0, np.pi*2, 30)

gp.plot( (th, np.cos(th), dict(title="cos")),
         (th, np.sin(th), dict(title="sin")),
         _xrange = [0,2.*np.pi],
         _yrange = [-1,1],
         multiplot='title "multiplot sin,cos" layout 2,1')
#+END_SRC

#+ATTR_HTML: :width 90%
[[file:files/gnuplotlib/sinusoids-multi-plot.svg]]

*** Long version

[[https://github.com/dkogan/gnuplotlib/][=gnuplotlib=]] and [[https://github.com/dkogan/numpysane/][=numpysane=]] are becoming mature (i.e. I use them all the time,
have done that for a while, and they work /very/ well), so I'm going to start
doing some proselytizing on this front. I want to do a talk in the near future,
and looking forward to that, I'm going to expand the docs, and I'm implementing
some long-envisioned-but-never-completed features. The first one of these is now
complete: multiplot support for =gnuplotlib=.

[[http://gnuplot.sourceforge.net/demo/layout.html][Gnuplot multiplots]] are a way to create more than one plot in a single window (or
hardcopy). These are a bit of a corner case, and I've been mostly getting by
without ever using these, but sometimes they're really nice. Use cases:

- Plotting several time series (or anything that shares an axis) on top of one
  another. You /could/ just put all your stuff on the same plot, but if you have
  many different datasets, and many different y-axis scales, this becomes messy
  quickly

- Getting around limitations in gnuplot. I recently discovered that gnuplot
  doesn't have a direct way to plot several stacked contours in 3D: more than
  one countour at a time is possible, but they'll all be drawn at the same z
  coordinate. One way to work around this is to use multiplots to plot each
  contour as a separate plot, and to tell gnuplot to put each subplot in the
  same location on the page

Making =gnuplotlib= support this was conceptually simple, but violated some core
assumptions of the library, so lots of typing was required to add this feature.
In the end, it came out nicely, and didn't break any of the previous APIs. The
big update two-fold. First I needed to separate *plot options* into

- *process options*: one per gnuplot process. Applies to the whole big plot.
  Stuff like =terminal= or =hardcopy= or =wait=
- *subplot options*: one per subplot. With multiplots we'll have many of these
  at the same time. These control each plot. Stuff like =xlabel=, =xrange=,
  =3d=, etc, etc

And then all the data passed to =plot()= needed to be wrappable into yet-another
outer list. While with a single subplot you'd do this:

#+BEGIN_SRC python
gp.plot( (x0, y0, curve_options0),
         (x1, y1, curve_options1),
         ...,
         subplot_options, process_options)
#+END_SRC

In multiplot mode, you'd pass a tuple of subplots instead of a tuple of curves,
where each suplot is more or less the whole argument set from each individual
=plot()= command (minus the process options):

#+BEGIN_SRC python
gp.plot( ( (x0, y0, curve_options0),
           (x1, y1, curve_options1),
           ...
           subplot_options0 ),

         ( (x2, y2, curve_options2),
           (x3, y3, curve_options3),
           ...
           subplot_options1 ),
         ...,
         process_options )
#+END_SRC

The [[https://github.com/dkogan/gnuplotlib/][=gnuplotlib= README]] and the [[https://github.com/dkogan/gnuplotlib/blob/master/demo.py][demo]] have some examples. Here're some
stacked contours:

#+BEGIN_SRC python
import gnuplotlib as gp
import numpy      as np

xx,yy = np.meshgrid(np.linspace(-5,5,100),
                    np.linspace(-5,5,100))
zz0 = np.sin(xx) + yy*yy/8.
zz1 = np.sin(xx) + yy*yy/10.
zz2 = np.sin(xx) + yy*yy/12.

commonset = ( 'origin 0,0',
              'size 1,1',
              'view 60,20,1,1',
              'xrange [0:100]',
              'yrange [0:100]',
              'zrange [0:150]',
              'contour base' )

gp.plot3d( (zz0, dict(_set = commonset + ('xyplane at 10',))),
           (zz1, dict(_set = commonset + ('xyplane at 80',  'border 15'), unset=('ztics',))),
           (zz2, dict(_set = commonset + ('xyplane at 150', 'border 15'), unset=('ztics',))),

           tuplesize=3,
           _with = np.array(('lines nosurface',
                             'labels boxed nosurface')),
           square=1,
           multiplot=True)
#+END_SRC

#+ATTR_HTML: :width 90%
[[file:files/gnuplotlib/stacked-contours.svg]]

This is pretty new, so if you try it out, and something is unclear or broken,
please complain.

** DONE tee is broken?                                                :tools:
   CLOSED: [2019-12-03 Tue 22:31]

Just found a highly surprising behavior in a core tool I've used for decades, so
clearly I'm making a note here. None of these are surprising:

#+BEGIN_EXAMPLE
$ seq 1000 | wc -l

1000


$ seq 1000 | tee /dev/null | wc -l

1000


$ seq 1000 | tee >( true ) | wc -l

1000


$ seq 1000 > >( true ) | wc -l

1000
#+END_EXAMPLE

I.e. I can write 1000 lines into =tee=, do stuff in one of the children, and the
other child get my 1000 lines still. The last one uses [[http://zsh.sourceforge.net/Doc/Release/Redirection.html#Multios][multios in zsh]] for the
tee. But check out what happens when I bump up the data size:

#+BEGIN_EXAMPLE
$ seq 100000 | wc -l

100000


$ seq 100000 | tee /dev/null | wc -l

100000


$ seq 100000 | tee >( true ) | wc -l

14139


$ seq 100000 > >( true ) | wc -l

1039
#+END_EXAMPLE

Whoa. What the hell? When I stumbled on this I had another, unrelated problem
breaking things in this area, which made for a long debugging session. Here're
some runs that give a hint of what's going on:

#+BEGIN_EXAMPLE
$ seq 100000 | tee >( true ) | wc -c

73728


$ seq 100000 > >( true ) | wc -c

4092


$ seq 100000 | tee >( cat > /dev/null ) | wc -l

100000
#+END_EXAMPLE

Figure it out?

Answer time! After a =tee=, a single writer parent feeds two reader children. If
a child exits before reading /all/ the data, then when the parent tries to feed
that dead child, the parent will get a =SIGPIPE=. And apparently the default
behavior of =tee= in GNU coreutils (and in the zsh multios redirection) is to
give up and to stop feeding /all/ the children at that point. So the second
child (=wc -l= in the examples) ends up with incomplete input. No errors are
thrown anywhere, and there's no indication at all that any data was truncated.
Lots of the data is just silently missing.

The GNU coreutils implementation of =tee= has an innocuous-looking option:

#+BEGIN_EXAMPLE
-p     diagnose errors writing to non pipes
#+END_EXAMPLE

I read the manpage several times, and it's /still/ not obvious to me that =-p=
does anything more than change something about diagnostic printing. But it does:
=tee -p= feeds all the children as much as it can until they're all dead (i.e.
what everybody was assuming it was doing the whole time):

#+BEGIN_EXAMPLE
$ seq 100000 | tee -p >( true ) | wc -l

100000
#+END_EXAMPLE

There's also =pee=, specific tee-to-process utility in the Debian =moreutils=
package. This utility can be used here, and /it/ does the reasonable thing by
default:

#+BEGIN_EXAMPLE
$ seq 100000 | pee true 'wc -l'

100000
#+END_EXAMPLE

So yeah. I'm not the first person to discover this, but I'm certain this was
quite surprising to each of us.

** DONE C++ probes with perf                                          :tools:
   CLOSED: [2019-12-16 Mon 21:19]

The Linux [[https://perf.wiki.kernel.org/index.php/Main_Page][=perf=]] tool can be used to (among many other things!) instrument
user-space code. Dynamic probes can be placed in arbitrary locations, but in my
usage, I almost always place them at function entry and exit points. Since
=perf= comes from the Linux kernel, it supports C well. But sometimes I need to
deal with C++, and =perf='s incomplete support is annoying. Today I figured out
how to make it sorta work, so I'm writing it up here.

For the record, I'm using perf 4.19.37 from linux-base=4.6 on Debian, on amd64.

Let's say I have this not-very-interesting C++ program in =tst.cc=:

#+BEGIN_SRC c++
#include <stdio.h>
namespace N
{
    void f(int x)
    {
        printf("%d\n", x);
    }
}

int main(int argc, char* argv[])
{
    N::f(argc);
    return 0;
}
#+END_SRC

It just calls the C++ function =N::f()=. I build this:

#+BEGIN_EXAMPLE
$ g++ -o tst tst.cc
#+END_EXAMPLE

And I ask =perf= about what functions are instrument-able:

#+BEGIN_EXAMPLE
$ perf probe -x tst --funcs

N::f
completed.7326
data_start
deregister_tm_clones
frame_dummy
main
printf@plt
register_tm_clones
#+END_EXAMPLE

Here =perf= says it can see =N::f= (it demangled the name, even), but if I try
to add a probe there, it barfs:

#+BEGIN_EXAMPLE
$ sudo perf probe -x tst --add N::f

Semantic error :There is non-digit char in line number.
#+END_EXAMPLE

The reason is that =perf='s probe syntax uses the =:= character for line
numbers, and this conflicts with the C++ scope syntax. =perf= /could/ infer that
a =::= is not a line number, but nobody has written that yet. I generally avoid
C++, so I'm going to stop at this post.

So how do we add a probe? Since =perf= can't handle mangled names, we can ask it
to skip the demangling:

#+BEGIN_EXAMPLE
$ perf probe -x tst --funcs --no-demangle

completed.7326
data_start
deregister_tm_clones
frame_dummy
main
printf@plt
register_tm_clones
#+END_EXAMPLE

But now my function is gone! Apparently there's a function filter that by
default throws out all functions that start with =_=. Let's disabled that
filter:

#+BEGIN_EXAMPLE
$ perf probe -x tst --funcs --no-demangle --filter '*'

_DYNAMIC
_GLOBAL_OFFSET_TABLE_
_IO_stdin_used
_ZN1N1fEi
__FRAME_END__
__TMC_END__
__data_start
__do_global_dtors_aux
__do_global_dtors_aux_fini_array_entry
__dso_handle
__frame_dummy_init_array_entry
__libc_csu_fini
__libc_csu_init
_edata
_fini
_init
_start
completed.7326
data_start
deregister_tm_clones
frame_dummy
main
printf@plt
register_tm_clones
#+END_EXAMPLE

Aha. There's my function: =_ZN1N1fEi=. Can I add a probe there?

#+BEGIN_EXAMPLE
# perf probe -x tst --add _ZN1N1fEi

Failed to find symbol _ZN1N1fEi in /tmp/tst
  Error: Failed to add events.
#+END_EXAMPLE

Nope. Apparently I need to explicitly tell it that I'm dealing with unmangled symbols:


#+BEGIN_EXAMPLE
# perf probe -x tst --add _ZN1N1fEi --no-demangle

Added new event:
  probe_tst:_ZN1N1fEi  (on _ZN1N1fEi in /tmp/tst)

You can now use it in all perf tools, such as:

        perf record -e probe_tst:_ZN1N1fEi -aR sleep 1
#+END_EXAMPLE

There it goes! Now let's add another probe, at the function exit:

#+BEGIN_EXAMPLE
# sudo perf probe -x tst --add _ZN1N1fEi_ret=_ZN1N1fEi%return --no-demangle 

Added new event:
  probe_tst:_ZN1N1fEi_ret__return (on _ZN1N1fEi%return in /tmp/tst)

You can now use it in all perf tools, such as:

        perf record -e probe_tst:_ZN1N1fEi_ret__return -aR sleep 1
#+END_EXAMPLE

And now I should be able to run the instrumented program, and to see all the
crossings of my probes:

#+BEGIN_EXAMPLE
# perf record -eprobe_tst:_ZN1N1fEi{,_ret__return} ./tst                  

1
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.017 MB perf.data (2 samples) ]


# perf script

             tst  6216 [001] 834490.086097:             probe_tst:_ZN1N1fEi: (559833acb135)
             tst  6216 [001] 834490.086196: probe_tst:_ZN1N1fEi_ret__return: (559833acb135 <- 559833acb172)
#+END_EXAMPLE

Sweet! Again, note that this is =perf= version 4.19.37, and other versions may
behave differently.

** DONE vnl-uniq                                           :tools:data:vnlog:
   CLOSED: [2019-12-21 Sat 14:17]

I just added a new tool to the [[https://github.com/dkogan/vnlog][=vnlog toolkit=]]: [[https://github.com/dkogan/vnlog#vnl-uniq][=vnl-uniq=]]. Similar to the
others, this one is a wrapper for the =uniq= tool in GNU coreutils. It reads
just enough of the input to get the legend, writes out the (possibly-modified)
legend, and then calls =exec= to pass control to =uniq= to handle the rest of
the data stream (i.e. to do all the actual work). The primary use case is to
make histograms:

#+BEGIN_EXAMPLE
$ cat objects.vnl

# size  color
1      blue
2      yellow
1      yellow
5      blue
3      yellow
4      orange
2      orange


$ < objects.vnl vnl-filter -p color |
                vnl-sort -k color   |
                vnl-uniq -c

# count color
      2 blue
      2 orange
      3 yellow
#+END_EXAMPLE

I also added a =--vnl-count NAME= to be able to name the =count= column.

As happens each time I wrap one of these tools, I end up reading the
documentation, and learning about new options. Apparently =uniq= knows how to
use a subset of the fields when testing for uniqueness: =uniq -f N= skips the
first =N= columns for the purposes of uniqueness. Naturally, =vnl-uniq= supports
this, and I added an extension: negative =N= can be passed-in to use /only/ the
last =-N= columns. So to use just the one last column, pass =-f -1=. This allows
the above to be invoked a bit more simply:

#+BEGIN_EXAMPLE
$ < objects.vnl vnl-sort -k color |
                vnl-uniq -c -f-1

# count size color
      2 1      blue
      2 2      orange
      3 1      yellow
#+END_EXAMPLE

Note that I didn't need to filter the input to throw out the columns I wasn't
interested in. And as a side-effect, the output of =vnl-uniq= now has the =size=
column also: this is the first size in a group of identical colors. Unclear if
this is useful, but it's what =uniq= does. Speaking of groups, something that
/is/ useful is =uniq --group=, which adds visual separation to groups of
identical fields. To report the full dataset, grouped by color:

#+BEGIN_EXAMPLE
$ < objects.vnl vnl-sort -k color |
                vnl-uniq --group -f-1

# size color
1      blue
5      blue

2      orange
4      orange

1      yellow
2      yellow
3      yellow
#+END_EXAMPLE

It looks like =uniq= provides no way to combine this with the counts (which
makes sense, given that =uniq= makes one pass through the data), but this can be
done by doing a join first. Looks complicated, but it's really not that bad:

#+BEGIN_EXAMPLE
$ vnl-join -j color <( < objects.vnl vnl-sort -k color )
                    <( < objects.vnl vnl-filter -p color | vnl-sort -k color | vnl-uniq -c -f-1 ) |
  vnl-filter -p '!color',color |
  vnl-align |
  vnl-uniq --group -f-1

# size count color
1      2     blue
5      2     blue

2      2     orange
4      2     orange

1      3     yellow
2      3     yellow
3      3     yellow
#+END_EXAMPLE

It's awkward that =uniq= works off trailing fields but =join= puts the key field
at the front, but that's how it is. If I care enough, I may add some sort of
=vnl-uniq --vnl-field F= to make this nicer, but it's not obviously worth the
typing.

** DONE numpysane and broadcasting in C   :tools:python:data:numpy:numpysane:
   CLOSED: [2020-03-15 Sun 01:08]

Since the beginning, the [[https://github.com/dkogan/numpysane][=numpysane=]] library provided a =broadcast_define()=
function to decorate existing Python routines to give them [[https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html][broadcasting
awareness]]. This was very useful, but slow. I just did lots of typing, and now I
have a flavor of this in C (the =numpysane_pywrap= module; new in =numpysane=
0.22). As expected, you get fast C loops! And similar to the rest of this
library, this is a port of something in [[http://pdl.perl.org][PDL]]: [[http://pdl.perl.org/?docs=PP&title=PDL::PP][=PDL::PP=]].

Full documentation lives here:

https://github.com/dkogan/numpysane/blob/master/README-pywrap.org

After writing this I realized that there was something similar available in
numpy this whole time:
https://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html

I haven't looked too deeply into this yet, but 2 things are clear:

There's a design difference: the numpy implementation uses function callbacks,
while I generate C code. Code generation is what =PDL::PP= does, and when I
thought about it earlier, it seemed like doing this with function pointers would
be too painful. I guess it's doable, though.

And at least in one case, the gufuncs aren't doing the right broadcasting thing:

#+BEGIN_EXAMPLE
>>> a = np.arange(5).reshape(5,1)
>>> b = np.arange(3)

>>> np.matmul(a,b)
ValueError: matmul: Input operand 1 has a mismatch in
   its core dimension 0, with gufunc signature
   (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)
#+END_EXAMPLE

This should work. And if you do this with =numpysane.broadcast_define()= or with
=numpysane_pywrap=, it /does/ work. I'll look at it later to figure out what it's
doing.

** DONE org-babel for documentation	  :tools:python:emacs:org:gnuplotlib:
   CLOSED: [2020-03-22 Sun 17:50]

So I just gave a [[https://github.com/dkogan/talk-numpysane-gnuplotlib/raw/master/numpysane-gnuplotlib.pdf][talk]] at [[https://www.socallinuxexpo.org][SCaLE 18x]] about [[https://github.com/dkogan/numpysane][=numpysane=]] and [[https://github.com/dkogan/gnuplotlib][=gnuplotlib=]], two
libraries I wrote to make using numpy bearable. With these two, it's actually
quite nice!

Prior to the talk I overhauled the documentation for both these projects. The
=gnuplotlib= docs now have a [[https://github.com/dkogan/gnuplotlib/blob/master/guide/guide.org][tutorial/gallery page]], which is interesting-enough
to write about. Check it out! Mostly it is a sequence of

- Here's a bit of Python code
- And here's the plot you get if you run that code

Clearly you want the plots in the documentation to correspond to the code, so
you want something to actually run each code snippet to produce each plot.
Automatically. I don't want to maintain these manually, and periodically
discover that the code doesn't make the plot I claim it does or worse: that the
code barfs. This is vaguely what Jupyter notebooks do, but they're ridiculous,
so I'm doing something better:

- The documentation page is a plain-text [[https://orgmode.org][org-mode]] file. org is a magical Emacs
  mode; if you've never heard of it, stop everything and go check it out
- This org file has a sequence of [[https://orgmode.org/worg/org-contrib/babel/intro.html][org-babel]] snippets. org-babel is a way to
  include snippets from various languages into org documents. Here I'm using
  Python, obviously
- I have org evaluate each snippet to produce the resulting figure, as an .svg
- I commit this .org file and the .svg plots, and push them to the git repo

That's it. The git repo is hosted by github, which has a rudimentary renderer
for .org documents. I'm committing the .svg files, so that's enough to get
rendered documentation that looks nice. Note that the /usual/ workflow is to use
org to export to html, but here I'm outsourcing that job to github; I just make
the .svg files, and that's enough.

Look at the link again: [[https://github.com/dkogan/gnuplotlib/blob/master/guide/guide.org][=gnuplotlib= tutorial/gallery]]. This is just a .org file
committed to the git repo. github is doing its normal org->html thing to display
this file. This has drawbacks too: github is ignoring the =:noexport:= tag on
the =init= section at the end of the file, so it's actually showing all the
emacs lisp goop that makes this work (described below!). It's at the end, so I
guess this is good-enough.

Those of us that use org-mode would be completely unsurprised to hear that the
talk is /also/ written as .org document. And the slides that show =gnuplotlib=
plots use the same org-babel system to render the plots.

It's all oh-so-nice. As with anything as flexible as org-babel, it's easy to get
into a situation where you're bending it to serve a not-quite-intended purpose.
But since this all lives in emacs, you can make it do whatever you want with a
bit of emacs lisp.

I ended up advising a few things (mailing list post [[https://lists.gnu.org/archive/html/emacs-orgmode/2020-03/msg00086.html][here]]). And I stumbled on an
(arguable) bug in emacs that needed working around (mailing list post [[https://lists.gnu.org/archive/html/emacs-devel/2020-03/msg00314.html][here]]).
I'll summarize both here.

*** Handling large Local Variables blocks

The advises I ended up with ended up longer than emacs expected, which made
emacs not evaluate them when loading the buffer. As I discovered (see the
mailing list post) the loading code looks for the string =Local Variables= in
the last 3000 bytes of the buffer only, and I exceeded that. Stefan Monnier
suggested a workaround in [[https://lists.gnu.org/archive/html/emacs-devel/2020-03/msg00320.html][this post]]. Instead of the normal =Local Variables=
block at the end:

#+begin_src emacs-lisp
Local Variables:
eval: (progn ... ...
             ... ...
             LONG chunk of emacs-lisp
      )
End:
#+end_src

I do this:

#+begin_src emacs-lisp
(progn ;;local-config
   lisp lisp lisp
   as long as I want
)

Local Variables:
eval: (progn (re-search-backward "^(progn ;;local-config") (eval (read (current-buffer))))
End:
#+end_src

So emacs sees a small chunk of code that searches backwards through the buffer
(as far back as needed) for the /real/ lisp to evaluate. As an aside, this blog
is /also/ an .org document, and the lisp snippets above are org-babel blocks
that I'm not evaluating. The exporter knows to respect the emacs-lisp syntax
highlighting, however.

*** Advises

OK, so what was all the stuff I needed to tell org-babel to do specially here?

First off, org needed to be able to communicate to the Python session the name
of the file to write the plot to. I do this by making the whole plist for this
org-babel snippet available to python:

#+begin_src emacs-lisp
;; THIS advice makes all the org-babel parameters available to python in the
;; _org_babel_params dict. I care about _org_babel_params['_file'] specifically,
;; but everything is available
(defun dima-org-babel-python-var-to-python (var)
  "Convert an elisp value to a python variable.
  Like the original, but supports (a . b) cells and symbols
"
  (if (listp var)
      (if (listp (cdr var))
          (concat "[" (mapconcat #'org-babel-python-var-to-python var ", ") "]")
        (format "\"\"\"%s\"\"\"" var))
    (if (symbolp var)
        (format "\"\"\"%s\"\"\"" var)
      (if (eq var 'hline)
          org-babel-python-hline-to
        (format
         (if (and (stringp var) (string-match "[\n\r]" var)) "\"\"%S\"\"" "%S")
         (if (stringp var) (substring-no-properties var) var))))))
(defun dima-alist-to-python-dict (alist)
  "Generates a string defining a python dict from the given alist"
  (let ((keyvalue-list
         (mapcar (lambda (x)
                   (format "%s = %s, "
                           (replace-regexp-in-string
                            "[^a-zA-Z0-9_]" "_"
                            (symbol-name (car x)))
                           (dima-org-babel-python-var-to-python (cdr x))))
                 alist)))
    (concat
     "dict( "
     (apply 'concat keyvalue-list)
     ")")))
(defun dima-org-babel-python-pass-all-params (f params)
  (cons
   (concat
    "_org_babel_params = "
    (dima-alist-to-python-dict params))
   (funcall f params)))
(unless
    (advice-member-p
     #'dima-org-babel-python-pass-all-params
     #'org-babel-variable-assignments:python)
  (advice-add
   #'org-babel-variable-assignments:python
   :around #'dima-org-babel-python-pass-all-params))
#+end_src

So if there's a =:file= plist key, the python code can grab that, and write the
plot to that filename. But I don't really want to specify an output file for
every single org-babel snippet. All I really care about is that each plot gets a
unique filename. So I omit the =:file= key entirely, and use this advice to
generate one for me:

#+begin_src emacs-lisp
;; This sets a default :file tag, set to a unique filename. I want each demo to
;; produce an image, but I don't care what it is called. I omit the :file tag
;; completely, and this advice takes care of it
(defun dima-org-babel-python-unique-plot-filename
    (f &optional arg info params)
  (funcall f arg info
           (cons (cons ':file
                       (format "guide-%d.svg"
                               (condition-case nil
                                   (setq dima-unique-plot-number (1+ dima-unique-plot-number))
                                 (error (setq dima-unique-plot-number 0)))))
                 params)))
(unless
    (advice-member-p
     #'dima-org-babel-python-unique-plot-filename
     #'org-babel-execute-src-block)
  (advice-add
   #'org-babel-execute-src-block
   :around #'dima-org-babel-python-unique-plot-filename))
#+end_src

This uses the =dima-unique-plot-number= integer to keep track of each plot. I
increment this with each plot. Getting closer. It isn't strictly required, but
it'd be nice if each plot had the same output filename each time I generated it.
So I want to reset the plot number to 0 each time:

#+begin_src emacs-lisp
;; If I'm regenerating ALL the plots, I start counting the plots from 0
(defun dima-reset-unique-plot-number
    (&rest args)
    (setq dima-unique-plot-number 0))
(unless
    (advice-member-p
     #'dima-reset-unique-plot-number
     #'org-babel-execute-buffer)
  (advice-add
   #'org-babel-execute-buffer
   :after #'dima-reset-unique-plot-number))
#+end_src

Finally, I want to lie to the user a little bit. The code I'm /actually/
executing writes each plot to an .svg. But the code I'd like the user to see
should use the default output: an interactive, graphical window. I do that by
tweaking the python session to tell the =gnuplotlib= object to write to .svg
files from org by default, instead of using the graphical terminal:

#+begin_src emacs-lisp
;; I'm using github to display guide.org, so I'm not using the "normal" org
;; exporter. I want the demo text to not contain the hardcopy= tags, but clearly
;; I need the hardcopy tag when generating the plots. I add some python to
;; override gnuplotlib.plot() to add the hardcopy tag somewhere where the reader
;; won't see it. But where to put this python override code? If I put it into an
;; org-babel block, it will be rendered, and the :export tags will be ignored,
;; since github doesn't respect those (probably). So I put the extra stuff into
;; an advice. Whew.
(defun dima-org-babel-python-set-demo-output (f body params)
  (with-temp-buffer
    (insert body)
    (beginning-of-buffer)
    (when (search-forward "import gnuplotlib as gp" nil t)
      (end-of-line)
      (insert
       "\n"
       "if not hasattr(gp.gnuplotlib, 'orig_init'):\n"
       "    gp.gnuplotlib.orig_init = gp.gnuplotlib.__init__\n"
       "gp.gnuplotlib.__init__ = lambda self, *args, **kwargs: gp.gnuplotlib.orig_init(self, *args, hardcopy=_org_babel_params['_file'] if 'file' in _org_babel_params['_result_params'] else None, **kwargs)\n"))
    (setq body (buffer-substring-no-properties (point-min) (point-max))))
  (funcall f body params))

(unless
    (advice-member-p
     #'dima-org-babel-python-set-demo-output
     #'org-babel-execute:python)
  (advice-add
   #'org-babel-execute:python
   :around #'dima-org-babel-python-set-demo-output))
)
#+end_src

And that's it. The advises in the talk are slightly different, in uninteresting
ways. Some of this should be upstreamed to org-babel somehow. Now entirely clear
which part, but I'll cross that bridge when I get to it.


* making dvipng work for equations                                 :noexport:
- OPTIONS do not work. Need to enable explicitly with
  (setq org-html-with-latex 'dvipng)

- o-blog translates the path of file: links and copies the file in
  (o-blog-publish-linked-files)

- equations aren't processed there

- equations are touched by the ltxpng stuff in ox-html.el such as

  org-html--format-image

* init                                                             :noexport:
Local Variables:
eval: (progn
          (org-babel-do-load-languages
           'org-babel-load-languages
            '((gnuplot . t)))
          (setq org-format-latex-options (plist-put (plist-put org-format-latex-options :background "White") :foreground "Black"))
          (auto-fill-mode)
          (load-library "compile")
          (load-library "o-blog")
          (setq org-html-table-default-attributes
            '(:border "2" :cellspacing "0" :cellpadding "6" :rules "all" :frame "box"))
          ;; don't automatically subscript with _
          (setq org-export-with-sub-superscripts '{})
          (local-set-key (kbd "<f5>")
                         (lambda () (interactive)
                           (org-publish-blog (buffer-file-name))
                           (browse-url "http://127.0.0.1/blog/")))
          (local-set-key (kbd "<S-f5>")
                         (lambda () (interactive)
                           (shell-command "cd out; git clean -ffdx")
                           (org-publish-blog (buffer-file-name))
                           (shell-command "git push origin master; cd out; git add -A && git commit -a -m 'update' && git push origin master;")))
          (defun ob:link-to-post (post)
            (format "%s/%s" (ob:path-to-root) (ob:post-htmlfile post)))
          (defun ob:get-post-by-title (title)
            (let ((posts (ob:get-posts
                          (lambda (x)
                            (equal title (ob:post-title x)))
                          1)))
              (if posts (car posts) nil))))
End:
