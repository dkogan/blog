#+TITLE: Dima Kogan
#-DESCRIPTION: Dima's notes
#+DATE: <2014-03-18 Tue 22:25>
#+AUTHOR Dima Kogan

#+STARTUP: logdone
#+STARTUP: overview

#+PLUGIN_QRCODE: nil
#+TEMPLATE_DIR: templates
#+URL: http://notes.secretsauce.net

#+DEFAULT_CATEGORY: Notes
#+FILENAME_SANITIZER: ob-sanitize-string
#+POST_SORTER: ob-sort-posts-by-title
#+OPTIONS: tex:dvipng

* Blog details
** Copyright
  :PROPERTIES:
  :SNIPPET:  t
  :END:

   The text is in the public domain. All code is copyright [[mailto:dima -at- secretsauce -dot- net][Dima Kogan]], licensed
   under the LGPL.

** Navigation
  :PROPERTIES:
  :SNIPPET:  t
  :END:

- [[file:{lisp}(ob:path-to-root){/lisp}/tags][Tags]]
- [[file:{lisp}(ob:path-to-root){/lisp}/archives.html][Archives]]
- [[file:{lisp}(ob:path-to-root){/lisp}/index.xml][RSS]]
- [[https://www.github.com/dkogan][Github]]

* Toplevel page
  :PROPERTIES:
  :PAGE:     index.html
  :TEMPLATE: blog_static_no_title.html
  :END:

This space is meant to contain whatever Dima is thinking about at various times.
The full set of entries is in the [[file:{lisp}(ob:path-to-root){/lisp}/archives.html][Archives]]. Various coding projects/experiments
are in the [[https://www.github.com/dkogan][Github]] repostories. Contact info is at the bottom of the page (in
tiny print).

Most recent post: [[file:{lisp}(ob:link-to-post (car ALL-POSTS)){/lisp}][{lisp}(ob:post-title (car ALL-POSTS)){/lisp}]]

* Notes
** DONE First post                                                   :o@blog:
  CLOSED: [2014-03-17 Mon 03:48]

So it has come to this. I'd like to know what the hell I actually do all day, so
maybe keeping a record will make that more clear. Today, I started a blog! I had
some ideas about the publishing system I wanted going in, and finding that
system was interesting. Requirements:

- Editing in emacs with org-mode
- Publishing with a simple file copy (or git push, etc)
- Static pages; no server work other than sending the files
- No javascript; no client work other than basic rendering
- Premade templates so that things look reasonable out of the box
- Does not require me to actually learn web development

Turns out you can't have everything, but you can get close. In its simplest, org
itself can push HTML without anything else. This is minimally-styled, and not
assembled into pages that talk to each other. The org-mode wiki (worg) has a
list of various publishing systems that use org as a backend:
http://orgmode.org/worg/org-blog-wiki.html. There are several exporters to
common blogging platforms, and a few specially-written things.

In the end, I was looking at two systems: [[http://renard.github.io/o-blog/][*o-blog*]] and [[https://github.com/kelvinh/org-page][*org-page*]]. Both are
/blogging/ systems, so you get RSS, tags, timelines, etc.

*** Advantages of o-blog

- Neither o-blog or org-page are used heavily, but o-blog appears to have more
  users: I could only find one org-page site that wasn't the author's personal
  page
- Org-page feels a bit rougher than o-blog, which has a much nicer-looking
  layout out of the box
- Org-page feels more boilerplaty. Each file needs lots of tags that could
  potentially conflict
- Org-page has some sort of git integration, which maybe is actually a positive,
  but I didn't dig into it. The main publishing function takes some git refs,
  the system expects a particular branching structure, etc. Those are probably
  good, but it should be obvious what the basic export-all-this-stuff command is

*** Advantages of org-page

- Org-page organizes the posts into several files, while o-blog has the whole
  thing in one file. With o-blog this probably will get slow as the posts
  accumulate. The author is currently working on an update that supposedly would
  resolve this
- The major downside of o-blog is that it produces a very heavy,
  javascript-laden site that doesn't work /at all/ with JS turned off.

I was leaning towards o-blog, so I learned some web-development. O-blog uses
LESS instead of CSS, using some runtime javascript to convert the LESS (that the
browser doesn't understand) to CSS (which the browser /does/ understand). So I
exported the LESS to CSS, scrubbed the output to get rid of the shinier bits,
and I have my blog. This look reasonable, but still not ideal. Current issues:

- The new JS-free navigation bars I wrote disappear when the browser window is
  narrower than some threshold. This is almost certainly intentional in the CSS.
  It needs to do something better than simply disappearing. The JS version /did/
  do someting better, and I should try to match that
- O-blog renders equations with MathJax, which uses javascript. Org has the
  capability to generate images for each equation, and output those. I'd like to
  do that, but o-blog can't figure that out. I'll fix it at some point
- Tables look somewhat weird. Pretty sure this is an org feature (not o-blog).
  On opera I see a full grid, except the left bar. On firefox I see the top and
  bottom bars and no others.

There're probably more, and I'll discover them as I go. In the meantime, the
code and content are on Github.
** DONE Using DEMs to get GPX elevation profiles                :hiking:data:
   CLOSED: [2014-03-18 Tue 00:49]

When considering a new hike, I often want to know the elevation profile of the
route. Usually all I have is a 2D elevation track, from [[http://www.openstreetmap.org][OpenStreetMap]] for
instance; clearly this lacks elevation information.

Unrelatedly we have access to gridded elevation data. This primarily comes from
the [[http://en.wikipedia.org/wiki/SRTM][SRTM]] project: data available here: http://dds.cr.usgs.gov/srtm/version2_1/.
The raw SRTM data is pretty good, but there are some gaps. Some people have
cleaned up the raw data, to make clean tiles available. One such data source is
here: http://www.viewfinderpanoramas.org/dem3/.

So we have 2D track data and topography. We can thus combine these into a full
3D track. This isn't perfect since DEM data is granular, but it's way better
than nothing.

I just found out that there's a route to [[http://www.openstreetmap.org/#map=15/34.1662/-117.9293][Fish Canyon Falls]] that goes around the
rock quarry, and thus is open year-round. Bypassing the quarry requires climbing
up a steep hillside to gain a ridge, then descending the other side of the ridge
to the bottom of the canyon behind the quarry. Just how much extra climbing is
involved here? To find out, I wrote this:

#+CAPTION: =gpxSampleDEM.pl=
#+begin_src perl
#!/usr/bin/perl
use strict;
use warnings;

use Getopt::Euclid;
use feature ':5.10';
use autodie;

use Geo::Gpx;
use PDL;


my $W = 1201; # I use 3-minute DEMs, so each DEM is 1201 x 1201

my $gpx_fh;
if( $ARGV{'<input>'} eq '-' )
{
    $gpx_fh = \*STDIN;
}
else
{
  open $gpx_fh, '<', $ARGV{'<input>'};
}

my $gpx = Geo::Gpx->new( input => $gpx_fh );

my $iter = $gpx->iterate_points();
while( my $pt = $iter->() )
{
    say join( ' ', $pt->{lon}, $pt->{lat}, elevation( $pt->{lon}, $pt->{lat} ) );
}



sub elevation
{
    my ($lon, $lat) = @_;

    state %DEMs;
    my $demfileE = floor $lon;
    my $demfileN = floor $lat;

    $DEMs{$demfileE}{$demfileN} //= readDEM($demfileE, $demfileN);
    my $dem = $DEMs{$demfileE}{$demfileN};
    return 0 if( !ref($dem) );

    # use PDL::Graphics::Gnuplot;
    # gplot( with => 'image', $dem );
    # sleep(20);

    # the DEMs start in the NW corner
    my $ilon =      ($lon - $demfileE)  * $W;
    my $ilat = (1 - ($lat - $demfileN)) * $W;

    return 100.0/2.54/12.0 * $dem->interpND( pdl[$ilon, $ilat] );
}

sub readDEM
{
    my ($demfileE, $demfileN) = @_;

    my $path;
    if   ($demfileN >= 0 && $demfileE >= 0){ $path = sprintf("$ARGV{'--demdir'}/N%.2dE%.3d.hgt", $demfileN,  $demfileE); }
    elsif($demfileN >= 0 && $demfileE <  0){ $path = sprintf("$ARGV{'--demdir'}/N%.2dW%.3d.hgt", $demfileN, -$demfileE); }
    elsif($demfileN  < 0 && $demfileE >= 0){ $path = sprintf("$ARGV{'--demdir'}/S%.2dE%.3d.hgt", -$demfileN, $demfileE); }
    else                                   { $path = sprintf("$ARGV{'--demdir'}/S%.2dW%.3d.hgt", -$demfileN, -$demfileE); }

    say STDERR "Reading DEM '$path'";
    if( ! -e $path )
    {
        warn "DEM '$path' not found. All of its elevations will read as 0";
        return 0;
    }

    # I read the DEM on disk into the piddle, then flip the endianness of the
    # data. I wouldn't have to copy anything if the data was little-endian to
    # start with; I'd just mmap into the piddle.
    open my $fd, '<', $path;
    my $dem = zeros(short, $W, $W);
    sysread( $fd, ${$dem->get_dataref}, $W*$W*2, 0 );
    ${$dem->get_dataref} = pack( "s*", unpack("s>*", ${$dem->get_dataref}));

    # I also convert to floating point. Turns out the PDL interpolation routines
    # don't work with integers
    return $dem->float;
}



__END__

=head1 NAME

gpxSampleDEM.pl - Samples SRTM DEM data to compute elevations for a GPX track

=head1 OPTIONAL ARGUMENTS

=over

=item --demdir <demdir>

Directory that contains the DEM files

=for Euclid:
  demdir.type: string, -d demdir && -e demdir
  demdir.default: '.'

=item <input>

GPX input. If omitted or '-', the input is read from standard input

=for Euclid:
  input.type: readable
  input.default: '-'

=back

=head1 AUTHOR

Dima Kogan, C<< <dima@secretsauce.net> >>
#+end_src

The script is fairly straightforward. It examines every track point in the GPX,
finds the appropriate elevation using plain bilinear interpolation, and outputs
a (lon,lat,ele) tuple on STDOUT. On Debian the dependencies are

- =libgetopt-euclid-perl=
- =libgeo-gpx-perl=
- =pdl=

You need to pre-download 3" DEMs, and pass the directory to the script (1" would
certainly work better, but I haven't tried). Given the [[file:files/FishCanyonFalls/FishCanyonFallsTrail.gpx][gpx file]] scraped from an
OpenStreetMap way (itself traced from the satellite imagery), you can do this:

#+begin_src sh
gpxSampleDEM.pl --demdir DEMs FishCanyonFallsTrail.gpx | \
  feedgnuplot --domain --3d --lines --square_xy          \
     --xlabel 'lon(deg)' --ylabel 'lat(deg)' --zlabel 'Elevation(m)'
#+end_src

This makes an interactive 3D plot of the route. For a more traditional elevation
profile that's monotonic in distance, you can do something like this:

#+begin_src sh
gpxSampleDEM.pl --demdir DEMs FishCanyonFallsTrail.gpx | \
  awk '{print $3}'                                     | \
  feedgnuplot --lines                                    \
     --xlabel 'Monotonic with distance' --ylabel 'Elevation(m)'
#+end_src

I actually did go see this waterfall today (which is really nice). Here's a plot
of the elevation profile I gathered with my GPS unit today overlaid over the
elevation profile from the DEM:

#+begin_comment
Following plot made by exporting each data source, and plotting with gnuplot

gpx_xyz.pl ~/hiking/gpx/FishCanyonFalls.gpx | awk '!/#/ {print $3}' > real
gpxSampleDEM.pl --demdir ~/projects/horizon/DEMs_SRTM3.bak/ FishCanyonFallsTrail.gpx | awk '{print $3}' > fake

set xlabel "Monotonic with distance"
set ylabel "Elevation (m)"
set terminal svg
set output "FishCanyonFalls.svg"
plot "real" using ($0/1101):1 with lines title "Actual track from a hike", "fake" using ($0/1400):1 with lines title "Generated from a DEM"
set output
#+end_comment

#+ATTR_HTML: :width 80%
[[file:files/FishCanyonFalls/FishCanyonFalls.svg]]

Immediately several issues are noticeable[fn:1]. First of all, while each curve
is monotonic with distance, the relationship of the domain with distance is
different. This plot assumes they're both /linear/ with distance. It's not
really true, but close enough I suppose.

Second, we see that the DEM curve has some high-frequency oscillations. Those
are switchbacks that sample the DEM in a way that the DEM data is too coarse to
represent. The trail does not really oscillate like that, which is confirmed by
the GPS track. This effect would probably be mitigated with finer DEM data (1"
DEMs are available), but I haven't attempted this.

Third, we see that during the initial climb the DEM-derived elevation
consistently underreports the elevation. I suspect this is another artifact of
the coarseness of the DEM. If we're walking on a ridge, a bilinear interpolation
would take into account neighboring DEM pixels, which would be lower in
elevation (since it's a ridge). So on a ridge I would expect the DEM-derived
elevations to be under-reported, and in a canyon I'd expect them to be
over-reported. In this particular track, the initial climb and the initial
descent are on ridges, while the second climb is in a canyon. This brings us to
the next point.

The data in the second climb doesn't match /at all/. Here it's the GPS data
that's at fault. The canyon walls block the GPS signal, so GPS readings are
unreliable there.

So the grand conclusion of all this would appear to be that you can use 3" DEM
data to derive an elevation profile, but one should not expect this profile to
be terribly accurate. Still it's useful. Based purely on the DEM, I can see that
a round-trip on this route would entail 2800ft of net elevation gain. Seeing the
real track, this probably is an underestimate of ~200ft. Not bad.


[fn:1] The above analysis assumes that the implementation of the DEM sampler is
bug-free and that the DEM data is correct. While I don't know of any bugs, there
could be some. Same for the DEM data

** DONE X11 urgency hint and notifications                            :tools:
   CLOSED: [2014-03-19 Wed 00:20]

X11 has a common system for window notifications: the urgency hint. The relevant
section of the [[http://tronche.com/gui/x/icccm/sec-4.html#s-4.1.2.4][ICCCM standard]]:

#+begin_quote
The UrgencyHint flag, if set in the flags field, indicates that the client deems
the window contents to be urgent, requiring the timely response of the user. The
window manager must make some effort to draw the user's attention to this window
while this flag is set.
#+end_quote

Some window managers are uncompliant and don't support this. Possibly as a
result, people really like to reinvent this particular wheel: [[http://www.linuxjournal.com/content/tech-tip-get-notifications-your-scripts-notify-send][notify-send]],
[[http://mattn.github.io/growl-for-linux/][growl]], and more. My WM ([[http://notion.sourceforge.net/][notion]]) /does/ support this very well, with some really
nice UI integration. Thus applications can request to be drawn as urgent. This
really begs for a commandline tool so shells can request the user's attention at
key points. For instance I really want to say something like

#+begin_src sh
make; seturgent
#+end_src

I.e. this would launch a source build, and when the build completes, this
particular terminal emulator window would request the user's attention. The
build could take a long, time, and the user may want to do stuff with the build
products, but in the meantime they can go do something else.

This =seturgent= tool didn't exist, so I wrote one:

#+CAPTION: =seturgent=
#+begin_src perl
#!/usr/bin/perl

# Copyright 2012,2013 Dima Kogan
# License: GPL 3 or later

use strict;
use warnings;
use feature qw(say);

use X11::Protocol;
use X11::Protocol::WM;
use X11::WindowHierarchy;

# if no arguments are given, sets urgency on the current window
#
# if an argument is given, uses it as a regex on the window name (all matches
# are set as urgent)

my $usage = "$0 [regex on the window name]";
die $usage if @ARGV > 1;


my $x = X11::Protocol->new()
  or die "Couldn't open display";

my @ids;
if( ! @ARGV )
{
  @ids = ($ENV{WINDOWID});
}
else
{
  my @windows = x11_filter_hierarchy( filter => qr{$ARGV[0]} )
    or die "No matching windows found";

  say "Found " . scalar(@windows) . " matching windows";
  @ids = map {$_->{id}} @windows;
}

foreach my $id(@ids)
{
  die "No window id" unless $id;
  X11::Protocol::WM::change_wm_hints( $x, $id,
                                      urgency => 1 );
}
#+end_src

This uses [[https://metacpan.org/pod/X11::WindowHierarchy][X11::WindowHierarchy]] to find the window, and [[https://metacpan.org/pod/X11::Protocol::WM][X11::Protocol::WM]] to set
the urgency hint. Both are available in Debian. Usage is very straightforward:
with no arguments, the current window is set urgent. Otherwise, the one and only
argument is treated like a regex on the window title. If a single match is
found, that window is set urgent.

Now I /can/ say

#+begin_src sh
make; seturgent
#+end_src
** DONE Already-running process notifications                         :tools:
   CLOSED: [2014-03-20 Thu 22:35]

The tool described in the last post ([[file:{lisp}(ob:link-to-post (ob:get-post-by-title "X11 urgency hint and notifications")){/lisp}][X11 urgency hint and notifications]]) works
well, but there's a common use case it does not support: completion notification
of already-running process. That post describes how to be notified when a build
completes:

#+begin_src sh
make; seturgent
#+end_src

But what if we already started the build? Another helper tool is required. Here
it is:

#+BEGIN_SRC sh

# As is, this can't be an external utility since it uses the shell builtin
# 'wait', which only works on direct children of this shell. An external utility
# creates another shell, so this doesn't work
function waitfor()
{
    # waits for a process to exit, and sets urgency when that happens. Expects a
    # single pgrep-able argument on the commandline. If no argument is given,
    # it'll look for the only child process.

    # if this process is a child of this shell, I use a blocking wait.
    # Otherwise, I poll.

    PID_ALL=$(pgrep -s0 -f $1)

    # filter out the current process (the shell) and 'xclip'. I have xclip
    # zombies apparently
    PID=$(comm -23 <(echo $PID_ALL | sort) <(echo $$ `pidof xclip` | xargs -n1 | sort))
    N=$(echo $PID | wc -w)

    if [[ $N -eq 1 ]]; then
        echo "Found unique process with pid $PID"
        kill -CONT $PID # resume this process, since it's almost certainly
                        # paused right now
        wait $PID;
        seturgent
    elif [[ $N -ne 0 ]]; then
        echo "Found more than one matching process. Doing nothing";
    elif [[ -z $1 ]]; then
        echo "No children of the current shell to wait on. Doing nothing";
    else
        echo "Found no matching processes in this shell. Looking globally.";
        PID=$(pgrep -f $1)
        N=$(echo $PID | wc -w)
        if [[ $N -eq 0 ]]; then
            echo "Found no matching global process either. Giving up.";
        elif [[ $N -ne 1 ]]; then
            echo "Found more than one global process. Giving up";
        else
            echo "Found unique process with pid $PID"
            while (ps -p $PID > /dev/null) { sleep 10; }
            seturgent;
        fi
    fi
}

#+END_SRC

This is a =zsh= shell script that lives in my =.zshrc=.

- with no argument, it acts on the only child of this shell
- with an argument, it uses =pgrep= to find a matching process, first in the
  local shell, then outside of the local shell

Once the target process is identified, the script waits for the process to exit,
then it sets the urgency hint on the terminal emulator window. If there's any
ambiguity about which process is being targeted, nothing is done.

The most common use case: if a long-running process is currently active, one
would temporarily suspend it with =C-z=, then issue a =waitfor=. This
re-activates the process, and sets the urgency when finished. One could also
re-implement the use case from the previous post as

#+begin_src sh
make & waitfor
#+end_src


As said previously, this is a =zsh= script. It probably needs to be tweaked a
little bit to work in =bash=, but I have not done this.

The reason this is a shell script, is that the wait-for-this-process-to-finish
operation on Linux only works from the parent of the process being waited on. As
implemented, =waitfor()= doesn't spawn a new process, and runs in the shell
process itself, which is the parent of the thing being waited on. If this was
anything other than a shell script, then the waiter would /also/ be a child of
the shell, so the process being waited on, and the process doing the waiting
would be /siblings/. The script works that case too, but it polls every 10
seconds, instead of being notified of completion.

I've been using this for a little bit. It's not perfect, and there're some warts
I'd like to fix. Still, it does the job, and it's already something I use every
day.

** DONE Cscope benchmarks                                  :tools:data:emacs:
   CLOSED: [2014-03-25 Tue 03:36]

I read and write lots of C code, and I find the [[http://cscope.sourceforge.net/][cscope]] tool to be invaluable in
searching and navigating code bases. Recently I took over maintership of the
[[https://github.com/dkogan/xcscope.el][xcscope.el]] Emacs interface to cscope. There are a surprising number of different
Emacs interfaces to cscope, and this one seems to be the most mature and
full-featured (and I made it much nicer).

One feature that some other interfaces have ([[http://lists.gnu.org/archive/html/gnu-emacs-sources/2008-04/msg00021.html][ascope]] for instance) is that
instead of running a new cscope process for each query, they leave the process
running, and reuse it for each query. This keeps the database in memory, and
doesn't waste cycles reloading it every time. This is the major feature of these
interfaces, and glorious performance benefits are claimed.

Currently =xcscope= does /not/ do this, and I sometimes consider implementing
this feature. It's going to be a pain to do, so I decided to run some tests to
see if the performance benefits really are worth it.

*** Benchmark machine

All tests were run on my old-ish laptop. It is a 2008 vintage machine, has a
Core2-Duo CPU, 4GB of RAM and a non-SSD hard disk. I.e. this should be fast
enough for most things.

*** Test description

The code base under test is the linux kernel. This should be near the upper
bound of what most people would be indexing. Sure, larger projects exist, but
you're generally working on a contained piece, rather than the whole thing at
once (this is true of the kernel too, actually).

I perform multiple discrete cscope operations using the command-line tool. Each
operation starts a new =cscope= process, which reloads the database. I.e. I
perform the operation that's supposedly slow every time.

I measure how long it takes to build the search database, then to re-build it,
then to re-build it after =touch=-ing a file. Then I measure how long it takes
to run a search, then to re-run it, then to re-run it after =touch=-ing a file.

I do all this with the default settings, then again with settings more
appropriate for a kernel:

- /kernel mode/: =-k= option. Doesn't look in =/usr/include=
- /inverted-index mode/: =-q= option. Builds an extra index for faster searches

Finally, each search is also run with the =-d= option. This only runs the
search; it does /not/ also update the database with each search. By default,
cscope /does/ update the database with every search.

Specifically, I get the list of files with 

#+begin_src sh
cscope-indexer -l -r  
#+end_src

I build an index with

#+begin_src sh
cscope -b
#+end_src

If I'm indexing in kernel mode and I'm building an inverted index, I also pass
in =-q -k=. The test search looks for all uses of the =main= symbol:

#+begin_src sh
cscope -L0 main
#+end_src

Once again, if I'm indexing in kernel mode and I'm building an inverted index, I
also pass in =-q -k=. When I want to touch an arbitrary file, I do

#+begin_src sh
touch include/drm/drm_edid.h 
#+end_src

There's no significance to this file. It's just anything that's in the index.

As one can imagine, the disk cache plays a very significant role here, and
subsequent runs of the same command complete faster than the first. The numbers
shown below were obtained after clearning the disk cache first, but even so I'm
seeing lots of variability. There's maybe a +- 20% error in those measurements.
It isn't clear how much this is relevant to real-world performance, since in
real life you'll never have a completely cold cache.

*** Results

**** Default settings

|                                              | First time | Subsequent times |
|----------------------------------------------+------------+------------------|
| Initial database build                       | 60 sec     | 0.2 sec          |
| Database re-build after touching a file      | 8 sec      | 0.2 sec          |
| Initial search                               | 9 sec      | 1.3 sec          |
| Re-search after touching a file              | 9 sec      | 1.3 sec          |
| Initial no-db-update search                  | 6 sec      | 1.1 sec          |
| No-db-update re-search after touching a file | 6 sec      | 1.1 sec          |

**** Kernel, inverted-index mode

|                                              | First time | Subsequent times |
|----------------------------------------------+------------+------------------|
| Initial database build                       | 130 sec    | 0.2 sec          |
| Database re-build after touching a file      | 90 sec     | 0.2 sec          |
| Initial search                               | 3 sec      | 0.2 sec          |
| Re-search after touching a file              | 100 sec    | 0.2 sec          |
| Initial no-db-update search                  | 1 sec      | 0.01 sec         |
| No-db-update re-search after touching a file | 1 sec      | 0.01 sec         |


*** Conclusions

I've known about the cscope inverted index for a while, but never actually tried
to use it. Looks like it works as advertised: takes significantly longer to
build, but once built the speedup it provides is substantial. With the inverted
index for some reason I was consistently seeing a post-touch re-search take
slightly longer than a post-touch database re-build (100 seconds vs 90 seconds).
I would expect the re-search to do a re-build as part of its operation, so this
is somewhat surprising. On the other hand I only see this with a cold cache, and
those results are pretty variable, so this might just be noise. And it doesn't
really matter, since we'll rarely have such a cold cache.

It's extremely clear that the overhead of just loading the database is
immaterial. It takes 0.01 seconds load the database and then to run a search in
an inverted index. It's a bit slower without an inverted index, but all the time
there is spent searching, not loading the index into memory. I know this because
I get the same no-inverted-index search timings with the cscope interactive
tool, which loads the database just once. The only way keeping the =cscope=
process running is advantageous is if this makes it more likely the caches stay
warm. This is difficult to test, but I doubt it's true. If I run repeated
queries even with a new process every time, the data stays cached, and things
run quickly. What I think is much more likely is that the people who wrote
cscope interfaces such as =ascope= only used interfaces such as =xcscope=
without the =-d= option. I.e. they were updating the database with every query,
which clearly can be slow with a large codebase. Then they were /not/ doing this
with their persistent =cscope= sessions, and attributing the performance gains
to not loading the database rather than rebuilding the index too often. In any
case, I think it's pretty clear that this feature is not worth the work, so I'm
keeping =xcscope= as is.

* init                                                             :noexport:
;; Local Variables:
;; eval: (progn
;;           (local-set-key (kbd "<f5>")
;;                          (lambda () (interactive)
;;                            (org-publish-blog (buffer-file-name))))
;;           (local-set-key (kbd "<S-f5>")
;;                          (lambda () (interactive)
;;                            (shell-command "cd out; git clean -ffdx")
;;                            (org-publish-blog (buffer-file-name))
;;                            (shell-command "cd out; git add -A && git commit -a -m 'new post' && git push;")))
;;
;;           (defun ob:link-to-post (post)
;;             (format "%s/%s" (ob:path-to-root) (ob:post-htmlfile post)))
;;
;;           (defun ob:get-post-by-title (title)
;;             (let ((posts (ob:get-posts
;;                           (lambda (x)
;;                             (equal title (ob:post-title x)))
;;                           1)))
;;               (if posts (car posts) nil))))
;; End:
